{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f106fd6d",
   "metadata": {},
   "source": [
    "## Áí∞Â¢ÉÊßãÁØâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a35361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"üü¶\"\n",
    "        case logging.INFO:\n",
    "            level = \"üü©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"üü®\"\n",
    "        case logging.ERROR:\n",
    "            level = \"üü•\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"üõë\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(\"debug.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "NVIDIA_SMI = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True).stdout\n",
    "logging.info(NVIDIA_SMI)\n",
    "logging.info(f\"Python {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) PyTorch„Å®Transformers„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´\n",
    "%pip install torch==2.4.0\n",
    "\n",
    "# 2) vLLM„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´\n",
    "%pip install vllm==0.6.3\n",
    "\n",
    "# 3) Flash Attention„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´\n",
    "# 2.8.3„ÅØundefined symbol„Ç®„É©„Éº„ÅåÁô∫Áîü„Åô„Çã„Åü„ÇÅ2.7.3„Çí„Ç§„É≥„Çπ„Éà„Éº„É´\n",
    "# https://github.com/Dao-AILab/flash-attention/issues/1832\n",
    "# %pip install \"https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl\" --no-build-isolation\n",
    "%pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0307c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspaces/tinyzero/TinyZero\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b337fc",
   "metadata": {},
   "source": [
    "## „Éá„Éº„Çø„Çª„ÉÉ„ÉàÊßãÁØâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5f272b",
   "metadata": {},
   "source": [
    "```sh\n",
    "python ./examples/data_preprocess/countdown.py --local_dir countdown\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from datasets import Dataset, load_dataset\n",
    "from random import randint, seed, choice\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "from verl.utils.hdfs_io import copy, makedirs\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--local_dir', default='~/data/countdown')\n",
    "parser.add_argument('--hdfs_dir', default=None)\n",
    "parser.add_argument('--num_samples', type=int, default=100000)\n",
    "parser.add_argument('--num_operands', type=int, default=6)\n",
    "parser.add_argument('--max_target', type=int, default=1000)\n",
    "parser.add_argument('--min_number', type=int, default=1)\n",
    "parser.add_argument('--max_number', type=int, default=100)\n",
    "parser.add_argument('--train_size', type=int, default=327680)\n",
    "parser.add_argument('--test_size', type=int, default=1024)\n",
    "parser.add_argument('--template_type', type=str, default='base')\n",
    "\n",
    "args = parser.parse_args([\"--local_dir\", \"countdown\"])\n",
    "\n",
    "# „Éá„Éê„ÉÉ„Ç∞Áî®„Å´‰∏äÊõ∏„Åç\n",
    "args.train_size = 32 \n",
    "args.test_size = 32\n",
    "\n",
    "data_source = 'countdown'\n",
    "TRAIN_SIZE = args.train_size\n",
    "TEST_SIZE = args.test_size\n",
    "\n",
    "TRAIN_SIZE, TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d509182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4\n",
    "raw_dataset = load_dataset('Jiayi-Pan/Countdown-Tasks-3to4', split='train')\n",
    "len(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e30bb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(raw_dataset) > TRAIN_SIZE + TEST_SIZE\n",
    "train_dataset = raw_dataset.select(range(TRAIN_SIZE))\n",
    "test_dataset = raw_dataset.select(range(TRAIN_SIZE, TRAIN_SIZE + TEST_SIZE))\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prefix(dp, template_type):\n",
    "    \"\"\"\n",
    "    „Éá„Éº„Çø„Éù„Ç§„É≥„Éà„Åã„Çâ„Éó„É≠„É≥„Éó„Éà„ÅÆ„Éó„É¨„Éï„Ç£„ÉÉ„ÇØ„Çπ„Çí‰ΩúÊàê„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        dp (dict): target„Å®nums„ÇíÂê´„ÇÄ„Éá„Éº„Çø„Éù„Ç§„É≥„Éà\n",
    "        template_type (str): ‰ΩøÁî®„Åô„Çã„ÉÜ„É≥„Éó„É¨„Éº„Éà„ÅÆÁ®ÆÈ°û\n",
    "            - base: ‰∏ÄËà¨ÁöÑ„Å™„Éô„Éº„Çπ„É¢„Éá„É´Áî®\n",
    "            - qwen-instruct: Qwen Instruct„É¢„Éá„É´Áî®\n",
    "\n",
    "    Returns:\n",
    "        str: „Éó„É≠„É≥„Éó„Éà„ÅÆ„Éó„É¨„Éï„Ç£„ÉÉ„ÇØ„Çπ\n",
    "    \"\"\"\n",
    "    target = dp['target']\n",
    "    numbers = dp['nums']\n",
    "\n",
    "    # „Éô„Éº„Çπ„É¢„Éá„É´„ÅÆÂ†¥Âêà\n",
    "    if template_type == 'base':\n",
    "        # User: {numbers} „Çí‰Ωø„Å£„Å¶ {target} „Å´„Å™„ÇãÊï∞Âºè„Çí‰Ωú„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "        # ÂõõÂâáÊºîÁÆóÔºà+, -, *, /Ôºâ„Åå‰Ωø„Åà„ÄÅÂêÑÊï∞Â≠ó„ÅØ‰∏ÄÂ∫¶„Å†„Åë‰Ωø„Åà„Åæ„Åô„ÄÇ\n",
    "        # ÊÄùËÄÉÈÅéÁ®ã„ÅØ <think> </think> „Çø„Ç∞„ÅßÁ§∫„Åó„ÄÅÊúÄÁµÇÁöÑ„Å™Á≠î„Åà„ÅØ <answer> </answer> „Çø„Ç∞„Åß\n",
    "        # Ëøî„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ‰æã: <answer> (1 + 2) / 3 </answer>„ÄÇ\n",
    "        # Assistant: È†Ü„ÇíËøΩ„Å£„Å¶Ëß£„ÅÑ„Å¶„ÅÑ„Åç„Åæ„Åô„ÄÇ<think>\n",
    "        prefix = f\"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\n",
    "User: Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>.\n",
    "Assistant: Let me solve this step by step.\n",
    "<think>\"\"\"\n",
    "\n",
    "    # Qwen Instruct„É¢„Éá„É´„ÅÆÂ†¥Âêà\n",
    "    elif template_type == 'qwen-instruct':\n",
    "\n",
    "        # <|im_start|>system\n",
    "        # „ÅÇ„Å™„Åü„ÅØÊúâËÉΩ„Å™„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ„Åæ„ÅöÈ†≠„ÅÆ‰∏≠„ÅßÊé®Ë´ñ„Éó„É≠„Çª„Çπ„ÇíËÄÉ„Åà„ÄÅ„Åù„ÅÆÂæå„É¶„Éº„Ç∂„Éº„Å´Á≠î„Åà„Çí‰ºù„Åà„Åæ„Åô„ÄÇ\n",
    "        # <|im_end|>\n",
    "        # <|im_start|>user\n",
    "        # {numbers} „Çí‰Ωø„Å£„Å¶ {target} „Å´„Å™„ÇãÊï∞Âºè„Çí‰Ωú„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "        # ÂõõÂâáÊºîÁÆóÔºà+, -, *, /Ôºâ„Åå‰Ωø„Åà„ÄÅÂêÑÊï∞Â≠ó„ÅØ‰∏ÄÂ∫¶„Å†„Åë‰Ωø„Åà„Åæ„Åô„ÄÇ\n",
    "        # ÊÄùËÄÉÈÅéÁ®ã„ÅØ <think> </think> „Çø„Ç∞„ÅßÁ§∫„Åó„ÄÅÊúÄÁµÇÁöÑ„Å™Á≠î„Åà„ÅØ\n",
    "        #  <answer> </answer> „Çø„Ç∞„ÅßËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "        # ‰æã: <answer> (1 + 2) / 3 </answer>„ÄÇ\n",
    "        # <|im_end|>\n",
    "        # <|im_start|>assistant\n",
    "        # È†Ü„ÇíËøΩ„Å£„Å¶Ëß£„ÅÑ„Å¶„ÅÑ„Åç„Åæ„Åô„ÄÇ\n",
    "        # <think>\n",
    "        prefix = f\"\"\"<|im_start|>system\\nYou are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.<|im_end|>\\n<|im_start|>user\\n Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>.<|im_end|>\\n<|im_start|>assistant\\nLet me solve this step by step.\\n<think>\"\"\"\n",
    "    return prefix\n",
    "\n",
    "make_prefix(train_dataset[0], 'base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_map_fn(split):\n",
    "    \"\"\"\n",
    "    „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂâçÂá¶ÁêÜ„Å´‰ΩøÁî®„Åô„Çã„Éû„ÉÉ„ÉóÈñ¢Êï∞„Çí‰ΩúÊàê„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        split (str): „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂàÜÂâ≤ÂêçÔºà‰æã: 'train', 'test'Ôºâ\n",
    "    Returns:\n",
    "        function: „Éá„Éº„Çø„Éù„Ç§„É≥„Éà„ÇíÂá¶ÁêÜ„Åô„Çã„Éû„ÉÉ„ÉóÈñ¢Êï∞\n",
    "    \"\"\"\n",
    "\n",
    "    def process_fn(example, idx):\n",
    "        \"\"\"\n",
    "        „Éá„Éº„Çø„Éù„Ç§„É≥„Éà„ÇíÂâçÂá¶ÁêÜ„Åô„Çã\n",
    "\n",
    "        Args:\n",
    "            example (dict): „Éá„Éº„Çø„Éù„Ç§„É≥„Éà\n",
    "            idx (int): „Éá„Éº„Çø„Éù„Ç§„É≥„Éà„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ\n",
    "        Returns:\n",
    "            dict: ÂâçÂá¶ÁêÜ„Åï„Çå„Åü„Éá„Éº„Çø„Éù„Ç§„É≥„Éà\n",
    "        \"\"\"\n",
    "\n",
    "        # „Éá„Éº„Çø„Éù„Ç§„É≥„Éà„Åã„Çâ„Éó„É≠„É≥„Éó„Éà„Çí‰ΩúÊàê\n",
    "        question = make_prefix(example, template_type=args.template_type)\n",
    "\n",
    "        solution = {\n",
    "            \"target\": example['target'],\n",
    "            \"numbers\": example['nums']\n",
    "        }\n",
    "\n",
    "        # ÂâçÂá¶ÁêÜ„Åó„Åü„Éá„Éº„Çø„ÇíËæûÊõ∏ÂΩ¢Âºè„ÅßËøî„Åô\n",
    "        data = {\n",
    "            \"data_source\": data_source,\n",
    "            \"prompt\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            }],\n",
    "            \"ability\": \"math\",\n",
    "            \"reward_model\": {\n",
    "                \"style\": \"rule\",\n",
    "                \"ground_truth\": solution\n",
    "            },\n",
    "            \"extra_info\": {\n",
    "                'split': split,\n",
    "                'index': idx,\n",
    "            }\n",
    "        }\n",
    "        return data\n",
    "    return process_fn\n",
    "\n",
    "# Ê§úË®º\n",
    "make_map_fn(\"train\")(train_dataset[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb883d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂâçÂá¶ÁêÜ„Å®ParquetÂΩ¢Âºè„Åß„ÅÆ‰øùÂ≠ò\n",
    "\n",
    "local_dir = args.local_dir\n",
    "OVERWRITE = False\n",
    "\n",
    "if not os.path.exists(os.path.join(local_dir, \"train.parquet\")) or OVERWRITE:\n",
    "    train_dataset = train_dataset.map(function=make_map_fn('train'), with_indices=True)\n",
    "    train_dataset.to_parquet(os.path.join(local_dir, 'train.parquet'))\n",
    "\n",
    "if not os.path.exists(os.path.join(local_dir, \"test.parquet\")) or OVERWRITE:\n",
    "    test_dataset = test_dataset.map(function=make_map_fn('test'), with_indices=True)\n",
    "    test_dataset.to_parquet(os.path.join(local_dir, 'test.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06016cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hadoop File System (HDFS) „Å´„Éá„Éº„Çø„Çí„Ç≥„Éî„Éº\n",
    "# ÂàÜÊï£Âá¶ÁêÜÁî®\n",
    "\n",
    "hdfs_dir = args.hdfs_dir\n",
    "\n",
    "if hdfs_dir is not None:\n",
    "    makedirs(hdfs_dir)\n",
    "    copy(src=local_dir, dst=hdfs_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d4f6cc",
   "metadata": {},
   "source": [
    "## „É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£Èñ¢Êï∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c09d6d3",
   "metadata": {},
   "source": [
    "### „Éï„Ç°„Ç§„É´„Ç∑„Çπ„ÉÜ„É†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f28d1",
   "metadata": {},
   "source": [
    "HDFSÔºàHadoop Distributed File SystemÔºâ„Çí‰ΩøÁî®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee03606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_HDFS_PREFIX = \"hdfs://\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cdab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_non_local(path):\n",
    "    return path.startswith(_HDFS_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88074a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md5_encode(path: str) -> str:\n",
    "    return hashlib.md5(path.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d06dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_temp_path(hdfs_path: str, cache_dir: str) -> str:\n",
    "    \"\"\"Return a local temp path that joins cache_dir and basename of hdfs_path\n",
    "\n",
    "    Args:\n",
    "        hdfs_path:\n",
    "        cache_dir:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # make a base64 encoding of hdfs_path to avoid directory conflict\n",
    "    encoded_hdfs_path = md5_encode(hdfs_path)\n",
    "    temp_dir = os.path.join(cache_dir, encoded_hdfs_path)\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    dst = os.path.join(temp_dir, os.path.basename(hdfs_path))\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy(src: str, dst: str, **kwargs) -> bool:\n",
    "    r\"\"\"Works like shutil.copy() for file, and shutil.copytree for dir, and supports hdfs.\n",
    "\n",
    "    Copy data and mode bits (\"cp src dst\"). Return the file's destination.\n",
    "    The destination may be a directory.\n",
    "    If source and destination are the same file, a SameFileError will be\n",
    "    raised.\n",
    "\n",
    "    Arg:\n",
    "        src (str): source file path\n",
    "        dst (str): destination file path\n",
    "        kwargs: keyword arguments for hdfs copy\n",
    "\n",
    "    Returns:\n",
    "        str: destination file path\n",
    "\n",
    "    \"\"\"\n",
    "    if _is_non_local(src) or _is_non_local(dst):\n",
    "        # TODO(haibin.lin):\n",
    "        # - handle SameFileError for hdfs files(?)\n",
    "        # - return file destination for hdfs files\n",
    "        return _copy(src, dst)\n",
    "    else:\n",
    "        if os.path.isdir(src):\n",
    "            return shutil.copytree(src, dst, **kwargs)\n",
    "        else:\n",
    "            return shutil.copy(src, dst, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f9451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_cmd(cmd: str, timeout=None):\n",
    "    return os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89282bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hdfs_cmd(cmd: str) -> str:\n",
    "    return f\"{_HDFS_BIN_PATH} dfs {cmd}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _copy(from_path: str, to_path: str, timeout: int = None) -> bool:\n",
    "    if to_path.startswith(\"hdfs\"):\n",
    "        if from_path.startswith(\"hdfs\"):\n",
    "            returncode = _run_cmd(_hdfs_cmd(f\"-cp -f {from_path} {to_path}\"), timeout=timeout)\n",
    "        else:\n",
    "            returncode = _run_cmd(_hdfs_cmd(f\"-put -f {from_path} {to_path}\"), timeout=timeout)\n",
    "    else:\n",
    "        if from_path.startswith(\"hdfs\"):\n",
    "            returncode = _run_cmd(_hdfs_cmd(f\"-get \\\n",
    "                {from_path} {to_path}\"), timeout=timeout)\n",
    "        else:\n",
    "            try:\n",
    "                shutil.copy(from_path, to_path)\n",
    "                returncode = 0\n",
    "            except shutil.SameFileError:\n",
    "                returncode = 0\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"copy {from_path} {to_path} failed: {e}\")\n",
    "                returncode = -1\n",
    "    return returncode == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf74c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_local_path_from_hdfs(src: str, cache_dir=None, filelock='.file.lock', verbose=False) -> str:\n",
    "    \"\"\"Copy src from hdfs to local if src is on hdfs or directly return src.\n",
    "    If cache_dir is None, we will use the default cache dir of the system. Note that this may cause conflicts if\n",
    "    the src name is the same between calls\n",
    "\n",
    "    Args:\n",
    "        src (str): a HDFS path of a local path\n",
    "\n",
    "    Returns:\n",
    "        a local path of the copied file\n",
    "    \"\"\"\n",
    "    from filelock import FileLock\n",
    "\n",
    "    assert src[-1] != '/', f'Make sure the last char in src is not / because it will cause error. Got {src}'\n",
    "\n",
    "    if _is_non_local(src):\n",
    "        # download from hdfs to local\n",
    "        if cache_dir is None:\n",
    "            # get a temp folder\n",
    "            cache_dir = tempfile.gettempdir()\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        assert os.path.exists(cache_dir)\n",
    "        local_path = get_local_temp_path(src, cache_dir)\n",
    "        # get a specific lock\n",
    "        filelock = md5_encode(src) + '.lock'\n",
    "        lock_file = os.path.join(cache_dir, filelock)\n",
    "\n",
    "        with FileLock(lock_file=lock_file):\n",
    "            if not os.path.exists(local_path):\n",
    "                if verbose:\n",
    "                    print(f'Copy from {src} to {local_path}')\n",
    "                copy(src, local_path)\n",
    "\n",
    "        return local_path\n",
    "    else:\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af587c0d",
   "metadata": {},
   "source": [
    "### FSDP„ÉØ„Éº„Ç´„Éº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b4fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.distributed\n",
    "import verl.utils.hdfs_io as hdfs_io\n",
    "import verl.utils.torch_functional as verl_F\n",
    "from omegaconf import DictConfig, open_dict\n",
    "from verl import DataProto\n",
    "from verl.single_controller.base import Worker\n",
    "from verl.single_controller.base.decorator import register, Dispatch\n",
    "from verl.utils import hf_tokenizer\n",
    "from verl.utils.debug import log_gpu_memory_usage\n",
    "from verl.utils.fs import copy_local_path_from_hdfs\n",
    "from verl.utils.fsdp_utils import get_fsdp_wrap_policy, offload_fsdp_grad, init_fn, get_init_weight_context_manager\n",
    "from verl.utils.fsdp_utils import offload_fsdp_optimizer, offload_fsdp_param_and_grad, load_fsdp_optimizer, \\\n",
    "    load_fsdp_param_and_grad\n",
    "from verl.utils.import_utils import import_external_libs\n",
    "from verl.utils.model import compute_position_id_with_mask\n",
    "from verl.utils.flops_counter import FlopsCounter\n",
    "from verl.workers.sharding_manager.fsdp_ulysses import FSDPUlyssesShardingManager\n",
    "\n",
    "from codetiming import Timer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ceada",
   "metadata": {},
   "source": [
    "#### ActorRolloutRefWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0ba653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorRolloutRefWorker(Worker):\n",
    "    \"\"\"\n",
    "    This worker can be instantiated as a standalone actor or a standalone rollout or a standalone reference policy\n",
    "    or a hybrid engine based on the config.rollout\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: DictConfig, role: str):\n",
    "        logger.info(f\"ActorRolloutRefWorker„ÇíÂàùÊúüÂåñ {config=} {role=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        import torch.distributed\n",
    "        if not torch.distributed.is_initialized():\n",
    "            torch.distributed.init_process_group(backend=\"nccl\")\n",
    "\n",
    "        # build device mesh for FSDP\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        from torch.distributed.device_mesh import init_device_mesh\n",
    "        # TODO(sgm): support FSDP hybrid shard for larger model\n",
    "        self.device_mesh = init_device_mesh('cuda', mesh_shape=(world_size,), mesh_dim_names=['fsdp'])\n",
    "\n",
    "        # build device mesh for Ulysses Sequence Parallel\n",
    "        self.ulysses_device_mesh = None\n",
    "        self.ulysses_sequence_parallel_size = self.config.actor.get('ulysses_sequence_parallel_size', 1)\n",
    "        dp = world_size // self.ulysses_sequence_parallel_size\n",
    "        if self.ulysses_sequence_parallel_size > 1:\n",
    "            self.ulysses_device_mesh = init_device_mesh('cuda',\n",
    "                                                        mesh_shape=(dp, self.ulysses_sequence_parallel_size),\n",
    "                                                        mesh_dim_names=['dp', 'sp'])\n",
    "\n",
    "        self.ulysses_sharding_manager = FSDPUlyssesShardingManager(self.ulysses_device_mesh)\n",
    "\n",
    "        self.role = role\n",
    "        assert self.role in ['actor', 'rollout', 'ref', 'actor_rollout', 'actor_rollout_ref']\n",
    "\n",
    "        self._is_actor = self.role in ['actor', 'actor_rollout', 'actor_rollout_ref']\n",
    "        self._is_rollout = self.role in ['rollout', 'actor_rollout', 'actor_rollout_ref']\n",
    "        self._is_ref = self.role in ['ref', 'actor_rollout_ref']\n",
    "\n",
    "        self._is_offload_param = False\n",
    "        self._is_offload_grad = False\n",
    "        self._is_offload_optimizer = False\n",
    "        if self._is_actor:\n",
    "            self._is_offload_param = self.config.actor.fsdp_config.get('param_offload', False)\n",
    "            self._is_offload_grad = self.config.actor.fsdp_config.get('grad_offload', False)\n",
    "            self._is_offload_optimizer = self.config.actor.fsdp_config.get('optimizer_offload', False)\n",
    "        elif self._is_ref:\n",
    "            # TODO: it seems that manual offload is slowly than FSDP offload\n",
    "            self._is_offload_param = self.config.ref.fsdp_config.get('param_offload', False)\n",
    "\n",
    "        # normalize config\n",
    "        if self._is_actor:\n",
    "            self.config.actor.ppo_mini_batch_size //= (self.device_mesh.shape[0] // self.ulysses_sequence_parallel_size)\n",
    "            self.config.actor.ppo_micro_batch_size //= (self.device_mesh.shape[0] //\n",
    "                                                        self.ulysses_sequence_parallel_size)\n",
    "            self.config.actor.ppo_mini_batch_size *= self.config.rollout.n\n",
    "            self.config.actor.ppo_micro_batch_size *= self.config.rollout.n\n",
    "        if self._is_rollout:\n",
    "            self.config.rollout.log_prob_micro_batch_size //= (self.device_mesh.shape[0] //\n",
    "                                                               self.ulysses_sequence_parallel_size)\n",
    "            self.config.rollout.log_prob_micro_batch_size *= self.config.rollout.n\n",
    "        if self._is_ref:\n",
    "            self.config.ref.log_prob_micro_batch_size //= (self.device_mesh.shape[0] //\n",
    "                                                           self.ulysses_sequence_parallel_size)\n",
    "            self.config.ref.log_prob_micro_batch_size *= self.config.rollout.n\n",
    "\n",
    "    def _build_model_optimizer(self,\n",
    "                               model_path,\n",
    "                               fsdp_config,\n",
    "                               optim_config,\n",
    "                               override_model_config,\n",
    "                               use_remove_padding=False,\n",
    "                               enable_gradient_checkpointing=False,\n",
    "                               trust_remote_code=False):\n",
    "        from verl.utils.model import print_model_size, update_model_config\n",
    "        from verl.utils.torch_dtypes import PrecisionType\n",
    "        from transformers import AutoModelForCausalLM, AutoConfig\n",
    "        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy, MixedPrecision\n",
    "        from torch import optim\n",
    "\n",
    "        log_gpu_memory_usage('Before init from HF AutoModel', logger=logger)\n",
    "        local_path = copy_local_path_from_hdfs(model_path)\n",
    "\n",
    "        # note that we have to create model in fp32. Otherwise, the optimizer is in bf16, which is incorrect\n",
    "        # TODO(zhangchi.usc1992): 1. support create from random initialized model. 2. Support init with FSDP directly\n",
    "        self.tokenizer = hf_tokenizer(local_path, trust_remote_code=trust_remote_code)\n",
    "\n",
    "        torch_dtype = fsdp_config.get('model_dtype', None)\n",
    "        if torch_dtype is None:\n",
    "            torch_dtype = torch.float32 if self._is_actor else torch.bfloat16\n",
    "        else:\n",
    "            torch_dtype = PrecisionType.to_dtype(torch_dtype)\n",
    "\n",
    "        # override model kwargs\n",
    "        actor_model_config = AutoConfig.from_pretrained(local_path, trust_remote_code=trust_remote_code)\n",
    "\n",
    "        if use_remove_padding:\n",
    "            from verl.models.registry import check_model_support_rmpad\n",
    "            check_model_support_rmpad(actor_model_config.model_type)\n",
    "\n",
    "        if use_remove_padding and self.ulysses_sequence_parallel_size > 1:\n",
    "            from verl.models.transformers.monkey_patch import apply_monkey_patch\n",
    "            apply_monkey_patch(actor_model_config, verbose=True)\n",
    "\n",
    "        override_config_kwargs = {\n",
    "            'bos_token_id': self.tokenizer.bos_token_id,\n",
    "            'eos_token_id': self.tokenizer.eos_token_id,\n",
    "            'pad_token_id': self.tokenizer.pad_token_id,\n",
    "        }\n",
    "        override_config_kwargs.update(override_model_config)\n",
    "        update_model_config(actor_model_config, override_config_kwargs=override_config_kwargs)\n",
    "        if self.rank == 0:\n",
    "            print(f'Model config after override: {actor_model_config}')\n",
    "\n",
    "        # NOTE(fix me): tie_word_embedding causes meta_tensor init to hang\n",
    "        init_context = get_init_weight_context_manager(use_meta_tensor=not actor_model_config.tie_word_embeddings)\n",
    "\n",
    "        with init_context(), warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            actor_module = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=local_path,\n",
    "                                                                torch_dtype=torch_dtype,\n",
    "                                                                config=actor_model_config,\n",
    "                                                                attn_implementation='flash_attention_2',\n",
    "                                                                trust_remote_code=trust_remote_code)\n",
    "            # some parameters may not in torch_dtype. TODO(zhangchi.usc1992) remove this after we switch to fsdp2\n",
    "            actor_module.to(torch_dtype)\n",
    "\n",
    "            if enable_gradient_checkpointing:\n",
    "                actor_module.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant': False})\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "        if self.rank == 0:\n",
    "            print_model_size(actor_module)\n",
    "\n",
    "        log_gpu_memory_usage('After init from HF AutoModel', logger=logger)\n",
    "\n",
    "        # We wrap FSDP for rollout as well\n",
    "        mixed_precision_config = fsdp_config.get('mixed_precision', None)\n",
    "        if mixed_precision_config is not None:\n",
    "            param_dtype = PrecisionType.to_dtype(mixed_precision_config.get('param_dtype', 'bf16'))\n",
    "            reduce_dtype = PrecisionType.to_dtype(mixed_precision_config.get('reduce_dtype', 'fp32'))\n",
    "            buffer_dtype = PrecisionType.to_dtype(mixed_precision_config.get('buffer_dtype', 'fp32'))\n",
    "        else:\n",
    "            param_dtype = torch.bfloat16\n",
    "            reduce_dtype = torch.float32\n",
    "            buffer_dtype = torch.float32\n",
    "\n",
    "        mixed_precision = MixedPrecision(param_dtype=param_dtype, reduce_dtype=reduce_dtype, buffer_dtype=buffer_dtype)\n",
    "\n",
    "        if self._is_ref:\n",
    "            mixed_precision = None\n",
    "\n",
    "        auto_wrap_policy = get_fsdp_wrap_policy(module=actor_module, config=fsdp_config.get('wrap_policy', None))\n",
    "\n",
    "        if self._is_rollout and self.config.rollout.name == 'hf':\n",
    "            # TODO(zhangchi.usc1992, shengguangming) fix me. Current, auto_wrap_policy causes HFRollout to hang in Gemma\n",
    "            auto_wrap_policy = None\n",
    "\n",
    "        print(f'wrap_policy: {auto_wrap_policy}')\n",
    "\n",
    "        # TODO(sgm): support hybrid\n",
    "        if auto_wrap_policy is None:\n",
    "            sharding_strategy = ShardingStrategy.SHARD_GRAD_OP\n",
    "        else:\n",
    "            sharding_strategy = ShardingStrategy.FULL_SHARD\n",
    "\n",
    "        # TODO: add transformer policy\n",
    "        actor_module_fsdp = FSDP(\n",
    "            actor_module,\n",
    "            param_init_fn=init_fn,\n",
    "            use_orig_params=False,\n",
    "            auto_wrap_policy=auto_wrap_policy,\n",
    "            device_id=torch.cuda.current_device(),\n",
    "            sharding_strategy=sharding_strategy,  # zero3\n",
    "            mixed_precision=mixed_precision,\n",
    "            sync_module_states=True,\n",
    "            device_mesh=self.device_mesh,\n",
    "            forward_prefetch=False)\n",
    "\n",
    "        log_gpu_memory_usage('After Actor FSDP init', logger=logger)\n",
    "\n",
    "        # TODO: add more optimizer args into config\n",
    "        if self._is_actor:\n",
    "            from verl.utils.torch_functional import get_constant_schedule_with_warmup\n",
    "            actor_optimizer = optim.AdamW(actor_module_fsdp.parameters(),\n",
    "                                          lr=optim_config.lr,\n",
    "                                          betas=optim_config.get('betas', (0.9, 0.999)),\n",
    "                                          weight_decay=optim_config.get('weight_decay', 1e-2))\n",
    "\n",
    "            total_steps = optim_config.get('total_training_steps', 0)\n",
    "            num_warmup_steps_ratio = optim_config.get('lr_warmup_steps_ratio', 0.)\n",
    "            num_warmup_steps = int(num_warmup_steps_ratio * total_steps)\n",
    "\n",
    "            print(f'Total steps: {total_steps}, num_warmup_steps: {num_warmup_steps}')\n",
    "\n",
    "            actor_lr_scheduler = get_constant_schedule_with_warmup(optimizer=actor_optimizer,\n",
    "                                                                   num_warmup_steps=num_warmup_steps)\n",
    "        else:\n",
    "            actor_optimizer = None\n",
    "            actor_lr_scheduler = None\n",
    "\n",
    "        log_gpu_memory_usage('After actor optimizer init', logger=logger)\n",
    "\n",
    "        return actor_module_fsdp, actor_optimizer, actor_lr_scheduler, actor_model_config\n",
    "\n",
    "    def _build_rollout(self):\n",
    "        from torch.distributed.device_mesh import init_device_mesh\n",
    "        # TODO(sgm): support FSDP hybrid shard for larger model\n",
    "        infer_tp = self.config.rollout.tensor_model_parallel_size\n",
    "        dp = self.world_size // infer_tp\n",
    "        assert self.world_size % infer_tp == 0, f'rollout world_size: {self.world_size} is not divisible by infer_tp: {infer_tp}'\n",
    "        rollout_device_mesh = init_device_mesh('cuda', mesh_shape=(dp, infer_tp), mesh_dim_names=['dp', 'infer_tp'])\n",
    "\n",
    "        if self.config.rollout.name == 'hf':\n",
    "            from verl.workers.rollout import HFRollout\n",
    "            from verl.workers.sharding_manager import BaseShardingManager\n",
    "            rollout = HFRollout(module=self.actor_module_fsdp, config=self.config.rollout)\n",
    "            rollout_sharding_manager = BaseShardingManager()\n",
    "            # TODO: a sharding manager that do nothing?\n",
    "        elif self.config.rollout.name == 'vllm':\n",
    "            from verl.workers.rollout.vllm_rollout import vLLMRollout\n",
    "            from verl.workers.sharding_manager import FSDPVLLMShardingManager\n",
    "            log_gpu_memory_usage('Before building vllm rollout', logger=None)\n",
    "            rollout = vLLMRollout(actor_module=self.actor_module_fsdp,\n",
    "                                  config=self.config.rollout,\n",
    "                                  tokenizer=self.tokenizer,\n",
    "                                  model_hf_config=self.actor_model_config)\n",
    "            log_gpu_memory_usage('After building vllm rollout', logger=None)\n",
    "            if torch.distributed.get_world_size() == 1:\n",
    "                self.config.rollout.load_format = 'dummy_hf'\n",
    "            rollout_sharding_manager = FSDPVLLMShardingManager(module=self.actor_module_fsdp,\n",
    "                                                               inference_engine=rollout.inference_engine,\n",
    "                                                               model_config=self.actor_model_config,\n",
    "                                                               full_params='hf' in self.config.rollout.load_format,\n",
    "                                                               device_mesh=rollout_device_mesh)\n",
    "            log_gpu_memory_usage('After building sharding manager', logger=None)\n",
    "\n",
    "        return rollout, rollout_sharding_manager\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.ONE_TO_ALL)\n",
    "    def init_model(self):\n",
    "        from verl.workers.actor import DataParallelPPOActor\n",
    "        # This is used to import external_lib into the huggingface systems\n",
    "        import_external_libs(self.config.model.get('external_lib', None))\n",
    "\n",
    "        from omegaconf import OmegaConf\n",
    "        override_model_config = OmegaConf.to_container(self.config.model.get('override_config', OmegaConf.create()))\n",
    "\n",
    "        use_remove_padding = self.config.model.get('use_remove_padding', False)\n",
    "\n",
    "        if self._is_actor or self._is_rollout:\n",
    "            # we need the model for actor and rollout\n",
    "            if self._is_actor:\n",
    "                optim_config = self.config.actor.optim\n",
    "                fsdp_config = self.config.actor.fsdp_config\n",
    "            else:\n",
    "                optim_config = None\n",
    "                fsdp_config = OmegaConf.create()\n",
    "            self.actor_module_fsdp, self.actor_optimizer, self.actor_lr_scheduler, self.actor_model_config = self._build_model_optimizer(\n",
    "                model_path=self.config.model.path,\n",
    "                fsdp_config=fsdp_config,\n",
    "                optim_config=optim_config,\n",
    "                override_model_config=override_model_config,\n",
    "                use_remove_padding=use_remove_padding,\n",
    "                enable_gradient_checkpointing=self.config.model.get('enable_gradient_checkpointing', False),\n",
    "                trust_remote_code=self.config.model.get('trust_remote_code', False))\n",
    "\n",
    "            # get the original unwrapped module\n",
    "            self.actor_module = self.actor_module_fsdp._fsdp_wrapped_module\n",
    "\n",
    "            if self._is_offload_param:\n",
    "                # param is require during state_dict in sharding manager\n",
    "                offload_fsdp_grad(module=self.actor_module_fsdp)\n",
    "                log_gpu_memory_usage('After offload actor grad during init', logger=logger)\n",
    "            if self._is_offload_optimizer:\n",
    "                offload_fsdp_optimizer(optimizer=self.actor_optimizer)\n",
    "                log_gpu_memory_usage('After offload actor optimizer during init', logger=logger)\n",
    "        # load from checkpoint\n",
    "        if self._is_actor:\n",
    "            OmegaConf.set_struct(self.config.actor, True)\n",
    "            with open_dict(self.config.actor):\n",
    "                self.config.actor.use_remove_padding = use_remove_padding\n",
    "            self.actor = DataParallelPPOActor(config=self.config.actor,\n",
    "                                              actor_module=self.actor_module_fsdp,\n",
    "                                              actor_optimizer=self.actor_optimizer)\n",
    "\n",
    "        if self._is_rollout:\n",
    "            self.rollout, self.rollout_sharding_manager = self._build_rollout()\n",
    "\n",
    "        if self._is_ref:\n",
    "            self.ref_module_fsdp = self._build_model_optimizer(model_path=self.config.model.path,\n",
    "                                                               fsdp_config=self.config.ref.fsdp_config,\n",
    "                                                               optim_config=None,\n",
    "                                                               override_model_config=override_model_config,\n",
    "                                                               use_remove_padding=use_remove_padding,\n",
    "                                                               trust_remote_code=self.config.model.get(\n",
    "                                                                   'trust_remote_code', False))[0]\n",
    "            if self._is_offload_param:\n",
    "                offload_fsdp_param_and_grad(module=self.ref_module_fsdp, offload_grad=self._is_offload_grad)\n",
    "\n",
    "            OmegaConf.set_struct(self.config.ref, True)\n",
    "            with open_dict(self.config.ref):\n",
    "                self.config.ref.use_remove_padding = use_remove_padding\n",
    "            self.ref_policy = DataParallelPPOActor(config=self.config.ref, actor_module=self.ref_module_fsdp)\n",
    "\n",
    "        if self._is_actor:\n",
    "            self.flops_counter = FlopsCounter(self.actor_model_config)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\n",
    "    def update_actor(self, data: DataProto):\n",
    "        data = data.to('cuda')\n",
    "\n",
    "        assert self._is_actor\n",
    "        if self._is_offload_param:\n",
    "            load_fsdp_param_and_grad(module=self.actor_module_fsdp,\n",
    "                                     device_id=torch.cuda.current_device(),\n",
    "                                     load_grad=self._is_offload_grad)\n",
    "        if self._is_offload_optimizer:\n",
    "            load_fsdp_optimizer(optimizer=self.actor_optimizer, device_id=torch.cuda.current_device())\n",
    "\n",
    "        data.batch = data.batch.cuda()\n",
    "\n",
    "        log_gpu_memory_usage('Before update policy', logger=logger)\n",
    "\n",
    "        with self.ulysses_sharding_manager:\n",
    "            data = self.ulysses_sharding_manager.preprocess_data(data=data)\n",
    "            # perform training\n",
    "            with Timer(name='update_policy', logger=None) as timer:\n",
    "                metrics = self.actor.update_policy(data=data)\n",
    "            delta_time = timer.last\n",
    "            global_num_tokens = data.meta_info['global_token_num']\n",
    "            estimated_flops, promised_flops = self.flops_counter.estimate_flops(global_num_tokens, delta_time)\n",
    "            metrics['mfu/actor'] = estimated_flops * self.config.actor.ppo_epochs / promised_flops / self.world_size\n",
    "\n",
    "            self.actor_lr_scheduler.step()\n",
    "            lr = self.actor_lr_scheduler.get_last_lr()[0]\n",
    "            metrics['actor/lr'] = lr\n",
    "\n",
    "            log_gpu_memory_usage('After update policy', logger=logger)\n",
    "\n",
    "            # TODO: here, we should return all metrics\n",
    "            output = DataProto(meta_info={'metrics': metrics})\n",
    "\n",
    "            output = self.ulysses_sharding_manager.postprocess_data(data=output)\n",
    "            output = output.to('cpu')\n",
    "\n",
    "        if self._is_offload_param:\n",
    "            offload_fsdp_param_and_grad(module=self.actor_module_fsdp, offload_grad=self._is_offload_grad)\n",
    "        if self._is_offload_optimizer:\n",
    "            offload_fsdp_optimizer(optimizer=self.actor_optimizer)\n",
    "        torch.cuda.empty_cache()\n",
    "        return output\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\n",
    "    def generate_sequences(self, prompts: DataProto):\n",
    "        prompts = prompts.to('cuda')\n",
    "        # set to False if it is validation\n",
    "        recompute_log_prob = prompts.meta_info.get('recompute_log_prob', True)\n",
    "\n",
    "        assert self._is_rollout\n",
    "        if self._is_offload_param:\n",
    "            load_fsdp_param_and_grad(module=self.actor_module_fsdp,\n",
    "                                     device_id=torch.cuda.current_device(),\n",
    "                                     load_grad=self._is_offload_grad)\n",
    "\n",
    "        prompts.batch = prompts.batch.cuda()\n",
    "        meta_info = {'eos_token_id': self.tokenizer.eos_token_id, 'pad_token_id': self.tokenizer.pad_token_id}\n",
    "        prompts.meta_info.update(meta_info)\n",
    "        with self.rollout_sharding_manager:\n",
    "            log_gpu_memory_usage('After entering rollout sharding manager', logger=logger)\n",
    "\n",
    "            prompts = self.rollout_sharding_manager.preprocess_data(prompts)\n",
    "            output = self.rollout.generate_sequences(prompts=prompts)\n",
    "\n",
    "            log_gpu_memory_usage('After rollout generation', logger=logger)\n",
    "\n",
    "            output = self.rollout_sharding_manager.postprocess_data(output)\n",
    "\n",
    "        if self._is_actor and recompute_log_prob:\n",
    "            # we should always recompute old_log_probs when it is HybridEngine\n",
    "            output.meta_info['micro_batch_size'] = self.config.rollout.log_prob_micro_batch_size\n",
    "            output.meta_info['max_token_len'] = self.config.rollout.log_prob_max_token_len_per_gpu\n",
    "            output.meta_info['use_dynamic_bsz'] = self.config.rollout.log_prob_use_dynamic_bsz\n",
    "            output.meta_info['temperature'] = self.config.rollout.temperature\n",
    "            # perform recompute log_prob\n",
    "            with self.ulysses_sharding_manager:\n",
    "                output = self.ulysses_sharding_manager.preprocess_data(output)\n",
    "                old_log_probs = self.actor.compute_log_prob(data=output)\n",
    "                output.batch['old_log_probs'] = old_log_probs\n",
    "                output = self.ulysses_sharding_manager.postprocess_data(output)\n",
    "\n",
    "        output = output.to('cpu')\n",
    "\n",
    "        if self._is_offload_param:\n",
    "            # NOTE(sgm): the grad is already in CPU, only offload param here\n",
    "            offload_fsdp_param_and_grad(module=self.actor_module_fsdp, offload_grad=self._is_offload_grad)\n",
    "        # clear kv cache\n",
    "        torch.cuda.empty_cache()\n",
    "        log_gpu_memory_usage('After recompute log prob', logger=logger)\n",
    "        return output\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\n",
    "    def compute_ref_log_prob(self, data: DataProto):\n",
    "        assert self._is_ref\n",
    "\n",
    "        data = data.to('cuda')\n",
    "\n",
    "        if self._is_offload_param:\n",
    "            load_fsdp_param_and_grad(module=self.ref_module_fsdp,\n",
    "                                     device_id=torch.cuda.current_device(),\n",
    "                                     load_grad=self._is_offload_grad)\n",
    "\n",
    "        micro_batch_size = self.config.ref.log_prob_micro_batch_size\n",
    "        data.meta_info['micro_batch_size'] = micro_batch_size\n",
    "        data.meta_info['temperature'] = self.config.rollout.temperature\n",
    "        data.meta_info['max_token_len'] = self.config.ref.log_prob_max_token_len_per_gpu\n",
    "        data.meta_info['use_dynamic_bsz'] = self.config.ref.log_prob_use_dynamic_bsz\n",
    "        with self.ulysses_sharding_manager:\n",
    "            data = self.ulysses_sharding_manager.preprocess_data(data)\n",
    "            output = self.ref_policy.compute_log_prob(data=data)\n",
    "            output = DataProto.from_dict(tensors={'ref_log_prob': output})\n",
    "            output = self.ulysses_sharding_manager.postprocess_data(output)\n",
    "\n",
    "        output = output.to('cpu')\n",
    "\n",
    "        if self._is_offload_param:\n",
    "            offload_fsdp_param_and_grad(module=self.ref_module_fsdp, offload_grad=self._is_offload_grad)\n",
    "        torch.cuda.empty_cache()\n",
    "        return output\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.ONE_TO_ALL)\n",
    "    def save_checkpoint(self, local_path, hdfs_path=None):\n",
    "        assert self._is_actor\n",
    "        import torch\n",
    "        if self._is_offload_param:\n",
    "            load_fsdp_param_and_grad(module=self.actor_module_fsdp,\n",
    "                                     device_id=torch.cuda.current_device(),\n",
    "                                     load_grad=self._is_offload_grad)\n",
    "\n",
    "        # TODO: support DCP and save sharded checkpoints\n",
    "        import torch.distributed\n",
    "        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, StateDictType, FullStateDictConfig\n",
    "        cfg = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n",
    "        with FSDP.state_dict_type(self.actor.actor_module, StateDictType.FULL_STATE_DICT, cfg):\n",
    "            state_dict = self.actor.actor_module.state_dict()\n",
    "        if self.rank == 0:\n",
    "            print(f'Saving actor checkpoint to {local_path}')\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "            self.actor_module.save_pretrained(local_path, state_dict=state_dict)\n",
    "            self.tokenizer.save_pretrained(local_path)\n",
    "            if hdfs_path is not None:\n",
    "                print(f'Uploading actor checkpoint to {hdfs_path}')\n",
    "                hdfs_io.makedirs(hdfs_path, exist_ok=True)\n",
    "                hdfs_io.copy(src=local_path, dst=hdfs_path)\n",
    "\n",
    "        torch.distributed.barrier()\n",
    "        if self._is_offload_param:\n",
    "            offload_fsdp_param_and_grad(module=self.actor_module_fsdp, offload_grad=self._is_offload_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e5d99",
   "metadata": {},
   "source": [
    "#### CriticWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8286af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticWorker(Worker):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        logger.info(f\"CriticWorker„ÇíÂàùÊúüÂåñ {config=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        import torch.distributed\n",
    "        if not torch.distributed.is_initialized():\n",
    "            torch.distributed.init_process_group(backend=\"nccl\")\n",
    "        self.config = config\n",
    "\n",
    "        # build device mesh for Ulysses Sequence Parallel\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        from torch.distributed.device_mesh import init_device_mesh\n",
    "        self.ulysses_device_mesh = None\n",
    "        self.ulysses_sequence_parallel_size = self.config.get('ulysses_sequence_parallel_size', 1)\n",
    "        dp = world_size // self.ulysses_sequence_parallel_size\n",
    "        if self.ulysses_sequence_parallel_size > 1:\n",
    "            self.ulysses_device_mesh = init_device_mesh('cuda',\n",
    "                                                        mesh_shape=(dp, self.ulysses_sequence_parallel_size),\n",
    "                                                        mesh_dim_names=['dp', 'sp'])\n",
    "\n",
    "        self.ulysses_sharding_manager = FSDPUlyssesShardingManager(self.ulysses_device_mesh)\n",
    "\n",
    "        # set FSDP offload params\n",
    "        self._is_offload_param = self.config.model.fsdp_config.param_offload\n",
    "        self._is_offload_grad = self.config.model.fsdp_config.grad_offload\n",
    "        self._is_offload_optimizer = self.config.model.fsdp_config.optimizer_offload\n",
    "\n",
    "        # normalize config\n",
    "        self.config.ppo_mini_batch_size //= (torch.distributed.get_world_size() // self.ulysses_sequence_parallel_size)\n",
    "        self.config.ppo_micro_batch_size //= (torch.distributed.get_world_size() // self.ulysses_sequence_parallel_size)\n",
    "        self.config.forward_micro_batch_size //= (torch.distributed.get_world_size() //\n",
    "                                                  self.ulysses_sequence_parallel_size)\n",
    "\n",
    "    def _build_critic_model_optimizer(self, config):\n",
    "        # the following line is necessary\n",
    "        from verl.utils.model import LambdaLayer, print_model_size, squeeze\n",
    "        from verl.utils.torch_dtypes import PrecisionType\n",
    "        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy, MixedPrecision\n",
    "        from torch import optim\n",
    "\n",
    "        local_path = copy_local_path_from_hdfs(config.model.path)\n",
    "        # note that the tokenizer between actor and critic may be different. So override tokenizer info with actor info\n",
    "        # using random initialized model from any architecture. May not be the same as Actor.\n",
    "\n",
    "        tokenizer_path = copy_local_path_from_hdfs(config.model.tokenizer_path)\n",
    "        self.tokenizer = hf_tokenizer(tokenizer_path, trust_remote_code=config.model.get('trust_remote_code', False))\n",
    "\n",
    "        from omegaconf import OmegaConf\n",
    "        override_config = OmegaConf.to_container(self.config.model.get('override_config', OmegaConf.create()))\n",
    "        override_config_kwargs = {\n",
    "            'bos_token_id': self.tokenizer.bos_token_id,\n",
    "            'eos_token_id': self.tokenizer.eos_token_id,\n",
    "            'pad_token_id': self.tokenizer.pad_token_id,\n",
    "        }\n",
    "        override_config_kwargs.update(override_config)\n",
    "        if self.rank == 0:\n",
    "            print(f'Critic overriding config {override_config_kwargs}')\n",
    "\n",
    "        torch_dtype = self.config.model.fsdp_config.get('model_dtype', 'fp32')\n",
    "        torch_dtype = PrecisionType.to_dtype(torch_dtype)\n",
    "\n",
    "        from transformers import AutoConfig, AutoModelForTokenClassification\n",
    "        from torch import nn\n",
    "\n",
    "        trust_remote_code = False\n",
    "        critic_model_config = AutoConfig.from_pretrained(local_path, trust_remote_code=trust_remote_code)\n",
    "        critic_model_config.num_labels = 1\n",
    "\n",
    "        use_remove_padding = config.model.get('use_remove_padding', False)\n",
    "        if use_remove_padding:\n",
    "            from verl.models.registry import check_model_support_rmpad\n",
    "            check_model_support_rmpad(critic_model_config.model_type)\n",
    "\n",
    "        if use_remove_padding and self.ulysses_sequence_parallel_size > 1:\n",
    "            from verl.models.transformers.monkey_patch import apply_monkey_patch\n",
    "            apply_monkey_patch(critic_model_config, verbose=True)\n",
    "\n",
    "        init_context = get_init_weight_context_manager()\n",
    "        with init_context(), warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            setattr(critic_model_config, 'classifier_dropout', 0.)\n",
    "            setattr(critic_model_config, 'hidden_dropout', '0')\n",
    "            critic_module = AutoModelForTokenClassification.from_pretrained(pretrained_model_name_or_path=local_path,\n",
    "                                                                            torch_dtype=torch_dtype,\n",
    "                                                                            config=critic_model_config,\n",
    "                                                                            attn_implementation='flash_attention_2',\n",
    "                                                                            trust_remote_code=trust_remote_code)\n",
    "\n",
    "            # some parameters may not in torch_dtype\n",
    "            critic_module.to(torch_dtype)\n",
    "\n",
    "            if config.model.get('enable_gradient_checkpointing', False):\n",
    "                critic_module.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant': False})\n",
    "        if self.rank == 0:\n",
    "            print_model_size(critic_module)\n",
    "\n",
    "        self.critic_model_config = critic_model_config\n",
    "\n",
    "        fsdp_config = self.config.model.fsdp_config\n",
    "        mixed_precision_config = fsdp_config.get('mixed_precision', None)\n",
    "        if mixed_precision_config is not None:\n",
    "            param_dtype = PrecisionType.to_dtype(mixed_precision_config.get('param_dtype', 'bf16'))\n",
    "            reduce_dtype = PrecisionType.to_dtype(mixed_precision_config.get('reduce_dtype', 'fp32'))\n",
    "            buffer_dtype = PrecisionType.to_dtype(mixed_precision_config.get('buffer_dtype', 'fp32'))\n",
    "        else:\n",
    "            param_dtype = torch.bfloat16\n",
    "            reduce_dtype = torch.float32\n",
    "            buffer_dtype = torch.float32\n",
    "\n",
    "        mixed_precision = MixedPrecision(param_dtype=param_dtype, reduce_dtype=reduce_dtype, buffer_dtype=buffer_dtype)\n",
    "\n",
    "        auto_wrap_policy = get_fsdp_wrap_policy(module=critic_module, config=self.config.model.fsdp_config.wrap_policy)\n",
    "\n",
    "        log_gpu_memory_usage('Before critic FSDP', logger=None)\n",
    "\n",
    "        critic_module = FSDP(critic_module,\n",
    "                             param_init_fn=init_fn,\n",
    "                             use_orig_params=False,\n",
    "                             auto_wrap_policy=auto_wrap_policy,\n",
    "                             device_id=torch.cuda.current_device(),\n",
    "                             sharding_strategy=ShardingStrategy.FULL_SHARD,\n",
    "                             mixed_precision=mixed_precision,\n",
    "                             sync_module_states=True,\n",
    "                             forward_prefetch=False)\n",
    "\n",
    "        log_gpu_memory_usage('After critic FSDP', logger=None)\n",
    "\n",
    "        critic_optimizer = optim.AdamW(critic_module.parameters(),\n",
    "                                       lr=config.optim.lr,\n",
    "                                       betas=config.optim.get('betas', (0.9, 0.999)),\n",
    "                                       weight_decay=config.optim.get('weight_decay', 1e-2))\n",
    "\n",
    "        total_steps = config.optim.get('total_training_steps', 0)\n",
    "        num_warmup_steps_ratio = config.optim.get('lr_warmup_steps_ratio', 0.)\n",
    "        num_warmup_steps = int(num_warmup_steps_ratio * total_steps)\n",
    "\n",
    "        print(f'Total steps: {total_steps}, num_warmup_steps: {num_warmup_steps}')\n",
    "\n",
    "        from verl.utils.torch_functional import get_constant_schedule_with_warmup\n",
    "        critic_lr_scheduler = get_constant_schedule_with_warmup(optimizer=critic_optimizer,\n",
    "                                                                num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "        return critic_module, critic_optimizer, critic_lr_scheduler\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.ONE_TO_ALL)\n",
    "    def init_model(self):\n",
    "        # This is used to import external_lib into the huggingface systems\n",
    "        import_external_libs(self.config.model.get('external_lib', None))\n",
    "\n",
    "        from verl.workers.critic import DataParallelPPOCritic\n",
    "        self.critic_module, self.critic_optimizer, self.critic_lr_scheduler = self._build_critic_model_optimizer(\n",
    "            self.config)\n",
    "\n",
    "        if self._is_offload_param:\n",
    "            offload_fsdp_param_and_grad(module=self.critic_module, offload_grad=self._is_offload_grad)\n",
    "        if self._is_offload_optimizer:\n",
    "            offload_fsdp_optimizer(optimizer=self.critic_optimizer)\n",
    "\n",
    "        self.critic = DataParallelPPOCritic(config=self.config,\n",
    "                                            critic_module=self.critic_module,\n",
    "                                            critic_optimizer=self.critic_optimizer)\n",
    "\n",
    "        self.flops_counter = FlopsCounter(self.critic_model_config)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\n",
    "    def compute_values(self, data: DataProto):\n",
    "        data = data.to('cuda')\n",
    "\n",
    "        if self._is_offload_param:\n",
    "            load_fsdp_param_and_grad(module=self.critic_module,\n",
    "                                     device_id=torch.cuda.current_device(),\n",
    "                                     load_grad=self._is_offload_grad)\n",
    "        micro_batch_size = self.config.forward_micro_batch_size\n",
    "        data.meta_info['micro_batch_size'] = micro_batch_size\n",
    "        data.meta_info['max_token_len'] = self.config.forward_max_token_len_per_gpu\n",
    "        data.meta_info['use_dynamic_bsz'] = self.config.use_dynamic_bsz\n",
    "        # perform forward computation\n",
    "        with self.ulysses_sharding_manager:\n",
    "            data = self.ulysses_sharding_manager.preprocess_data(data=data)\n",
    "            values = self.critic.compute_values(data=data)\n",
    "            output = DataProto.from_dict(tensors={'values': values})\n",
    "            output = self.ulysses_sharding_manager.postprocess_data(data=output)\n",
    "\n",
    "        output = output.to('cpu')\n",
    "        if self._is_offload_param:\n",
    "            offload_fsdp_param_and_grad(module=self.critic_module, offload_grad=self._is_offload_grad)\n",
    "        torch.cuda.empty_cache()\n",
    "        return output\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\n",
    "    def update_critic(self, data: DataProto):\n",
    "        data = data.to('cuda')\n",
    "        if self._is_offload_param:\n",
    "            load_fsdp_param_and_grad(module=self.critic_module,\n",
    "                                     device_id=torch.cuda.current_device(),\n",
    "                                     load_grad=self._is_offload_grad)\n",
    "        if self._is_offload_optimizer:\n",
    "            load_fsdp_optimizer(optimizer=self.critic_optimizer, device_id=torch.cuda.current_device())\n",
    "\n",
    "        # perform forward computation\n",
    "        with self.ulysses_sharding_manager:\n",
    "            data = self.ulysses_sharding_manager.preprocess_data(data=data)\n",
    "\n",
    "            with Timer(name='update_critic', logger=None) as timer:\n",
    "                metrics = self.critic.update_critic(data=data)\n",
    "            delta_time = timer.last\n",
    "\n",
    "            global_num_tokens = data.meta_info['global_token_num']\n",
    "            estimated_flops, promised_flops = self.flops_counter.estimate_flops(global_num_tokens, delta_time)\n",
    "            metrics['mfu/critic'] = estimated_flops * self.config.ppo_epochs / promised_flops / self.world_size\n",
    "\n",
    "            self.critic_lr_scheduler.step()\n",
    "            lr = self.critic_lr_scheduler.get_last_lr()[0]\n",
    "            metrics['critic/lr'] = lr\n",
    "\n",
    "            output = DataProto(batch=None, meta_info={'metrics': metrics})\n",
    "            output = self.ulysses_sharding_manager.postprocess_data(data=output)\n",
    "\n",
    "        if self._is_offload_param:\n",
    "            offload_fsdp_param_and_grad(module=self.critic_module, offload_grad=self._is_offload_grad)\n",
    "        if self._is_offload_optimizer:\n",
    "            offload_fsdp_optimizer(optimizer=self.critic_optimizer)\n",
    "        torch.cuda.empty_cache()\n",
    "        output = output.to('cpu')\n",
    "        return output\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.ONE_TO_ALL)\n",
    "    def save_checkpoint(self, local_path, hdfs_path=None):\n",
    "        import torch\n",
    "        if self._is_offload_param:\n",
    "            load_fsdp_param_and_grad(module=self.critic_module,\n",
    "                                     device_id=torch.cuda.current_device(),\n",
    "                                     load_grad=self._is_offload_grad)\n",
    "\n",
    "        # TODO: support DCP and save sharded checkpoints\n",
    "        import torch.distributed\n",
    "        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, StateDictType, FullStateDictConfig\n",
    "        cfg = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n",
    "        with FSDP.state_dict_type(self.critic_module, StateDictType.FULL_STATE_DICT, cfg):\n",
    "            state_dict = self.critic_module.state_dict()\n",
    "        if self.rank == 0:\n",
    "            print(f'Saving critic checkpoint to {local_path}')\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "            self.critic_module._fsdp_wrapped_module.save_pretrained(local_path, state_dict=state_dict)\n",
    "            self.tokenizer.save_pretrained(local_path)\n",
    "            if hdfs_path is not None:\n",
    "                print(f'Uploading critic checkpoint to {hdfs_path}')\n",
    "                hdfs_io.makedirs(hdfs_path, exist_ok=True)\n",
    "                hdfs_io.copy(src=local_path, dst=hdfs_path)\n",
    "\n",
    "        torch.distributed.barrier()\n",
    "        if self._is_offload_param:\n",
    "            offload_fsdp_param_and_grad(module=self.critic_module, offload_grad=self._is_offload_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a3599c",
   "metadata": {},
   "source": [
    "#### RewardModelWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(sgm): we may need to extract it to dp_reward_model.py\n",
    "class RewardModelWorker(Worker):\n",
    "    \"\"\"\n",
    "    Note that we only implement the reward model that is subclass of AutoModelForTokenClassification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        logger.info(f\"RewardModelWorker„ÇíÂàùÊúüÂåñ {config=}\")\n",
    "\n",
    "        import torch.distributed\n",
    "        if not torch.distributed.is_initialized():\n",
    "            torch.distributed.init_process_group(backend=\"nccl\")\n",
    "        self.config = config\n",
    "\n",
    "        # build device mesh for Ulysses Sequence Parallel\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        from torch.distributed.device_mesh import init_device_mesh\n",
    "        self.ulysses_device_mesh = None\n",
    "        self.ulysses_sequence_parallel_size = self.config.get('ulysses_sequence_parallel_size', 1)\n",
    "        dp = world_size // self.ulysses_sequence_parallel_size\n",
    "        if self.ulysses_sequence_parallel_size > 1:\n",
    "            self.ulysses_device_mesh = init_device_mesh('cuda',\n",
    "                                                        mesh_shape=(dp, self.ulysses_sequence_parallel_size),\n",
    "                                                        mesh_dim_names=['dp', 'sp'])\n",
    "\n",
    "        self.ulysses_sharding_manager = FSDPUlyssesShardingManager(self.ulysses_device_mesh)\n",
    "\n",
    "        self.use_remove_padding = self.config.model.get('use_remove_padding', False)\n",
    "        self.config.micro_batch_size //= torch.distributed.get_world_size()\n",
    "\n",
    "    def _build_model(self, config):\n",
    "        # the following line is necessary\n",
    "        from transformers import AutoModelForTokenClassification, AutoConfig\n",
    "        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy, CPUOffload\n",
    "\n",
    "        # download the checkpoint from hdfs\n",
    "        local_path = copy_local_path_from_hdfs(config.model.path)\n",
    "\n",
    "        if self.config.model.input_tokenizer is None:\n",
    "            self._do_switch_chat_template = False\n",
    "        else:\n",
    "            self._do_switch_chat_template = True\n",
    "            input_tokenizer_local_path = copy_local_path_from_hdfs(config.model.input_tokenizer)\n",
    "            self.input_tokenizer = hf_tokenizer(input_tokenizer_local_path,\n",
    "                                                trust_remote_code=config.model.get('trust_remote_code', False))\n",
    "            self.tokenizer = hf_tokenizer(local_path, trust_remote_code=config.model.get('trust_remote_code', False))\n",
    "\n",
    "        trust_remote_code = config.model.get('trust_remote_code', False)\n",
    "        model_config = AutoConfig.from_pretrained(local_path, trust_remote_code=trust_remote_code)\n",
    "        model_config.num_labels = 1\n",
    "\n",
    "        use_remove_padding = config.model.get('use_remove_padding', False)\n",
    "        if use_remove_padding:\n",
    "            from verl.models.registry import check_model_support_rmpad\n",
    "            check_model_support_rmpad(model_config.model_type)\n",
    "\n",
    "        if use_remove_padding and self.ulysses_sequence_parallel_size > 1:\n",
    "            from verl.models.transformers.monkey_patch import apply_monkey_patch\n",
    "            apply_monkey_patch(model_config, verbose=True)\n",
    "\n",
    "        # note that we have to create model in fp32. Otherwise, the optimizer is in bf16, which is incorrect\n",
    "        init_context = get_init_weight_context_manager(use_meta_tensor=not model_config.tie_word_embeddings)\n",
    "\n",
    "        with init_context(), warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            setattr(model_config, 'classifier_dropout', 0.)\n",
    "            reward_module = AutoModelForTokenClassification.from_pretrained(pretrained_model_name_or_path=local_path,\n",
    "                                                                            config=model_config,\n",
    "                                                                            torch_dtype=torch.bfloat16,\n",
    "                                                                            attn_implementation='flash_attention_2',\n",
    "                                                                            trust_remote_code=trust_remote_code)\n",
    "            reward_module.to(torch.bfloat16)\n",
    "        auto_wrap_policy = get_fsdp_wrap_policy(module=reward_module, config=self.config.model.fsdp_config)\n",
    "\n",
    "        reward_module = FSDP(\n",
    "            reward_module,\n",
    "            param_init_fn=init_fn,\n",
    "            use_orig_params=False,\n",
    "            auto_wrap_policy=auto_wrap_policy,\n",
    "            device_id=torch.cuda.current_device(),\n",
    "            sharding_strategy=ShardingStrategy.FULL_SHARD,  # zero3\n",
    "            sync_module_states=True,\n",
    "            cpu_offload=CPUOffload(offload_params=self.config.model.fsdp_config.param_offload),\n",
    "            forward_prefetch=False)\n",
    "\n",
    "        return reward_module\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.ONE_TO_ALL)\n",
    "    def init_model(self):\n",
    "        # This is used to import external_lib into the huggingface systems\n",
    "        import_external_libs(self.config.model.get('external_lib', None))\n",
    "        self.reward_module = self._build_model(config=self.config)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def _forward_micro_batch(self, micro_batch):\n",
    "        from flash_attn.bert_padding import pad_input, unpad_input, index_first_axis, rearrange\n",
    "        from verl.utils.ulysses import ulysses_pad_and_slice_inputs, gather_outpus_and_unpad\n",
    "\n",
    "        with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            input_ids = micro_batch['input_ids']\n",
    "            batch_size, seqlen = input_ids.shape\n",
    "            attention_mask = micro_batch['attention_mask']\n",
    "            position_ids = micro_batch['position_ids']\n",
    "\n",
    "            if self.use_remove_padding:\n",
    "                input_ids_rmpad, indices, *_ = unpad_input(input_ids.unsqueeze(-1),\n",
    "                                                           attention_mask)  # input_ids_rmpad (total_nnz, ...)\n",
    "                input_ids_rmpad = input_ids_rmpad.transpose(0, 1)  # (1, total_nnz)\n",
    "\n",
    "                # unpad the position_ids to align the rotary\n",
    "                position_ids_rmpad = index_first_axis(rearrange(position_ids.unsqueeze(-1), \"b s ... -> (b s) ...\"),\n",
    "                                                      indices).transpose(0, 1)\n",
    "\n",
    "                # pad and slice the inputs if sp > 1\n",
    "                if self.ulysses_sequence_parallel_size > 1:\n",
    "                    input_ids_rmpad, position_ids_rmpad, pad_size = ulysses_pad_and_slice_inputs(input_ids_rmpad, \\\n",
    "                                                                                                position_ids_rmpad, \\\n",
    "                                                                                                sp_size=self.ulysses_sequence_parallel_size)\n",
    "\n",
    "                # only pass input_ids and position_ids to enable flash_attn_varlen\n",
    "                output = self.reward_module(input_ids=input_ids_rmpad,\n",
    "                                            attention_mask=None,\n",
    "                                            position_ids=position_ids_rmpad,\n",
    "                                            use_cache=False)  # prevent model thinks we are generating\n",
    "                reward_rmpad = output.logits\n",
    "                reward_rmpad = reward_rmpad.squeeze(0)  # (total_nnz)\n",
    "\n",
    "                # gather output if sp > 1\n",
    "                if self.ulysses_sequence_parallel_size > 1:\n",
    "                    reward_rmpad = gather_outpus_and_unpad(reward_rmpad,\n",
    "                                                           gather_dim=0,\n",
    "                                                           unpad_dim=0,\n",
    "                                                           padding_size=pad_size)\n",
    "\n",
    "                # pad it back\n",
    "                rm_score = pad_input(reward_rmpad, indices=indices, batch=batch_size, seqlen=seqlen).squeeze(-1)\n",
    "            else:\n",
    "                output = self.reward_module(input_ids=input_ids,\n",
    "                                            attention_mask=attention_mask,\n",
    "                                            position_ids=position_ids)\n",
    "                rm_score = output.logits  # (batch_size, seq_len, 1)\n",
    "                rm_score = rm_score.squeeze(-1)\n",
    "\n",
    "            # extract the result of the last valid token\n",
    "            eos_mask_idx = torch.argmax(position_ids * attention_mask, dim=-1)  # (bsz,)\n",
    "            rm_score = rm_score[torch.arange(batch_size), eos_mask_idx]\n",
    "            return rm_score\n",
    "\n",
    "    def _expand_to_token_level(self, data: DataProto, scores: torch.Tensor):\n",
    "        batch_size = data.batch.batch_size[0]\n",
    "        # expand as token_level_reward\n",
    "        attention_mask = data.batch['attention_mask']\n",
    "        position_ids = data.batch['position_ids']\n",
    "        response_length = data.batch['responses'].shape[-1]\n",
    "        eos_mask_idx = torch.argmax(position_ids * attention_mask, dim=-1)  # (bsz,)\n",
    "        token_level_scores = torch.zeros_like(attention_mask, dtype=scores.dtype)  # (bsz, seqlen)\n",
    "        token_level_scores[torch.arange(batch_size), eos_mask_idx] = scores\n",
    "\n",
    "        # select the response part\n",
    "        token_level_scores = token_level_scores[:, -response_length:]\n",
    "\n",
    "        return token_level_scores\n",
    "\n",
    "    def _switch_chat_template(self, data: DataProto):\n",
    "        src_max_length = data.batch['attention_mask'].shape[-1]\n",
    "\n",
    "        src_tokenizer = self.input_tokenizer\n",
    "        target_tokenizer = self.tokenizer\n",
    "\n",
    "        rm_input_ids = []\n",
    "        rm_attention_mask = []\n",
    "\n",
    "        for i in range(data.batch.batch_size[0]):\n",
    "            # extract raw prompt\n",
    "            chat: list = data.non_tensor_batch['raw_prompt'][i].tolist()\n",
    "\n",
    "            # extract response\n",
    "            response_ids = data.batch['responses'][i]\n",
    "            response_length = response_ids.shape[-1]\n",
    "            valid_response_length = data.batch['attention_mask'][i][-response_length:].sum()\n",
    "            valid_response_ids = response_ids[:valid_response_length]\n",
    "\n",
    "            # decode\n",
    "            response = src_tokenizer.decode(valid_response_ids)\n",
    "            # remove bos and eos\n",
    "            response = response.replace(src_tokenizer.eos_token, '')\n",
    "\n",
    "            chat.append({'role': 'assistant', 'content': response})\n",
    "\n",
    "            prompt_with_chat_template = target_tokenizer.apply_chat_template(chat,\n",
    "                                                                             add_generation_prompt=False,\n",
    "                                                                             tokenize=False)\n",
    "            if self.rank == 0 and i == 0:\n",
    "                # for debugging purpose\n",
    "                print(f'Switch template. chat: {prompt_with_chat_template}')\n",
    "\n",
    "            # the maximum length is actually determined by the reward model itself\n",
    "            max_length = self.config.get('max_length', src_max_length)\n",
    "            if max_length is None:\n",
    "                max_length = src_max_length\n",
    "            input_ids, attention_mask = verl_F.tokenize_and_postprocess_data(\n",
    "                prompt=prompt_with_chat_template,\n",
    "                tokenizer=target_tokenizer,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=target_tokenizer.pad_token_id,\n",
    "                left_pad=False,  # right padding\n",
    "                truncation=self.config.get('truncation', 'right'))  # truncate from the right\n",
    "\n",
    "            rm_input_ids.append(input_ids)\n",
    "            rm_attention_mask.append(attention_mask)\n",
    "\n",
    "        rm_input_ids = torch.cat(rm_input_ids, dim=0)\n",
    "        rm_attention_mask = torch.cat(rm_attention_mask, dim=0)\n",
    "\n",
    "        rm_position_ids = compute_position_id_with_mask(rm_attention_mask)\n",
    "\n",
    "        rm_inputs = {'input_ids': rm_input_ids, 'attention_mask': rm_attention_mask, 'position_ids': rm_position_ids}\n",
    "\n",
    "        return DataProto.from_dict(rm_inputs)\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\n",
    "    def compute_rm_score(self, data: DataProto):\n",
    "        import itertools\n",
    "        from verl.utils.seqlen_balancing import rearrange_micro_batches, get_reverse_idx\n",
    "        data = data.to('cuda')\n",
    "        if self._do_switch_chat_template:\n",
    "            rm_data = self._switch_chat_template(data)\n",
    "\n",
    "        rm_data.batch = rm_data.batch.cuda()\n",
    "\n",
    "        # perform forward computation\n",
    "        with self.ulysses_sharding_manager:\n",
    "            rm_data = self.ulysses_sharding_manager.preprocess_data(data=rm_data)\n",
    "            data = self.ulysses_sharding_manager.preprocess_data(data=data)\n",
    "\n",
    "            use_dynamic_bsz = self.config.use_dynamic_bsz\n",
    "            if use_dynamic_bsz:\n",
    "                max_token_len = self.config.forward_max_token_len_per_gpu * self.ulysses_sequence_parallel_size\n",
    "                micro_batches, indices = rearrange_micro_batches(batch=rm_data.batch, max_token_len=max_token_len)\n",
    "            else:\n",
    "                micro_batches = rm_data.batch.split(self.config.micro_batch_size)\n",
    "            output = []\n",
    "            for micro_batch in micro_batches:\n",
    "                rm_score = self._forward_micro_batch(micro_batch)\n",
    "                output.append(rm_score)\n",
    "            scores = torch.cat(output, dim=0)  # (batch_size)\n",
    "\n",
    "            if use_dynamic_bsz:\n",
    "                indices = list(itertools.chain.from_iterable(indices))\n",
    "                assert len(indices) == scores.size(0), f\"{len(indices)} vs. {scores.size()}\"\n",
    "                revert_indices = torch.tensor(get_reverse_idx(indices), dtype=torch.long)\n",
    "                scores = scores[revert_indices]\n",
    "\n",
    "            token_level_scores = self._expand_to_token_level(data, scores)\n",
    "            # Note that this is only the scores, may not be the final rewards used to train RL\n",
    "            output = DataProto.from_dict(tensors={'rm_scores': token_level_scores})\n",
    "            output = self.ulysses_sharding_manager.postprocess_data(data=output)\n",
    "\n",
    "        output = output.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5aac21",
   "metadata": {},
   "source": [
    "### „ÉØ„Éº„Ç´„Éº„Ç∞„É´„Éº„Éó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7cf824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Dict, List, Any, Tuple\n",
    "\n",
    "import ray\n",
    "from ray.util import list_named_actors\n",
    "from ray.util.placement_group import placement_group, PlacementGroup\n",
    "from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy, NodeAffinitySchedulingStrategy\n",
    "from ray.experimental.state.api import get_actor\n",
    "\n",
    "from verl.single_controller.base import WorkerGroup, ResourcePool, ClassWithInitArgs, Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca972a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_string(length: int) -> str:\n",
    "    import random\n",
    "    import string\n",
    "    letters_digits = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choice(letters_digits) for _ in range(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f52c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_generator(self, method_name, dispatch_fn, collect_fn, execute_fn, blocking):\n",
    "\n",
    "    def func(*args, **kwargs):\n",
    "        args, kwargs = dispatch_fn(self, *args, **kwargs)\n",
    "        output = execute_fn(method_name, *args, **kwargs)\n",
    "        if blocking:\n",
    "            output = ray.get(output)\n",
    "        output = collect_fn(self, output)\n",
    "        return output\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b64280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayClassWithInitArgs(ClassWithInitArgs):\n",
    "\n",
    "    def __init__(self, cls, *args, **kwargs) -> None:\n",
    "        logger.info(f\"RayClassWithInitArgs„ÇíÂàùÊúüÂåñ {cls=} {args=} {kwargs=}\")\n",
    "        # self._options = kwargs.pop('options', dict())\n",
    "        super().__init__(cls, *args, **kwargs)\n",
    "        self._options = {}\n",
    "        self._additional_resource = {}\n",
    "\n",
    "    def set_additional_resource(self, additional_resource):\n",
    "        self._additional_resource = additional_resource\n",
    "\n",
    "    def update_options(self, options: Dict):\n",
    "        self._options.update(options)\n",
    "\n",
    "    def __call__(self,\n",
    "                 placement_group,\n",
    "                 placement_group_bundle_idx,\n",
    "                 use_gpu: bool = True,\n",
    "                 num_gpus=1,\n",
    "                 sharing_with=None) -> Any:\n",
    "        if sharing_with is not None:\n",
    "            target_node_id = ray.get(sharing_with.get_node_id.remote())\n",
    "            cuda_visible_devices = ray.get(sharing_with.get_cuda_visible_devices.remote())\n",
    "            options = {\"scheduling_strategy\": NodeAffinitySchedulingStrategy(node_id=target_node_id, soft=False)}\n",
    "            return self.cls.options(**options).remote(*self.args,\n",
    "                                                      cuda_visible_devices=cuda_visible_devices,\n",
    "                                                      **self.kwargs)\n",
    "\n",
    "        options = {\n",
    "            \"scheduling_strategy\":\n",
    "                PlacementGroupSchedulingStrategy(placement_group=placement_group,\n",
    "                                                 placement_group_bundle_index=placement_group_bundle_idx)\n",
    "        }\n",
    "        options.update(self._options)\n",
    "\n",
    "        if use_gpu:\n",
    "            options[\"num_gpus\"] = num_gpus\n",
    "\n",
    "        if len(self._additional_resource) > 1:\n",
    "            for k, v in self._additional_resource.items():\n",
    "                options[k] = v\n",
    "\n",
    "        # print(\"cls:\", self.cls)\n",
    "        # print(\"args: \", self.args)\n",
    "        # print(\"kwargs: \", self.kwargs)\n",
    "        return self.cls.options(**options).remote(*self.args, **self.kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c8cce",
   "metadata": {},
   "source": [
    "#### RayResourcePool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayResourcePool(ResourcePool):\n",
    "\n",
    "    def __init__(self,\n",
    "                 process_on_nodes: List[int] = None,\n",
    "                 use_gpu: bool = True,\n",
    "                 name_prefix: str = \"\",\n",
    "                 max_colocate_count: int = 5,\n",
    "                 detached=False) -> None:\n",
    "\n",
    "        logger.info(f\"RayResourcePool„ÇíÂàùÊúüÂåñ {process_on_nodes=} {name_prefix=} {max_colocate_count=}\")\n",
    "\n",
    "        super().__init__(process_on_nodes, max_colocate_count)\n",
    "        self.use_gpu = use_gpu\n",
    "        # print(f\"in RayProcessDispatchConfiguration: name_prefix = {name_prefix}\")\n",
    "        self.name_prefix = name_prefix\n",
    "        self.pgs = None\n",
    "        self.detached = detached\n",
    "\n",
    "    def get_placement_groups(self, strategy=\"STRICT_PACK\", name=None):\n",
    "        if self.pgs is not None:\n",
    "            return self.pgs\n",
    "\n",
    "        pg_name_prefix = name if name else \\\n",
    "            f\"{self.name_prefix}verl_group_{'_'.join([str(count) for count in self._store])}:\"\n",
    "        # print(f\"pg_name_prefix = {pg_name_prefix}\")\n",
    "        pg_scheme = [[{\n",
    "            \"CPU\": self.max_collocate_count,\n",
    "            \"GPU\": 1\n",
    "        } if self.use_gpu else {\n",
    "            \"CPU\": self.max_collocate_count\n",
    "        } for _ in range(process_count)] for process_count in self._store]\n",
    "\n",
    "        lifetime = 'detached' if self.detached else None\n",
    "\n",
    "        pgs = [\n",
    "            placement_group(bundles=bundles, strategy=strategy, name=pg_name_prefix + str(idx), lifetime=lifetime)\n",
    "            for idx, bundles in enumerate(pg_scheme)\n",
    "        ]\n",
    "\n",
    "        ray.get([pg.ready() for pg in pgs])\n",
    "\n",
    "        self.pgs = pgs\n",
    "        return pgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6999f6",
   "metadata": {},
   "source": [
    "#### RayWorkerGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a27ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayWorkerGroup(WorkerGroup):\n",
    "\n",
    "    def __init__(self,\n",
    "                 resource_pool: RayResourcePool = None,\n",
    "                 ray_cls_with_init: RayClassWithInitArgs = None,\n",
    "                 bin_pack: bool = True,\n",
    "                 name_prefix: str = None,\n",
    "                 detached=False,\n",
    "                 worker_names=None,\n",
    "                 **kwargs) -> None:\n",
    "        logger.info(f\"RayWorkerGroup„ÇíÂàùÊúüÂåñ {name_prefix=} {detached=} {worker_names=}\")\n",
    "\n",
    "        super().__init__(resource_pool=resource_pool, **kwargs)\n",
    "\n",
    "        self.ray_cls_with_init = ray_cls_with_init\n",
    "\n",
    "        self.name_prefix = get_random_string(length=6) if name_prefix is None else name_prefix\n",
    "\n",
    "        if worker_names is not None:\n",
    "            assert self._is_init_with_detached_workers\n",
    "            self._worker_names = worker_names\n",
    "\n",
    "        if self._is_init_with_detached_workers:\n",
    "            self._init_with_detached_workers(worker_names=worker_names)\n",
    "        else:\n",
    "            self._init_with_resource_pool(resource_pool=resource_pool,\n",
    "                                          ray_cls_with_init=ray_cls_with_init,\n",
    "                                          bin_pack=bin_pack,\n",
    "                                          detached=detached)\n",
    "\n",
    "        if ray_cls_with_init is not None:\n",
    "            self._bind_worker_method(self.ray_cls_with_init.cls, func_generator)\n",
    "\n",
    "    def _is_worker_alive(self, worker: ray.actor.ActorHandle):\n",
    "        worker_state_dict = get_actor(worker._actor_id.hex())\n",
    "        return worker_state_dict.get(\"state\", \"undefined\") == \"ALIVE\" if worker_state_dict is not None else False\n",
    "\n",
    "    def _init_with_detached_workers(self, worker_names):\n",
    "        workers = [ray.get_actor(name=name) for name in worker_names]\n",
    "        self._workers = workers\n",
    "        self._world_size = len(worker_names)\n",
    "\n",
    "    def _init_with_resource_pool(self, resource_pool, ray_cls_with_init, bin_pack, detached):\n",
    "        use_gpu = resource_pool.use_gpu\n",
    "\n",
    "        strategy = \"PACK\"\n",
    "        if bin_pack:\n",
    "            strategy = \"STRICT_PACK\"\n",
    "        pgs = resource_pool.get_placement_groups(strategy=strategy)\n",
    "        world_size = resource_pool.world_size\n",
    "        self._world_size = world_size\n",
    "        # cia.add_kwarg(\"_world_size\", world_size)\n",
    "        num_gpus = 1 / resource_pool.max_collocate_count\n",
    "\n",
    "        rank = -1\n",
    "        for pg_idx, local_world_size in enumerate(resource_pool.store):\n",
    "            pg = pgs[pg_idx]\n",
    "            assert local_world_size <= pg.bundle_count, \\\n",
    "                f\"when generating for {self.name_prefix}, for the \"\n",
    "            for local_rank in range(local_world_size):\n",
    "                rank += 1\n",
    "\n",
    "                # we pass in environment variable at option so that Worker can use environment variable to set\n",
    "                env_vars = {\n",
    "                    'WORLD_SIZE': str(world_size),\n",
    "                    'RANK': str(rank),\n",
    "                    'WG_PREFIX': self.name_prefix,\n",
    "                    'WG_BACKEND': 'ray',\n",
    "                    'RAY_LOCAL_WORLD_SIZE': str(local_world_size),\n",
    "                    'RAY_LOCAL_RANK': str(local_rank),\n",
    "                }\n",
    "                if rank != 0:\n",
    "                    env_vars['MASTER_ADDR'] = self._master_addr\n",
    "                    env_vars['MASTER_PORT'] = self._master_port\n",
    "\n",
    "                import re\n",
    "                cia_name = type(ray_cls_with_init.cls).__name__\n",
    "                match = re.search(r\"ActorClass\\(([^)]+)\\)\", cia_name)  # ray.remote(Obj) -> \"ActorClass(Obj)\"\n",
    "                cia_name = match.group(1) if match else cia_name  # \"ActorClass(Obj)\" -> \"Obj\"\n",
    "                name = f\"{self.name_prefix}{cia_name}_{pg_idx}:{local_rank}\"  # e.g. Worker_2:5\n",
    "\n",
    "                ray_cls_with_init.update_options({'runtime_env': {'env_vars': env_vars}, 'name': name})\n",
    "\n",
    "                if detached:\n",
    "                    ray_cls_with_init.update_options({'lifetime': 'detached'})\n",
    "\n",
    "                # create a worker\n",
    "                worker = ray_cls_with_init(placement_group=pg,\n",
    "                                           placement_group_bundle_idx=local_rank,\n",
    "                                           use_gpu=use_gpu,\n",
    "                                           num_gpus=num_gpus)\n",
    "                self._workers.append(worker)\n",
    "                self._worker_names.append(name)\n",
    "\n",
    "                if rank == 0:\n",
    "                    register_center_actor = None\n",
    "                    for _ in range(120):\n",
    "                        if f\"{self.name_prefix}_register_center\" not in list_named_actors():\n",
    "                            time.sleep(1)\n",
    "                        else:\n",
    "                            register_center_actor = ray.get_actor(f\"{self.name_prefix}_register_center\")\n",
    "                            break\n",
    "                    assert register_center_actor is not None, f\"failed to get register_center_actor: {self.name_prefix}_register_center in {list_named_actors(all_namespaces=True)}\"\n",
    "                    rank_zero_info = ray.get(register_center_actor.get_rank_zero_info.remote())\n",
    "                    self._master_addr, self._master_port = rank_zero_info['MASTER_ADDR'], rank_zero_info['MASTER_PORT']\n",
    "                    # print(f\"rank_zero_info: {rank_zero_info}\")\n",
    "                    # print(f\"master_addr: {self._master_addr}, master_port: {self._master_port}\")\n",
    "\n",
    "    @property\n",
    "    def worker_names(self):\n",
    "        return self._worker_names\n",
    "\n",
    "    @classmethod\n",
    "    def from_detached(cls, worker_names=None, ray_cls_with_init=None):\n",
    "        worker_group = cls(resource_pool=None,\n",
    "                           ray_cls_with_init=ray_cls_with_init,\n",
    "                           name_prefix=None,\n",
    "                           worker_names=worker_names)\n",
    "        return worker_group\n",
    "\n",
    "    def spawn(self, prefix_set):\n",
    "        \"\"\"\n",
    "        spawn to a dictionary of worker groups, each with a subset of method with prefix.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def _rebind_actor_methods(worker_group, actor_name):\n",
    "            \"\"\"\n",
    "            bind the method with actor_prefix to its original name\n",
    "            \"\"\"\n",
    "            prefix: str = actor_name + '_'\n",
    "            for method_name in dir(worker_group):\n",
    "                if method_name.startswith(prefix):\n",
    "                    # only valid when Python >= 3.9\n",
    "                    original_method_name = method_name.removeprefix(prefix)\n",
    "                    method = getattr(worker_group, method_name)\n",
    "                    setattr(worker_group, original_method_name, method)\n",
    "\n",
    "        new_worker_group_dict = {}\n",
    "        for prefix in prefix_set:\n",
    "            new_worker_group = self.from_detached(worker_names=self._worker_names,\n",
    "                                                  ray_cls_with_init=self.ray_cls_with_init)\n",
    "\n",
    "            _rebind_actor_methods(new_worker_group, prefix)\n",
    "            new_worker_group_dict[prefix] = new_worker_group\n",
    "        return new_worker_group_dict\n",
    "\n",
    "    def execute_rank_zero_sync(self, method_name: str, *args, **kwargs):\n",
    "        return ray.get(self.execute_all_async(method_name, **args, **kwargs))\n",
    "\n",
    "    def execute_rank_zero_async(self, method_name: str, *args, **kwargs):\n",
    "        remote_call = getattr(self._workers[0], method_name)\n",
    "        return remote_call.remote(*args, **kwargs)\n",
    "\n",
    "    def execute_rank_zero(self, method_name: str, *args, **kwargs):\n",
    "        return self.execute_rank_zero_async(method_name, *args, **kwargs)\n",
    "\n",
    "    def execute_all(self, method_name: str, *args, **kwargs):\n",
    "        return self.execute_all_async(method_name, *args, **kwargs)\n",
    "\n",
    "    def execute_all_sync(self, method_name: str, *args, **kwargs):\n",
    "        return ray.get(self.execute_all_async(method_name, *args, **kwargs))\n",
    "\n",
    "    def execute_all_async(self, method_name: str, *args, **kwargs):\n",
    "        # ËøôÈáåÊàë‰ª¨ÂÅáËÆæÔºåÂ¶ÇÊûú args Âíå kwargs ÈáåÈù¢ÊâÄÊúâÁöÑÂèÇÊï∞ÈÉΩÊòØ listÔºå‰∏îÊâÄÊúâÁöÑ list ÈïøÂ∫¶ÈÉΩ‰∏é len(self._workers) ‰∏ÄËá¥ÁöÑËØùÔºåÊàë‰ª¨‰ºöÊää\n",
    "        # list ‰∏≠ÁöÑÊØè‰∏Ä‰∏™ÂàÜÂà´ÂèëÂà∞ÂØπÂ∫îÁöÑ worker ‰∏äÂéª\n",
    "        # print(f\"execute_all_async: method {method_name}({args}, {kwargs})\")\n",
    "        length = len(self._workers)\n",
    "        if all(isinstance(arg, list) for arg in args) and all(isinstance(kwarg, list) for kwarg in kwargs.values()):\n",
    "            if all(len(arg) == length for arg in args) and all(len(kwarg) == length for kwarg in kwargs.values()):\n",
    "                # print(f\"splitting args and kwargs into {length} shards\")\n",
    "                result = []\n",
    "                for i in range(length):\n",
    "                    sliced_args = tuple(arg[i] for arg in args)\n",
    "                    sliced_kwargs = {k: v[i] for k, v in kwargs.items()}\n",
    "                    remote_call = getattr(self._workers[i], method_name)\n",
    "                    result.append(remote_call.remote(*sliced_args, **sliced_kwargs))\n",
    "                return result\n",
    "\n",
    "        return [getattr(worker, method_name).remote(*args, **kwargs) for worker in self._workers]\n",
    "\n",
    "    @property\n",
    "    def master_address(self):\n",
    "        return self._master_addr\n",
    "\n",
    "    @property\n",
    "    def master_port(self):\n",
    "        return self._master_port\n",
    "\n",
    "    @property\n",
    "    def workers(self):\n",
    "        return self._workers\n",
    "\n",
    "    @property\n",
    "    def world_size(self):\n",
    "        return self._world_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9257035",
   "metadata": {},
   "source": [
    "### „Éà„É¨„Éº„Éä„Éº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from pprint import pprint\n",
    "from typing import Type, Dict\n",
    "\n",
    "import numpy as np\n",
    "from codetiming import Timer\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "from verl import DataProto\n",
    "from verl.protocol import pad_dataproto_to_divisor, unpad_dataproto\n",
    "from verl.single_controller.base import Worker\n",
    "from verl.single_controller.ray import RayResourcePool, RayWorkerGroup, RayClassWithInitArgs\n",
    "from verl.single_controller.ray.base import create_colocated_worker_cls\n",
    "from verl.trainer.ppo import core_algos\n",
    "from verl.utils.seqlen_balancing import get_seqlen_balanced_partitions, log_seqlen_unbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e1c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from verl.utils.torch_functional import masked_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371941cc",
   "metadata": {},
   "source": [
    "#### „Ç´„Çπ„Çø„É†Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df006ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "WorkerType = Type[Worker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadfe4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Role(Enum):\n",
    "    \"\"\"\n",
    "    To create more roles dynamically, you can subclass Role and add new members\n",
    "    \"\"\"\n",
    "    Actor = 0\n",
    "    Rollout = 1\n",
    "    ActorRollout = 2\n",
    "    Critic = 3\n",
    "    RefPolicy = 4\n",
    "    RewardModel = 5\n",
    "    ActorRolloutRef = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def _timer(name: str, timing_raw: Dict[str, float]):\n",
    "    with Timer(name=name, logger=None) as timer:\n",
    "        yield\n",
    "    timing_raw[name] = timer.last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca8f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.AdaptiveKLController, kl_penalty='kl'):\n",
    "    responses = data.batch['responses']\n",
    "    response_length = responses.size(1)\n",
    "    token_level_scores = data.batch['token_level_scores']\n",
    "    batch_size = data.batch.batch_size[0]\n",
    "    attention_mask = data.batch['attention_mask']\n",
    "    response_mask = attention_mask[:, -response_length:]\n",
    "\n",
    "    # compute kl between ref_policy and current policy\n",
    "    if 'ref_log_prob' in data.batch.keys():\n",
    "        kld = core_algos.kl_penalty(data.batch['old_log_probs'], data.batch['ref_log_prob'],\n",
    "                                    kl_penalty=kl_penalty)  # (batch_size, response_length)\n",
    "        kld = kld * response_mask\n",
    "        beta = kl_ctrl.value\n",
    "    else:\n",
    "        beta = 0\n",
    "        kld = torch.zeros_like(response_mask, dtype=torch.float32)\n",
    "\n",
    "    token_level_rewards = token_level_scores - beta * kld\n",
    "\n",
    "    current_kl = masked_mean(kld, mask=response_mask, axis=-1)  # average over sequence\n",
    "    current_kl = torch.mean(current_kl, dim=0).item()\n",
    "\n",
    "    # according to https://github.com/huggingface/trl/blob/951ca1841f29114b969b57b26c7d3e80a39f75a0/trl/trainer/ppo_trainer.py#L837\n",
    "    kl_ctrl.update(current_kl=current_kl, n_steps=batch_size)\n",
    "    data.batch['token_level_rewards'] = token_level_rewards\n",
    "\n",
    "    metrics = {'critic/kl': current_kl, 'critic/kl_coeff': beta}\n",
    "\n",
    "    return data, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b356aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantage(data: DataProto, adv_estimator, gamma=1.0, lam=1.0, num_repeat=1):\n",
    "    # prepare response group\n",
    "    # TODO: add other ways to estimate advantages\n",
    "    if adv_estimator == 'gae':\n",
    "        values = data.batch['values']\n",
    "        responses = data.batch['responses']\n",
    "        response_length = responses.size(-1)\n",
    "        attention_mask = data.batch['attention_mask']\n",
    "        response_mask = attention_mask[:, -response_length:]\n",
    "        token_level_rewards = data.batch['token_level_rewards']\n",
    "        advantages, returns = core_algos.compute_gae_advantage_return(token_level_rewards=token_level_rewards,\n",
    "                                                                      values=values,\n",
    "                                                                      eos_mask=response_mask,\n",
    "                                                                      gamma=gamma,\n",
    "                                                                      lam=lam)\n",
    "        data.batch['advantages'] = advantages\n",
    "        data.batch['returns'] = returns\n",
    "    elif adv_estimator == 'grpo':\n",
    "        token_level_rewards = data.batch['token_level_rewards']\n",
    "        index = data.non_tensor_batch['uid']\n",
    "        responses = data.batch['responses']\n",
    "        response_length = responses.size(-1)\n",
    "        attention_mask = data.batch['attention_mask']\n",
    "        response_mask = attention_mask[:, -response_length:]\n",
    "        advantages, returns = core_algos.compute_grpo_outcome_advantage(token_level_rewards=token_level_rewards,\n",
    "                                                                        eos_mask=response_mask,\n",
    "                                                                        index=index)\n",
    "        data.batch['advantages'] = advantages\n",
    "        data.batch['returns'] = returns\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2401a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_metrics(metrics: dict):\n",
    "    for key, val in metrics.items():\n",
    "        metrics[key] = np.mean(val)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c5af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_response_info(batch):\n",
    "    response_length = batch.batch['responses'].shape[-1]\n",
    "\n",
    "    prompt_mask = batch.batch['attention_mask'][:, :-response_length]\n",
    "    response_mask = batch.batch['attention_mask'][:, -response_length:]\n",
    "\n",
    "    prompt_length = prompt_mask.sum(-1).float()\n",
    "    response_length = response_mask.sum(-1).float()  # (batch_size,)\n",
    "\n",
    "    return dict(\n",
    "        response_mask=response_mask,\n",
    "        prompt_length=prompt_length,\n",
    "        response_length=response_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_data_metrics(batch, use_critic=True):\n",
    "    # TODO: add response length\n",
    "    sequence_score = batch.batch['token_level_scores'].sum(-1)\n",
    "    sequence_reward = batch.batch['token_level_rewards'].sum(-1)\n",
    "\n",
    "    advantages = batch.batch['advantages']\n",
    "    returns = batch.batch['returns']\n",
    "\n",
    "    max_response_length = batch.batch['responses'].shape[-1]\n",
    "\n",
    "    prompt_mask = batch.batch['attention_mask'][:, :-max_response_length].bool()\n",
    "    response_mask = batch.batch['attention_mask'][:, -max_response_length:].bool()\n",
    "\n",
    "    max_prompt_length = prompt_mask.size(-1)\n",
    "\n",
    "    response_info = _compute_response_info(batch)\n",
    "    prompt_length = response_info['prompt_length']\n",
    "    response_length = response_info['response_length']\n",
    "\n",
    "    valid_adv = torch.masked_select(advantages, response_mask)\n",
    "    valid_returns = torch.masked_select(returns, response_mask)\n",
    "\n",
    "    if use_critic:\n",
    "        values = batch.batch['values']\n",
    "        valid_values = torch.masked_select(values, response_mask)\n",
    "        return_diff_var = torch.var(valid_returns - valid_values)\n",
    "        return_var = torch.var(valid_returns)\n",
    "\n",
    "    metrics = {\n",
    "        # score\n",
    "        'critic/score/mean':\n",
    "            torch.mean(sequence_score).detach().item(),\n",
    "        'critic/score/max':\n",
    "            torch.max(sequence_score).detach().item(),\n",
    "        'critic/score/min':\n",
    "            torch.min(sequence_score).detach().item(),\n",
    "        # reward\n",
    "        'critic/rewards/mean':\n",
    "            torch.mean(sequence_reward).detach().item(),\n",
    "        'critic/rewards/max':\n",
    "            torch.max(sequence_reward).detach().item(),\n",
    "        'critic/rewards/min':\n",
    "            torch.min(sequence_reward).detach().item(),\n",
    "        # adv\n",
    "        'critic/advantages/mean':\n",
    "            torch.mean(valid_adv).detach().item(),\n",
    "        'critic/advantages/max':\n",
    "            torch.max(valid_adv).detach().item(),\n",
    "        'critic/advantages/min':\n",
    "            torch.min(valid_adv).detach().item(),\n",
    "        # returns\n",
    "        'critic/returns/mean':\n",
    "            torch.mean(valid_returns).detach().item(),\n",
    "        'critic/returns/max':\n",
    "            torch.max(valid_returns).detach().item(),\n",
    "        'critic/returns/min':\n",
    "            torch.min(valid_returns).detach().item(),\n",
    "        **({\n",
    "            # values\n",
    "            'critic/values/mean': torch.mean(valid_values).detach().item(),\n",
    "            'critic/values/max': torch.max(valid_values).detach().item(),\n",
    "            'critic/values/min': torch.min(valid_values).detach().item(),\n",
    "            # vf explained var\n",
    "            'critic/vf_explained_var': (1.0 - return_diff_var / (return_var + 1e-5)).detach().item(),\n",
    "        } if use_critic else {}),\n",
    "\n",
    "        # response length\n",
    "        'response_length/mean':\n",
    "            torch.mean(response_length).detach().item(),\n",
    "        'response_length/max':\n",
    "            torch.max(response_length).detach().item(),\n",
    "        'response_length/min':\n",
    "            torch.min(response_length).detach().item(),\n",
    "        'response_length/clip_ratio':\n",
    "            torch.mean(torch.eq(response_length, max_response_length).float()).detach().item(),\n",
    "        # prompt length\n",
    "        'prompt_length/mean':\n",
    "            torch.mean(prompt_length).detach().item(),\n",
    "        'prompt_length/max':\n",
    "            torch.max(prompt_length).detach().item(),\n",
    "        'prompt_length/min':\n",
    "            torch.min(prompt_length).detach().item(),\n",
    "        'prompt_length/clip_ratio':\n",
    "            torch.mean(torch.eq(prompt_length, max_prompt_length).float()).detach().item(),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7bac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_timing_metrics(batch, timing_raw):\n",
    "    response_info = _compute_response_info(batch)\n",
    "    num_prompt_tokens = torch.sum(response_info['prompt_length']).item()\n",
    "    num_response_tokens = torch.sum(response_info['response_length']).item()\n",
    "    num_overall_tokens = num_prompt_tokens + num_response_tokens\n",
    "\n",
    "    num_tokens_of_section = {\n",
    "        'gen': num_response_tokens,\n",
    "        **{\n",
    "            name: num_overall_tokens for name in ['ref', 'values', 'adv', 'update_critic', 'update_actor']\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        **{\n",
    "            f'timing_s/{name}': value for name, value in timing_raw.items()\n",
    "        },\n",
    "        **{\n",
    "            f'timing_per_token_ms/{name}': timing_raw[name] * 1000 / num_tokens_of_section[name] for name in set(num_tokens_of_section.keys(\n",
    "            )) & set(timing_raw.keys())\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d769f55",
   "metadata": {},
   "source": [
    "#### ResourcePoolManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c9dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ResourcePoolManager:\n",
    "    \"\"\"\n",
    "    Define a resource pool specification. Resource pool will be initialized first.\n",
    "    Mapping\n",
    "    \"\"\"\n",
    "    resource_pool_spec: dict[str, list[int]]\n",
    "    mapping: dict[Role, str]\n",
    "    resource_pool_dict: dict[str, RayResourcePool] = field(default_factory=dict)\n",
    "\n",
    "    def create_resource_pool(self):\n",
    "        for resource_pool_name, process_on_nodes in self.resource_pool_spec.items():\n",
    "            # max_colocate_count means the number of WorkerGroups (i.e. processes) in each RayResourcePool\n",
    "            # For FSDP backend, we recommend using max_colocate_count=1 that merge all WorkerGroups into one.\n",
    "            # For Megatron backend, we recommend using max_colocate_count>1 that can utilize different WorkerGroup for differnt models\n",
    "            resource_pool = RayResourcePool(process_on_nodes=process_on_nodes,\n",
    "                                            use_gpu=True,\n",
    "                                            max_colocate_count=1,\n",
    "                                            name_prefix=resource_pool_name)\n",
    "            self.resource_pool_dict[resource_pool_name] = resource_pool\n",
    "\n",
    "    def get_resource_pool(self, role: Role) -> RayResourcePool:\n",
    "        \"\"\"Get the resource pool of the worker_cls\"\"\"\n",
    "        return self.resource_pool_dict[self.mapping[role]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ec699",
   "metadata": {},
   "source": [
    "#### RayPPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8187a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayPPOTrainer(object):\n",
    "    \"\"\"\n",
    "    Note that this trainer runs on the driver process on a single CPU/GPU node.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: support each role have individual ray_worker_group_cls,\n",
    "    # i.e., support different backend of different role\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 tokenizer,\n",
    "                 role_worker_mapping: dict[Role, WorkerType],\n",
    "                 resource_pool_manager: ResourcePoolManager,\n",
    "                 ray_worker_group_cls: RayWorkerGroup = RayWorkerGroup,\n",
    "                 reward_fn=None,\n",
    "                 val_reward_fn=None):\n",
    "\n",
    "        # assert torch.cuda.is_available(), 'cuda must be available on driver'\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.reward_fn = reward_fn\n",
    "        self.val_reward_fn = val_reward_fn\n",
    "\n",
    "        self.hybrid_engine = config.actor_rollout_ref.hybrid_engine\n",
    "        assert self.hybrid_engine, 'Currently, only support hybrid engine'\n",
    "\n",
    "        if self.hybrid_engine:\n",
    "            assert Role.ActorRollout in role_worker_mapping, f'{role_worker_mapping.keys()=}'\n",
    "\n",
    "        self.role_worker_mapping = role_worker_mapping\n",
    "        self.resource_pool_manager = resource_pool_manager\n",
    "        self.use_reference_policy = Role.RefPolicy in role_worker_mapping\n",
    "        self.use_rm = Role.RewardModel in role_worker_mapping\n",
    "        self.ray_worker_group_cls = ray_worker_group_cls\n",
    "\n",
    "        # define KL control\n",
    "        if self.use_reference_policy:\n",
    "            if config.algorithm.kl_ctrl.type == 'fixed':\n",
    "                self.kl_ctrl = core_algos.FixedKLController(kl_coef=config.algorithm.kl_ctrl.kl_coef)\n",
    "            elif config.algorithm.kl_ctrl.type == 'adaptive':\n",
    "                assert config.algorithm.kl_ctrl.horizon > 0, f'horizon must be larger than 0. Got {config.critic.kl_ctrl.horizon}'\n",
    "                self.kl_ctrl = core_algos.AdaptiveKLController(init_kl_coef=config.algorithm.kl_ctrl.kl_coef,\n",
    "                                                               target_kl=config.algorithm.kl_ctrl.target_kl,\n",
    "                                                               horizon=config.algorithm.kl_ctrl.horizon)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            self.kl_ctrl = core_algos.FixedKLController(kl_coef=0.)\n",
    "\n",
    "        self._create_dataloader()\n",
    "\n",
    "    def _create_dataloader(self):\n",
    "        from torch.utils.data import DataLoader\n",
    "        # TODO: we have to make sure the batch size is divisible by the dp size\n",
    "        from verl.utils.dataset.rl_dataset import RLHFDataset, collate_fn\n",
    "        self.train_dataset = RLHFDataset(parquet_files=self.config.data.train_files,\n",
    "                                         tokenizer=self.tokenizer,\n",
    "                                         prompt_key=self.config.data.prompt_key,\n",
    "                                         max_prompt_length=self.config.data.max_prompt_length,\n",
    "                                         filter_prompts=True,\n",
    "                                         return_raw_chat=self.config.data.get('return_raw_chat', False),\n",
    "                                         truncation='error')\n",
    "        self.train_dataloader = DataLoader(dataset=self.train_dataset,\n",
    "                                           batch_size=self.config.data.train_batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "        self.val_dataset = RLHFDataset(parquet_files=self.config.data.val_files,\n",
    "                                       tokenizer=self.tokenizer,\n",
    "                                       prompt_key=self.config.data.prompt_key,\n",
    "                                       max_prompt_length=self.config.data.max_prompt_length,\n",
    "                                       filter_prompts=True,\n",
    "                                       return_raw_chat=self.config.data.get('return_raw_chat', False),\n",
    "                                       truncation='error')\n",
    "        self.val_dataloader = DataLoader(dataset=self.val_dataset,\n",
    "                                         batch_size=len(self.val_dataset),\n",
    "                                         shuffle=True,\n",
    "                                         drop_last=True,\n",
    "                                         collate_fn=collate_fn)\n",
    "\n",
    "        assert len(self.train_dataloader) >= 1\n",
    "        assert len(self.val_dataloader) >= 1\n",
    "\n",
    "        print(f'Size of train dataloader: {len(self.train_dataloader)}')\n",
    "        print(f'Size of val dataloader: {len(self.val_dataloader)}')\n",
    "\n",
    "        # inject total_training_steps to actor/critic optim_config. This is hacky.\n",
    "        total_training_steps = len(self.train_dataloader) * self.config.trainer.total_epochs\n",
    "\n",
    "        if self.config.trainer.total_training_steps is not None:\n",
    "            total_training_steps = self.config.trainer.total_training_steps\n",
    "\n",
    "        self.total_training_steps = total_training_steps\n",
    "        print(f'Total training steps: {self.total_training_steps}')\n",
    "\n",
    "        OmegaConf.set_struct(self.config, True)\n",
    "        with open_dict(self.config):\n",
    "            self.config.actor_rollout_ref.actor.optim.total_training_steps = total_training_steps\n",
    "            self.config.critic.optim.total_training_steps = total_training_steps\n",
    "\n",
    "    def _validate(self):\n",
    "        reward_tensor_lst = []\n",
    "        data_source_lst = []\n",
    "        for test_data in self.val_dataloader:\n",
    "            test_batch = DataProto.from_single_dict(test_data)\n",
    "            # test_batch = test_batch.to('cuda')\n",
    "\n",
    "            # we only do validation on rule-based rm\n",
    "            if self.config.reward_model.enable and test_batch[0].non_tensor_batch['reward_model']['style'] == 'model':\n",
    "                return {}\n",
    "\n",
    "            test_gen_batch = test_batch.pop(['input_ids', 'attention_mask', 'position_ids'])\n",
    "            test_gen_batch.meta_info = {\n",
    "                'eos_token_id': self.tokenizer.eos_token_id,\n",
    "                'pad_token_id': self.tokenizer.pad_token_id,\n",
    "                'recompute_log_prob': False,\n",
    "                'do_sample': False,\n",
    "                'validate': True,\n",
    "            }\n",
    "\n",
    "            # pad to be divisible by dp_size\n",
    "            test_gen_batch_padded, pad_size = pad_dataproto_to_divisor(test_gen_batch, self.actor_rollout_wg.world_size)\n",
    "            test_output_gen_batch_padded = self.actor_rollout_wg.generate_sequences(test_gen_batch_padded)\n",
    "            # unpad\n",
    "            test_output_gen_batch = unpad_dataproto(test_output_gen_batch_padded, pad_size=pad_size)\n",
    "            print('validation generation end')\n",
    "\n",
    "            test_batch = test_batch.union(test_output_gen_batch)\n",
    "\n",
    "            # evaluate using reward_function\n",
    "            # for certain reward function (e.g. sandbox), the generation can overlap with reward\n",
    "            reward_tensor = self.val_reward_fn(test_batch)\n",
    "\n",
    "            reward_tensor_lst.append(reward_tensor)\n",
    "            data_source_lst.append(test_batch.non_tensor_batch.get('data_source', ['unknown'] * reward_tensor.shape[0]))\n",
    "\n",
    "        reward_tensor = torch.cat(reward_tensor_lst, dim=0).sum(-1).cpu()  # (batch_size,)\n",
    "        data_sources = np.concatenate(data_source_lst, axis=0)\n",
    "        # evaluate test_score based on data source\n",
    "        data_source_reward = {}\n",
    "        for i in range(reward_tensor.shape[0]):\n",
    "            data_source = data_sources[i]\n",
    "            if data_source not in data_source_reward:\n",
    "                data_source_reward[data_source] = []\n",
    "            data_source_reward[data_source].append(reward_tensor[i].item())\n",
    "\n",
    "        metric_dict = {}\n",
    "        for data_source, rewards in data_source_reward.items():\n",
    "            metric_dict[f'val/test_score/{data_source}'] = np.mean(rewards)\n",
    "\n",
    "        return metric_dict\n",
    "\n",
    "    def init_workers(self):\n",
    "        \"\"\"Init resource pool and worker group\"\"\"\n",
    "        self.resource_pool_manager.create_resource_pool()\n",
    "\n",
    "        self.resource_pool_to_cls = {pool: {} for pool in self.resource_pool_manager.resource_pool_dict.values()}\n",
    "\n",
    "        # create actor and rollout\n",
    "        if self.hybrid_engine:\n",
    "            resource_pool = self.resource_pool_manager.get_resource_pool(Role.ActorRollout)\n",
    "            actor_rollout_cls = RayClassWithInitArgs(cls=self.role_worker_mapping[Role.ActorRollout],\n",
    "                                                     config=self.config.actor_rollout_ref,\n",
    "                                                     role='actor_rollout')\n",
    "            self.resource_pool_to_cls[resource_pool]['actor_rollout'] = actor_rollout_cls\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # create critic\n",
    "        if self.config.algorithm.adv_estimator == 'gae':\n",
    "            resource_pool = self.resource_pool_manager.get_resource_pool(Role.Critic)\n",
    "            critic_cls = RayClassWithInitArgs(cls=self.role_worker_mapping[Role.Critic], config=self.config.critic)\n",
    "            self.resource_pool_to_cls[resource_pool]['critic'] = critic_cls\n",
    "            self.use_critic = True\n",
    "        elif self.config.algorithm.adv_estimator == 'grpo':\n",
    "            self.use_critic = False\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # create reference policy if needed\n",
    "        if self.use_reference_policy:\n",
    "            resource_pool = self.resource_pool_manager.get_resource_pool(Role.RefPolicy)\n",
    "            ref_policy_cls = RayClassWithInitArgs(self.role_worker_mapping[Role.RefPolicy],\n",
    "                                                  config=self.config.actor_rollout_ref,\n",
    "                                                  role='ref')\n",
    "            self.resource_pool_to_cls[resource_pool]['ref'] = ref_policy_cls\n",
    "\n",
    "        # create a reward model if reward_fn is None\n",
    "        if self.use_rm:\n",
    "            # we create a RM here\n",
    "            resource_pool = self.resource_pool_manager.get_resource_pool(Role.RewardModel)\n",
    "            rm_cls = RayClassWithInitArgs(self.role_worker_mapping[Role.RewardModel], config=self.config.reward_model)\n",
    "            self.resource_pool_to_cls[resource_pool]['rm'] = rm_cls\n",
    "\n",
    "        # initialize WorkerGroup\n",
    "        # NOTE: if you want to use a different resource pool for each role, which can support different parallel size,\n",
    "        # you should not use `create_colocated_worker_cls`. Instead, directly pass different resource pool to different worker groups.\n",
    "        # See https://github.com/volcengine/verl/blob/master/examples/ray/tutorial.ipynb for more information.\n",
    "        all_wg = {}\n",
    "        self.wg_dicts = []\n",
    "        for resource_pool, class_dict in self.resource_pool_to_cls.items():\n",
    "            worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)\n",
    "            wg_dict = self.ray_worker_group_cls(resource_pool=resource_pool, ray_cls_with_init=worker_dict_cls)\n",
    "            spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())\n",
    "            all_wg.update(spawn_wg)\n",
    "            # keep the referece of WorkerDict to support ray >= 2.31. Ref: https://github.com/ray-project/ray/pull/45699\n",
    "            self.wg_dicts.append(wg_dict)\n",
    "\n",
    "        if self.use_critic:\n",
    "            self.critic_wg = all_wg['critic']\n",
    "            self.critic_wg.init_model()\n",
    "\n",
    "        if self.use_reference_policy:\n",
    "            self.ref_policy_wg = all_wg['ref']\n",
    "            self.ref_policy_wg.init_model()\n",
    "\n",
    "        if self.use_rm:\n",
    "            self.rm_wg = all_wg['rm']\n",
    "            self.rm_wg.init_model()\n",
    "\n",
    "        # we should create rollout at the end so that vllm can have a better estimation of kv cache memory\n",
    "        self.actor_rollout_wg = all_wg['actor_rollout']\n",
    "        self.actor_rollout_wg.init_model()\n",
    "\n",
    "    def _save_checkpoint(self):\n",
    "        actor_local_path = os.path.join(self.config.trainer.default_local_dir, 'actor',\n",
    "                                        f'global_step_{self.global_steps}')\n",
    "        actor_remote_path = None if self.config.trainer.default_hdfs_dir is None else os.path.join(\n",
    "            self.config.trainer.default_hdfs_dir, 'actor')\n",
    "        self.actor_rollout_wg.save_checkpoint(actor_local_path, actor_remote_path)\n",
    "\n",
    "        if self.use_critic:\n",
    "            critic_local_path = os.path.join(self.config.trainer.default_local_dir, 'critic',\n",
    "                                             f'global_step_{self.global_steps}')\n",
    "            critic_remote_path = None if self.config.trainer.default_hdfs_dir is None else os.path.join(\n",
    "                self.config.trainer.default_hdfs_dir, 'critic')\n",
    "            self.critic_wg.save_checkpoint(critic_local_path, critic_remote_path)\n",
    "\n",
    "    def _balance_batch(self, batch: DataProto, metrics, logging_prefix='global_seqlen'):\n",
    "        \"\"\"Reorder the data on single controller such that each dp rank gets similar total tokens\"\"\"\n",
    "        attention_mask = batch.batch['attention_mask']\n",
    "        batch_size = attention_mask.shape[0]\n",
    "        global_seqlen_lst = batch.batch['attention_mask'].view(batch_size, -1).sum(-1).tolist()  # (train_batch_size,)\n",
    "        world_size = self.actor_rollout_wg.world_size\n",
    "        global_partition_lst = get_seqlen_balanced_partitions(global_seqlen_lst,\n",
    "                                                              k_partitions=world_size,\n",
    "                                                              equal_size=True)\n",
    "        # reorder based on index. The data will be automatically equally partitioned by dispatch function\n",
    "        global_idx = torch.tensor([j for partition in global_partition_lst for j in partition])\n",
    "        batch.reorder(global_idx)\n",
    "        global_balance_stats = log_seqlen_unbalance(seqlen_list=global_seqlen_lst,\n",
    "                                                    partitions=global_partition_lst,\n",
    "                                                    prefix=logging_prefix)\n",
    "        metrics.update(global_balance_stats)\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        The training loop of PPO.\n",
    "        The driver process only need to call the compute functions of the worker group through RPC to construct the PPO dataflow.\n",
    "        The light-weight advantage computation is done on the driver process.\n",
    "        \"\"\"\n",
    "        from verl.utils.tracking import Tracking\n",
    "        from omegaconf import OmegaConf\n",
    "\n",
    "        logger = Tracking(project_name=self.config.trainer.project_name,\n",
    "                          experiment_name=self.config.trainer.experiment_name,\n",
    "                          default_backend=self.config.trainer.logger,\n",
    "                          config=OmegaConf.to_container(self.config, resolve=True))\n",
    "\n",
    "        self.global_steps = 0\n",
    "\n",
    "        # perform validation before training\n",
    "        # currently, we only support validation using the reward_function.\n",
    "        if self.val_reward_fn is not None and self.config.trainer.get('val_before_train', True):\n",
    "            val_metrics = self._validate()\n",
    "            pprint(f'Initial validation metrics: {val_metrics}')\n",
    "            logger.log(data=val_metrics, step=self.global_steps)\n",
    "            if self.config.trainer.get('val_only', False):\n",
    "                return\n",
    "\n",
    "        # we start from step 1\n",
    "        self.global_steps += 1\n",
    "\n",
    "        for epoch in range(self.config.trainer.total_epochs):\n",
    "            for batch_dict in self.train_dataloader:\n",
    "                print(f'epoch {epoch}, step {self.global_steps}')\n",
    "                metrics = {}\n",
    "                timing_raw = {}\n",
    "\n",
    "                batch: DataProto = DataProto.from_single_dict(batch_dict)\n",
    "\n",
    "                # pop those keys for generation\n",
    "                gen_batch = batch.pop(batch_keys=['input_ids', 'attention_mask', 'position_ids'])\n",
    "\n",
    "                with _timer('step', timing_raw):\n",
    "                    # generate a batch\n",
    "                    with _timer('gen', timing_raw):\n",
    "                        gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)\n",
    "\n",
    "                    batch.non_tensor_batch['uid'] = np.array([str(uuid.uuid4()) for _ in range(len(batch.batch))],\n",
    "                                                             dtype=object)\n",
    "                    # repeat to align with repeated responses in rollout\n",
    "                    batch = batch.repeat(repeat_times=self.config.actor_rollout_ref.rollout.n, interleave=True)\n",
    "                    batch = batch.union(gen_batch_output)\n",
    "\n",
    "                    # balance the number of valid tokens on each dp rank.\n",
    "                    # Note that this breaks the order of data inside the batch.\n",
    "                    # Please take care when you implement group based adv computation such as GRPO and rloo\n",
    "                    self._balance_batch(batch, metrics=metrics)\n",
    "\n",
    "                    # compute global_valid tokens\n",
    "                    batch.meta_info['global_token_num'] = torch.sum(batch.batch['attention_mask'], dim=-1).tolist()\n",
    "\n",
    "                    if self.use_reference_policy:\n",
    "                        # compute reference log_prob\n",
    "                        with _timer('ref', timing_raw):\n",
    "                            ref_log_prob = self.ref_policy_wg.compute_ref_log_prob(batch)\n",
    "                            batch = batch.union(ref_log_prob)\n",
    "\n",
    "                    # compute values\n",
    "                    if self.use_critic:\n",
    "                        with _timer('values', timing_raw):\n",
    "                            values = self.critic_wg.compute_values(batch)\n",
    "                            batch = batch.union(values)\n",
    "\n",
    "                    with _timer('adv', timing_raw):\n",
    "                        # compute scores. Support both model and function-based.\n",
    "                        # We first compute the scores using reward model. Then, we call reward_fn to combine\n",
    "                        # the results from reward model and rule-based results.\n",
    "                        if self.use_rm:\n",
    "                            # we first compute reward model score\n",
    "                            reward_tensor = self.rm_wg.compute_rm_score(batch)\n",
    "                            batch = batch.union(reward_tensor)\n",
    "\n",
    "                        # we combine with rule-based rm\n",
    "                        reward_tensor = self.reward_fn(batch)\n",
    "                        batch.batch['token_level_scores'] = reward_tensor\n",
    "\n",
    "                        # compute rewards. apply_kl_penalty if available\n",
    "                        if not self.config.actor_rollout_ref.actor.use_kl_loss:\n",
    "                            batch, kl_metrics = apply_kl_penalty(batch,\n",
    "                                                                 kl_ctrl=self.kl_ctrl,\n",
    "                                                                 kl_penalty=self.config.algorithm.kl_penalty)\n",
    "                            metrics.update(kl_metrics)\n",
    "                        else:\n",
    "                            batch.batch['token_level_rewards'] = batch.batch['token_level_scores']\n",
    "\n",
    "                        # compute advantages, executed on the driver process\n",
    "                        batch = compute_advantage(batch,\n",
    "                                                  adv_estimator=self.config.algorithm.adv_estimator,\n",
    "                                                  gamma=self.config.algorithm.gamma,\n",
    "                                                  lam=self.config.algorithm.lam,\n",
    "                                                  num_repeat=self.config.actor_rollout_ref.rollout.n)\n",
    "\n",
    "                    # update critic\n",
    "                    if self.use_critic:\n",
    "                        with _timer('update_critic', timing_raw):\n",
    "                            critic_output = self.critic_wg.update_critic(batch)\n",
    "                        critic_output_metrics = reduce_metrics(critic_output.meta_info['metrics'])\n",
    "                        metrics.update(critic_output_metrics)\n",
    "\n",
    "                    # implement critic warmup\n",
    "                    if self.config.trainer.critic_warmup <= self.global_steps:\n",
    "                        # update actor\n",
    "                        with _timer('update_actor', timing_raw):\n",
    "                            actor_output = self.actor_rollout_wg.update_actor(batch)\n",
    "                        actor_output_metrics = reduce_metrics(actor_output.meta_info['metrics'])\n",
    "                        metrics.update(actor_output_metrics)\n",
    "\n",
    "                    # validate\n",
    "                    if self.val_reward_fn is not None and self.config.trainer.test_freq > 0 and \\\n",
    "                        self.global_steps % self.config.trainer.test_freq == 0:\n",
    "                        with _timer('testing', timing_raw):\n",
    "                            val_metrics: dict = self._validate()\n",
    "                        metrics.update(val_metrics)\n",
    "\n",
    "                    if self.config.trainer.save_freq > 0 and \\\n",
    "                            self.global_steps % self.config.trainer.save_freq == 0:\n",
    "                        with _timer('save_checkpoint', timing_raw):\n",
    "                            self._save_checkpoint()\n",
    "\n",
    "                # collect metrics\n",
    "                metrics.update(compute_data_metrics(batch=batch, use_critic=self.use_critic))\n",
    "                metrics.update(compute_timing_metrics(batch=batch, timing_raw=timing_raw))\n",
    "\n",
    "                # TODO: make a canonical logger that supports various backend\n",
    "                logger.log(data=metrics, step=self.global_steps)\n",
    "\n",
    "                self.global_steps += 1\n",
    "\n",
    "                if self.global_steps >= self.total_training_steps:\n",
    "\n",
    "                    # perform validation after training\n",
    "                    if self.val_reward_fn is not None:\n",
    "                        val_metrics = self._validate()\n",
    "                        pprint(f'Final validation metrics: {val_metrics}')\n",
    "                        logger.log(data=val_metrics, step=self.global_steps)\n",
    "                    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed599d",
   "metadata": {},
   "source": [
    "### Â†±ÈÖ¨Èñ¢Êï∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a8e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import ast\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_solution(solution_str):\n",
    "    \"\"\"Extract the equation from the solution string.\"\"\"\n",
    "    # Remove everything before the first \"Assistant:\"\n",
    "    if \"Assistant:\" in solution_str:\n",
    "        solution_str = solution_str.split(\"Assistant:\", 1)[1]\n",
    "    elif \"<|im_start|>assistant\" in solution_str:\n",
    "        solution_str = solution_str.split(\"<|im_start|>assistant\", 1)[1]\n",
    "    else:\n",
    "        return None\n",
    "    solution_str = solution_str.split('\\n')[-1]\n",
    "\n",
    "    answer_pattern = r'<answer>(.*?)</answer>'\n",
    "    match = re.finditer(answer_pattern, solution_str)\n",
    "    matches = list(match)\n",
    "    if matches:\n",
    "        final_answer = matches[-1].group(1).strip()\n",
    "    else:\n",
    "        final_answer = None\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc301caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_equation(equation_str, available_numbers):\n",
    "    \"\"\"Validate that equation only uses available numbers and each number once.\"\"\"\n",
    "    try:\n",
    "        # Extract all numbers from the equation\n",
    "        numbers_in_eq = [int(n) for n in re.findall(r'\\d+', equation_str)]\n",
    "        \n",
    "        # Check if all numbers in equation are available\n",
    "        available_numbers = sorted(available_numbers)\n",
    "        numbers_in_eq = sorted(numbers_in_eq)\n",
    "        \n",
    "        # Each number should be used exactly once\n",
    "        return numbers_in_eq == available_numbers\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_equation(equation_str):\n",
    "    \"\"\"Safely evaluate the arithmetic equation using eval() with precautions.\"\"\"\n",
    "    try:\n",
    "        # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace\n",
    "        allowed_pattern = r'^[\\d+\\-*/().\\s]+$'\n",
    "        if not re.match(allowed_pattern, equation_str):\n",
    "            raise ValueError(\"Invalid characters in equation.\")\n",
    "\n",
    "        # Evaluate the equation with restricted globals and locals\n",
    "        result = eval(equation_str, {\"__builtins__\": None}, {})\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b5ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(solution_str, ground_truth, method='strict', format_score=0.1, score=1.):\n",
    "    \"\"\"The scoring function for countdown task.\n",
    "    \n",
    "    Args:\n",
    "        solution_str: the solution text\n",
    "        ground_truth: dictionary containing target number and available numbers\n",
    "        method: the method to extract the solution\n",
    "        format_score: the score for correct format but wrong answer\n",
    "        score: the score for the correct answer\n",
    "    \"\"\"\n",
    "    target = ground_truth['target']\n",
    "    numbers = ground_truth['numbers']\n",
    "    \n",
    "    equation = extract_solution(solution_str=solution_str)\n",
    "    do_print = random.randint(1, 64) == 1\n",
    "    \n",
    "    if do_print:\n",
    "        print(f\"--------------------------------\")\n",
    "        print(f\"Target: {target} | Numbers: {numbers}\")\n",
    "        print(f\"Extracted equation: {equation}\")\n",
    "        print(f\"Solution string: {solution_str}\")\n",
    "\n",
    "    if equation is None:\n",
    "        if do_print:\n",
    "            print(f\"No equation found\")\n",
    "        return 0\n",
    "    \n",
    "    # Validate equation uses correct numbers\n",
    "    if not validate_equation(equation, numbers):\n",
    "        if do_print:\n",
    "            print(f\"Invalid equation\")\n",
    "        return format_score\n",
    "        \n",
    "    # Evaluate equation\n",
    "    try:\n",
    "        result = evaluate_equation(equation)\n",
    "        if result is None:\n",
    "            if do_print:\n",
    "                print(f\"Could not evaluate equation\")\n",
    "            return format_score\n",
    "            \n",
    "        if abs(result - target) < 1e-5:  # Account for floating point precision\n",
    "            if do_print:\n",
    "                print(f\"Correct equation: {equation} = {result}\")\n",
    "            return score\n",
    "        else:\n",
    "            if do_print:\n",
    "                print(f\"Wrong result: equation = {result}, target = {target}\")\n",
    "            return format_score\n",
    "    except:\n",
    "        if do_print:\n",
    "            print(f\"Error evaluating equation\")\n",
    "        return format_score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cfd477",
   "metadata": {},
   "source": [
    "## Ë®ìÁ∑¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe9a36e",
   "metadata": {},
   "source": [
    "```sh\n",
    "N_GPUS=1 BASE_MODEL=\"qwen/qwen2.5-0.5b\" DATA_DIR=\"countdown\" ROLLOUT_TP_SIZE=1 VLLM_ATTENTION_BACKEND=\"XFORMERS\" bash ./scripts/train_tiny_zero.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2931326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from verl import DataProto\n",
    "import torch\n",
    "# from verl.utils.reward_score import gsm8k, math, multiply, countdown\n",
    "# from verl.trainer.ppo.ray_trainer import RayPPOTrainer\n",
    "import ray\n",
    "import hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Ç≥„Éû„É≥„Éâ„É©„Ç§„É≥„ÅÆÁí∞Â¢ÉÂ§âÊï∞„ÇíÂ§âÊï∞Âåñ\n",
    "\n",
    "BASE_MODEL=\"qwen/qwen2.5-0.5b\"\n",
    "DATA_DIR = \"countdown\"\n",
    "EXPERIMENT_NAME = \"countdown_ppo_demo\"\n",
    "N_GPUS = 1\n",
    "ROLLOUT_TP_SIZE = 1\n",
    "VLLM_ATTENTION_BACKEND = \"XFORMERS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO„Éà„É¨„Éº„Éä„Éº„ÅÆ„Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö„ÇíË™≠„ÅøËæº„Åø\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# TinyZero/verl/trainer/config/ppo_trainer.yaml\n",
    "ppo_trainer_yaml = \"\"\"\n",
    "data:\n",
    "  tokenizer: null\n",
    "  train_files: ~/data/rlhf/gsm8k/train.parquet\n",
    "  val_files: ~/data/rlhf/gsm8k/test.parquet\n",
    "  prompt_key: prompt\n",
    "  max_prompt_length: 512\n",
    "  max_response_length: 512\n",
    "  train_batch_size: 1024\n",
    "  val_batch_size: 1312\n",
    "  return_raw_input_ids: False  # This should be set to true when the tokenizer between policy and rm differs\n",
    "  return_raw_chat: False\n",
    "\n",
    "actor_rollout_ref:\n",
    "  hybrid_engine: True\n",
    "  model:\n",
    "    path: ~/models/deepseek-llm-7b-chat\n",
    "    external_lib: null\n",
    "    override_config: { }\n",
    "    enable_gradient_checkpointing: False\n",
    "    use_remove_padding: False\n",
    "  actor:\n",
    "    strategy: fsdp  # This is for backward-compatibility\n",
    "    ppo_mini_batch_size: 256\n",
    "    ppo_micro_batch_size: 64\n",
    "    use_dynamic_bsz: False\n",
    "    ppo_max_token_len_per_gpu: 16384 # n * ${data.max_prompt_length} + ${data.max_response_length}\n",
    "    grad_clip: 1.0\n",
    "    clip_ratio: 0.2\n",
    "    entropy_coeff: 0.001\n",
    "    use_kl_loss: False # True for GRPO\n",
    "    kl_loss_coef: 0.001 # for grpo\n",
    "    kl_loss_type: low_var_kl # for grpo\n",
    "    ppo_epochs: 1\n",
    "    shuffle: False\n",
    "    ulysses_sequence_parallel_size: 1 # sp size\n",
    "    optim:\n",
    "      lr: 1e-6\n",
    "      lr_warmup_steps_ratio: 0.  # the total steps will be injected during runtime\n",
    "      min_lr_ratio: null   # only useful for warmup with cosine\n",
    "      warmup_style: constant  # select from constant/cosine\n",
    "      total_training_steps: -1  # must be override by program\n",
    "    fsdp_config:\n",
    "      wrap_policy:\n",
    "        # transformer_layer_cls_to_wrap: None\n",
    "        min_num_params: 0\n",
    "      param_offload: False\n",
    "      grad_offload: False\n",
    "      optimizer_offload: False\n",
    "      fsdp_size: -1\n",
    "  ref:\n",
    "    fsdp_config:\n",
    "      param_offload: False\n",
    "      wrap_policy:\n",
    "        # transformer_layer_cls_to_wrap: None\n",
    "        min_num_params: 0\n",
    "      fsdp_size: -1\n",
    "    log_prob_micro_batch_size: 128\n",
    "    log_prob_use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}\n",
    "    log_prob_max_token_len_per_gpu: ${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}\n",
    "    ulysses_sequence_parallel_size: ${actor_rollout_ref.actor.ulysses_sequence_parallel_size} # sp size\n",
    "  rollout:\n",
    "    name: vllm\n",
    "    temperature: 1.0\n",
    "    top_k: -1 # 0 for hf rollout, -1 for vllm rollout\n",
    "    top_p: 1\n",
    "    prompt_length: ${data.max_prompt_length}  # not use for opensource\n",
    "    response_length: ${data.max_response_length}\n",
    "    # for vllm rollout\n",
    "    dtype: bfloat16 # should align with FSDP\n",
    "    gpu_memory_utilization: 0.5\n",
    "    ignore_eos: False\n",
    "    enforce_eager: True\n",
    "    free_cache_engine: True\n",
    "    load_format: dummy_dtensor\n",
    "    tensor_model_parallel_size: 2\n",
    "    max_num_batched_tokens: 8192\n",
    "    max_num_seqs: 1024\n",
    "    log_prob_micro_batch_size: 128\n",
    "    log_prob_use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}\n",
    "    log_prob_max_token_len_per_gpu: ${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}\n",
    "    # for hf rollout\n",
    "    do_sample: True\n",
    "    # number of responses (i.e. num sample times)\n",
    "    n: 1 # > 1 for grpo\n",
    "\n",
    "critic:\n",
    "  strategy: fsdp\n",
    "  optim:\n",
    "    lr: 1e-5\n",
    "    lr_warmup_steps_ratio: 0.  # the total steps will be injected during runtime\n",
    "    min_lr_ratio: null   # only useful for warmup with cosine\n",
    "    warmup_style: constant  # select from constant/cosine\n",
    "    total_training_steps: -1  # must be override by program\n",
    "  model:\n",
    "    path: ~/models/deepseek-llm-7b-chat\n",
    "    tokenizer_path: ${actor_rollout_ref.model.path}\n",
    "    override_config: { }\n",
    "    external_lib: ${actor_rollout_ref.model.external_lib}\n",
    "    enable_gradient_checkpointing: False\n",
    "    use_remove_padding: False\n",
    "    fsdp_config:\n",
    "      param_offload: False\n",
    "      grad_offload: False\n",
    "      optimizer_offload: False\n",
    "      wrap_policy:\n",
    "        # transformer_layer_cls_to_wrap: None\n",
    "        min_num_params: 0\n",
    "      fsdp_size: -1\n",
    "  ppo_mini_batch_size: ${actor_rollout_ref.actor.ppo_mini_batch_size}\n",
    "  ppo_micro_batch_size: 64\n",
    "  forward_micro_batch_size: ${critic.ppo_micro_batch_size}\n",
    "  use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}\n",
    "  ppo_max_token_len_per_gpu: 32768 # (${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}) * 2\n",
    "  forward_max_token_len_per_gpu: ${critic.ppo_max_token_len_per_gpu}\n",
    "  ulysses_sequence_parallel_size: 1 # sp size\n",
    "  ppo_epochs: ${actor_rollout_ref.actor.ppo_epochs}\n",
    "  shuffle: ${actor_rollout_ref.actor.shuffle}\n",
    "  grad_clip: 1.0\n",
    "  cliprange_value: 0.5\n",
    "\n",
    "reward_model:\n",
    "  enable: False\n",
    "  strategy: fsdp\n",
    "  model:\n",
    "    input_tokenizer: ${actor_rollout_ref.model.path}  # set this to null if the chat template is identical\n",
    "    path: ~/models/FsfairX-LLaMA3-RM-v0.1\n",
    "    external_lib: ${actor_rollout_ref.model.external_lib}\n",
    "    use_remove_padding: False\n",
    "    fsdp_config:\n",
    "      min_num_params: 0\n",
    "      param_offload: False\n",
    "  micro_batch_size: 64\n",
    "  max_length: null\n",
    "  ulysses_sequence_parallel_size: 1 # sp size\n",
    "  use_dynamic_bsz: ${critic.use_dynamic_bsz}\n",
    "  forward_max_token_len_per_gpu: ${critic.forward_max_token_len_per_gpu}\n",
    "\n",
    "algorithm:\n",
    "  gamma: 1.0\n",
    "  lam: 1.0\n",
    "  adv_estimator: gae\n",
    "  kl_penalty: kl  # how to estimate kl divergence\n",
    "  kl_ctrl:\n",
    "    type: fixed\n",
    "    kl_coef: 0.001\n",
    "\n",
    "trainer:\n",
    "  total_epochs: 30\n",
    "  total_training_steps: null\n",
    "  project_name: verl_examples\n",
    "  experiment_name: gsm8k\n",
    "  logger: [ 'console', 'wandb' ]\n",
    "  nnodes: 1\n",
    "  n_gpus_per_node: 8\n",
    "  save_freq: -1\n",
    "  test_freq: -1\n",
    "  critic_warmup: 0\n",
    "  default_hdfs_dir: ~/experiments/gsm8k/ppo/${trainer.experiment_name}\n",
    "  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}\n",
    "\"\"\"\n",
    "\n",
    "ppo_trainer_config = OmegaConf.create(ppo_trainer_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684dfd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO„Éà„É¨„Éº„Éã„É≥„Ç∞„ÅÆ„Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö„Çí‰∏äÊõ∏„Åç\n",
    "\n",
    "# TinyZero/scripts/train_tiny_zero.sh„Çà„ÇäÊäúÁ≤ã\n",
    "# python3 -m verl.trainer.main_ppo \\\n",
    "# data.train_files=$DATA_DIR/train.parquet \\\n",
    "# data.val_files=$DATA_DIR/test.parquet \\\n",
    "# data.train_batch_size=32 \\\n",
    "# data.val_batch_size=32 \\\n",
    "# data.max_prompt_length=256 \\\n",
    "# data.max_response_length=1024 \\\n",
    "# actor_rollout_ref.model.path=$BASE_MODEL \\\n",
    "# actor_rollout_ref.model.use_remove_padding=True \\\n",
    "# actor_rollout_ref.model.enable_gradient_checkpointing=True \\\n",
    "# actor_rollout_ref.actor.use_dynamic_bsz=True \\\n",
    "# actor_rollout_ref.actor.optim.lr=1e-6 \\\n",
    "# actor_rollout_ref.actor.ppo_mini_batch_size=4 \\\n",
    "# actor_rollout_ref.actor.ppo_micro_batch_size=4 \\\n",
    "# actor_rollout_ref.rollout.log_prob_micro_batch_size=1 \\\n",
    "# actor_rollout_ref.rollout.tensor_model_parallel_size=$ROLLOUT_TP_SIZE \\\n",
    "# actor_rollout_ref.rollout.gpu_memory_utilization=0.1 \\\n",
    "# actor_rollout_ref.ref.log_prob_micro_batch_size=1 \\\n",
    "# critic.optim.lr=1e-5 \\\n",
    "# critic.model.path=$BASE_MODEL \\\n",
    "# critic.ppo_micro_batch_size=1 \\\n",
    "# algorithm.kl_ctrl.kl_coef=0.001 \\\n",
    "# trainer.logger=[] \\\n",
    "# +trainer.val_before_train=False \\\n",
    "# trainer.default_hdfs_dir=null \\\n",
    "# trainer.n_gpus_per_node=$N_GPUS \\\n",
    "# trainer.nnodes=1 \\\n",
    "# trainer.save_freq=100 \\\n",
    "# trainer.test_freq=100 \\\n",
    "# trainer.project_name=TinyZero \\\n",
    "# trainer.experiment_name=$EXPERIMENT_NAME \\\n",
    "# trainer.total_epochs=15 2>&1 | tee verl_demo.log\n",
    "\n",
    "ppo_trainer_config.data.train_files = os.path.join(DATA_DIR, \"train.parquet\")\n",
    "ppo_trainer_config.data.val_files = os.path.join(DATA_DIR, \"test.parquet\")\n",
    "ppo_trainer_config.data.train_batch_size = 32\n",
    "ppo_trainer_config.data.val_batch_size = 32\n",
    "ppo_trainer_config.data.max_prompt_length = 256\n",
    "ppo_trainer_config.data.max_response_length = 1024\n",
    "ppo_trainer_config.actor_rollout_ref.model.path = BASE_MODEL\n",
    "ppo_trainer_config.actor_rollout_ref.model.use_remove_padding = True\n",
    "ppo_trainer_config.actor_rollout_ref.model.enable_gradient_checkpointing = True\n",
    "ppo_trainer_config.actor_rollout_ref.actor.use_dynamic_bsz = True\n",
    "ppo_trainer_config.actor_rollout_ref.actor.optim.lr = 1e-6\n",
    "ppo_trainer_config.actor_rollout_ref.actor.ppo_mini_batch_size = 4\n",
    "ppo_trainer_config.actor_rollout_ref.actor.ppo_micro_batch_size = 4\n",
    "ppo_trainer_config.actor_rollout_ref.rollout.log_prob_micro_batch_size = 1\n",
    "ppo_trainer_config.actor_rollout_ref.rollout.tensor_model_parallel_size = ROLLOUT_TP_SIZE\n",
    "ppo_trainer_config.actor_rollout_ref.rollout.gpu_memory_utilization = 0.1\n",
    "ppo_trainer_config.actor_rollout_ref.ref.log_prob_micro_batch_size = 1\n",
    "ppo_trainer_config.critic.optim.lr = 1e-5\n",
    "ppo_trainer_config.critic.model.path = BASE_MODEL\n",
    "ppo_trainer_config.critic.ppo_micro_batch_size = 1\n",
    "ppo_trainer_config.algorithm.kl_ctrl.kl_coef = 0.001\n",
    "ppo_trainer_config.trainer.logger = []\n",
    "ppo_trainer_config.trainer.val_before_train = False\n",
    "ppo_trainer_config.trainer.default_hdfs_dir = None\n",
    "ppo_trainer_config.trainer.n_gpus_per_node = N_GPUS\n",
    "ppo_trainer_config.trainer.nnodes = 1\n",
    "ppo_trainer_config.trainer.save_freq = 100\n",
    "ppo_trainer_config.trainer.test_freq = 100\n",
    "ppo_trainer_config.trainer.project_name = \"TinyZero\"\n",
    "ppo_trainer_config.trainer.experiment_name = EXPERIMENT_NAME\n",
    "\n",
    "# „Éá„Éê„ÉÉ„Ç∞ÁõÆÁöÑ„Åß„Ç®„Éù„ÉÉ„ÇØÊï∞„Çí1„Å´Ë®≠ÂÆö\n",
    "ppo_trainer_config.trainer.total_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1c62c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_rm_score_fn(data_source):\n",
    "    # if data_source == 'openai/gsm8k':\n",
    "    #     return gsm8k.compute_score\n",
    "    # elif data_source == 'lighteval/MATH':\n",
    "    #     return math.compute_score\n",
    "    # elif \"multiply\" in data_source or \"arithmetic\" in data_source:\n",
    "    #     return multiply.compute_score\n",
    "    # elif \"countdown\" in data_source:\n",
    "    #     return countdown.compute_score\n",
    "    # else:\n",
    "    #     raise NotImplementedError\n",
    "    return compute_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5be65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardManager():\n",
    "    \"\"\"The reward manager.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, num_examine) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_examine = num_examine  # the number of batches of decoded responses to print to the console\n",
    "\n",
    "    def __call__(self, data: DataProto):\n",
    "        \"\"\"We will expand this function gradually based on the available datasets\"\"\"\n",
    "\n",
    "        # If there is rm score, we directly return rm score. Otherwise, we compute via rm_score_fn\n",
    "        if 'rm_scores' in data.batch.keys():\n",
    "            return data.batch['rm_scores']\n",
    "\n",
    "        reward_tensor = torch.zeros_like(data.batch['responses'], dtype=torch.float32)\n",
    "\n",
    "        already_print_data_sources = {}\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            data_item = data[i]  # DataProtoItem\n",
    "\n",
    "            prompt_ids = data_item.batch['prompts']\n",
    "\n",
    "            prompt_length = prompt_ids.shape[-1]\n",
    "\n",
    "            valid_prompt_length = data_item.batch['attention_mask'][:prompt_length].sum()\n",
    "            valid_prompt_ids = prompt_ids[-valid_prompt_length:]\n",
    "\n",
    "            response_ids = data_item.batch['responses']\n",
    "            valid_response_length = data_item.batch['attention_mask'][prompt_length:].sum()\n",
    "            valid_response_ids = response_ids[:valid_response_length]\n",
    "\n",
    "            # decode\n",
    "            sequences = torch.cat((valid_prompt_ids, valid_response_ids))\n",
    "            sequences_str = self.tokenizer.decode(sequences)\n",
    "\n",
    "            ground_truth = data_item.non_tensor_batch['reward_model']['ground_truth']\n",
    "\n",
    "            # select rm_score\n",
    "            data_source = data_item.non_tensor_batch['data_source']\n",
    "            compute_score_fn = _select_rm_score_fn(data_source)\n",
    "\n",
    "            score = compute_score_fn(solution_str=sequences_str, ground_truth=ground_truth)\n",
    "            reward_tensor[i, valid_response_length - 1] = score\n",
    "\n",
    "            if data_source not in already_print_data_sources:\n",
    "                already_print_data_sources[data_source] = 0\n",
    "\n",
    "            if already_print_data_sources[data_source] < self.num_examine:\n",
    "                already_print_data_sources[data_source] += 1\n",
    "                print(sequences_str)\n",
    "\n",
    "        return reward_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff4926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ray.remote\n",
    "def main_task(config):\n",
    "    logger.info(f\"„É°„Ç§„É≥„Çø„Çπ„ÇØ„ÇíÈñãÂßã\")\n",
    "\n",
    "    # from verl.utils.fs import copy_local_path_from_hdfs\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    # 1) ÂàùÊúüÂåñ\n",
    "\n",
    "    # 1-1) Ë®≠ÂÆö„ÇíË°®Á§∫\n",
    "    from pprint import pprint\n",
    "    from omegaconf import OmegaConf\n",
    "    # pprint(OmegaConf.to_container(config, resolve=True))\n",
    "    OmegaConf.resolve(config)\n",
    "    logger.debug(f\"Ë®≠ÂÆö: {config}\")\n",
    "\n",
    "    # 1-2) Hadoop File System (HDFS) „Åã„Çâ„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ\n",
    "    local_path = copy_local_path_from_hdfs(config.actor_rollout_ref.model.path)\n",
    "    logger.debug(f\"„É¢„Éá„É´„ÅÆ„É≠„Éº„Ç´„É´„Éë„Çπ: {local_path}\")\n",
    "\n",
    "    # 1-3) „Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÅÆÂàùÊúüÂåñ\n",
    "    from verl.utils import hf_tokenizer\n",
    "    tokenizer = hf_tokenizer(local_path)\n",
    "    logger.debug(f\"„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº: {tokenizer}\")\n",
    "\n",
    "    # 1-4) Ray„ÅÆ„ÉØ„Éº„Ç´„Éº„Ç∞„É´„Éº„Éó„ÅÆÂàùÊúüÂåñ\n",
    "    if config.actor_rollout_ref.actor.strategy == 'fsdp':\n",
    "        logger.debug(\"FSDP„ÉØ„Éº„Ç´„Éº„Ç∞„É´„Éº„Éó„Çí‰ΩøÁî®\")\n",
    "        assert config.actor_rollout_ref.actor.strategy == config.critic.strategy\n",
    "        # from verl.workers.fsdp_workers import ActorRolloutRefWorker, CriticWorker\n",
    "        # from verl.single_controller.ray import RayWorkerGroup\n",
    "        ray_worker_group_cls = RayWorkerGroup\n",
    "\n",
    "    # elif config.actor_rollout_ref.actor.strategy == 'megatron':\n",
    "    #     logger.debug(\"Megatron„ÉØ„Éº„Ç´„Éº„Ç∞„É´„Éº„Éó„Çí‰ΩøÁî®\")\n",
    "    #     assert config.actor_rollout_ref.actor.strategy == config.critic.strategy\n",
    "    #     from verl.workers.megatron_workers import ActorRolloutRefWorker, CriticWorker\n",
    "    #     from verl.single_controller.ray.megatron import NVMegatronRayWorkerGroup\n",
    "    #     ray_worker_group_cls = NVMegatronRayWorkerGroup\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # from verl.trainer.ppo.ray_trainer import ResourcePoolManager, Role\n",
    "\n",
    "    role_worker_mapping = {\n",
    "        Role.ActorRollout: ray.remote(ActorRolloutRefWorker),\n",
    "        Role.Critic: ray.remote(CriticWorker),\n",
    "        Role.RefPolicy: ray.remote(ActorRolloutRefWorker)\n",
    "    }\n",
    "\n",
    "    global_pool_id = 'global_pool'\n",
    "\n",
    "    resource_pool_spec = {\n",
    "        global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes,\n",
    "    }\n",
    "\n",
    "    mapping = {\n",
    "        Role.ActorRollout: global_pool_id,\n",
    "        Role.Critic: global_pool_id,\n",
    "        Role.RefPolicy: global_pool_id,\n",
    "    }\n",
    "\n",
    "    # we should adopt a multi-source reward function here\n",
    "    # - for rule-based rm, we directly call a reward score\n",
    "    # - for model-based rm, we call a model\n",
    "    # - for code related prompt, we send to a sandbox if there are test cases\n",
    "    # - finally, we combine all the rewards together\n",
    "    # - The reward type depends on the tag of the data\n",
    "    if config.reward_model.enable:\n",
    "        if config.reward_model.strategy == 'fsdp':\n",
    "            logger.debug(\"FSDPÂ†±ÈÖ¨„É¢„Éá„É´„ÉØ„Éº„Ç´„Éº„Çí‰ΩøÁî®\")\n",
    "            # from verl.workers.fsdp_workers import RewardModelWorker\n",
    "        # elif config.reward_model.strategy == 'megatron':\n",
    "        #     logger.debug(\"MegatronÂ†±ÈÖ¨„É¢„Éá„É´„ÉØ„Éº„Ç´„Éº„Çí‰ΩøÁî®\")\n",
    "        #     from verl.workers.megatron_workers import RewardModelWorker\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        role_worker_mapping[Role.RewardModel] = ray.remote(RewardModelWorker)\n",
    "\n",
    "        mapping[Role.RewardModel] = global_pool_id\n",
    "    else:\n",
    "        logger.debug(\"Èñ¢Êï∞„Éô„Éº„Çπ„ÅÆÂ†±ÈÖ¨„Éû„Éç„Éº„Ç∏„É£„Éº„Çí‰ΩøÁî®\")\n",
    "\n",
    "    reward_fn = RewardManager(tokenizer=tokenizer, num_examine=0)\n",
    "\n",
    "    # Note that we always use function-based RM for validation\n",
    "    val_reward_fn = RewardManager(tokenizer=tokenizer, num_examine=1)\n",
    "\n",
    "    resource_pool_manager = ResourcePoolManager(\n",
    "        resource_pool_spec=resource_pool_spec,\n",
    "        mapping=mapping\n",
    "    )\n",
    "\n",
    "    trainer = RayPPOTrainer(\n",
    "        config=config,\n",
    "        tokenizer=tokenizer,\n",
    "        role_worker_mapping=role_worker_mapping,\n",
    "        resource_pool_manager=resource_pool_manager,\n",
    "        ray_worker_group_cls=ray_worker_group_cls,\n",
    "        reward_fn=reward_fn,\n",
    "        val_reward_fn=val_reward_fn\n",
    "    )\n",
    "\n",
    "    trainer.init_workers()\n",
    "\n",
    "    trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7faacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ë®ìÁ∑¥„ÇíÈñãÂßã\n",
    "\n",
    "# @hydra.main(config_path='config', config_name='ppo_trainer', version_base=None)\n",
    "def main(config):\n",
    "\n",
    "    # Ray„ÅåÂàùÊúüÂåñ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà\n",
    "    if not ray.is_initialized():\n",
    "        # „É≠„Éº„Ç´„É´„ÅÆRay„ÇØ„É©„Çπ„Çø„Éº„ÇíÂàùÊúüÂåñ\n",
    "        logger.debug(f\"Ray„ÇíÂàùÊúüÂåñ\")\n",
    "        ray.init(runtime_env={\n",
    "            'env_vars': {'TOKENIZERS_PARALLELISM': 'true', 'NCCL_DEBUG': 'WARN'}\n",
    "        })\n",
    "\n",
    "    # „Çø„Çπ„ÇØ„ÇíÂÆüË°å\n",
    "    # ray.get(main_task.remote(config))\n",
    "    main_task(config)\n",
    "\n",
    "main(ppo_trainer_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
