{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f106fd6d",
   "metadata": {},
   "source": [
    "## ç’°å¢ƒæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a35361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"ğŸŸ¦\"\n",
    "        case logging.INFO:\n",
    "            level = \"ğŸŸ©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"ğŸŸ¨\"\n",
    "        case logging.ERROR:\n",
    "            level = \"ğŸŸ¥\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"ğŸ›‘\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(\"debug.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "NVIDIA_SMI = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True).stdout\n",
    "logging.info(NVIDIA_SMI)\n",
    "logging.info(f\"Python {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) PyTorchã¨Transformersã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "%pip install torch==2.4.0\n",
    "\n",
    "# 2) vLLMã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "%pip install vllm==0.6.3\n",
    "\n",
    "# 3) Flash Attentionã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "# 2.8.3ã¯undefined symbolã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ãŸã‚2.7.3ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "# https://github.com/Dao-AILab/flash-attention/issues/1832\n",
    "# %pip install \"https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl\" --no-build-isolation\n",
    "%pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0307c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspaces/tinyzero/TinyZero\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b337fc",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5f272b",
   "metadata": {},
   "source": [
    "```sh\n",
    "python ./examples/data_preprocess/countdown.py --local_dir countdown\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from datasets import Dataset, load_dataset\n",
    "from random import randint, seed, choice\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "from verl.utils.hdfs_io import copy, makedirs\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--local_dir', default='~/data/countdown')\n",
    "parser.add_argument('--hdfs_dir', default=None)\n",
    "parser.add_argument('--num_samples', type=int, default=100000)\n",
    "parser.add_argument('--num_operands', type=int, default=6)\n",
    "parser.add_argument('--max_target', type=int, default=1000)\n",
    "parser.add_argument('--min_number', type=int, default=1)\n",
    "parser.add_argument('--max_number', type=int, default=100)\n",
    "parser.add_argument('--train_size', type=int, default=327680)\n",
    "parser.add_argument('--test_size', type=int, default=1024)\n",
    "parser.add_argument('--template_type', type=str, default='base')\n",
    "\n",
    "args = parser.parse_args([\"--local_dir\", \"countdown\"])\n",
    "\n",
    "data_source = 'countdown'\n",
    "TRAIN_SIZE = args.train_size\n",
    "TEST_SIZE = args.test_size\n",
    "\n",
    "TRAIN_SIZE, TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d509182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4\n",
    "raw_dataset = load_dataset('Jiayi-Pan/Countdown-Tasks-3to4', split='train')\n",
    "len(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e30bb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(raw_dataset) > TRAIN_SIZE + TEST_SIZE\n",
    "train_dataset = raw_dataset.select(range(TRAIN_SIZE))\n",
    "test_dataset = raw_dataset.select(range(TRAIN_SIZE, TRAIN_SIZE + TEST_SIZE))\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prefix(dp, template_type):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        dp (dict): targetã¨numsã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ\n",
    "        template_type (str): ä½¿ç”¨ã™ã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ç¨®é¡\n",
    "            - base: ä¸€èˆ¬çš„ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ç”¨\n",
    "            - qwen-instruct: Qwen Instructãƒ¢ãƒ‡ãƒ«ç”¨\n",
    "\n",
    "    Returns:\n",
    "        str: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹\n",
    "    \"\"\"\n",
    "    target = dp['target']\n",
    "    numbers = dp['nums']\n",
    "\n",
    "    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ\n",
    "    if template_type == 'base':\n",
    "        # User: {numbers} ã‚’ä½¿ã£ã¦ {target} ã«ãªã‚‹æ•°å¼ã‚’ä½œã£ã¦ãã ã•ã„ã€‚\n",
    "        # å››å‰‡æ¼”ç®—ï¼ˆ+, -, *, /ï¼‰ãŒä½¿ãˆã€å„æ•°å­—ã¯ä¸€åº¦ã ã‘ä½¿ãˆã¾ã™ã€‚\n",
    "        # æ€è€ƒéç¨‹ã¯ <think> </think> ã‚¿ã‚°ã§ç¤ºã—ã€æœ€çµ‚çš„ãªç­”ãˆã¯ <answer> </answer> ã‚¿ã‚°ã§\n",
    "        # è¿”ã—ã¦ãã ã•ã„ã€‚ä¾‹: <answer> (1 + 2) / 3 </answer>ã€‚\n",
    "        # Assistant: é †ã‚’è¿½ã£ã¦è§£ã„ã¦ã„ãã¾ã™ã€‚<think>\n",
    "        prefix = f\"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\n",
    "User: Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>.\n",
    "Assistant: Let me solve this step by step.\n",
    "<think>\"\"\"\n",
    "\n",
    "    # Qwen Instructãƒ¢ãƒ‡ãƒ«ã®å ´åˆ\n",
    "    elif template_type == 'qwen-instruct':\n",
    "\n",
    "        # <|im_start|>system\n",
    "        # ã‚ãªãŸã¯æœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ã¾ãšé ­ã®ä¸­ã§æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã‚’è€ƒãˆã€ãã®å¾Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ç­”ãˆã‚’ä¼ãˆã¾ã™ã€‚\n",
    "        # <|im_end|>\n",
    "        # <|im_start|>user\n",
    "        # {numbers} ã‚’ä½¿ã£ã¦ {target} ã«ãªã‚‹æ•°å¼ã‚’ä½œã£ã¦ãã ã•ã„ã€‚\n",
    "        # å››å‰‡æ¼”ç®—ï¼ˆ+, -, *, /ï¼‰ãŒä½¿ãˆã€å„æ•°å­—ã¯ä¸€åº¦ã ã‘ä½¿ãˆã¾ã™ã€‚\n",
    "        # æ€è€ƒéç¨‹ã¯ <think> </think> ã‚¿ã‚°ã§ç¤ºã—ã€æœ€çµ‚çš„ãªç­”ãˆã¯\n",
    "        #  <answer> </answer> ã‚¿ã‚°ã§è¿”ã—ã¦ãã ã•ã„ã€‚\n",
    "        # ä¾‹: <answer> (1 + 2) / 3 </answer>ã€‚\n",
    "        # <|im_end|>\n",
    "        # <|im_start|>assistant\n",
    "        # é †ã‚’è¿½ã£ã¦è§£ã„ã¦ã„ãã¾ã™ã€‚\n",
    "        # <think>\n",
    "        prefix = f\"\"\"<|im_start|>system\\nYou are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.<|im_end|>\\n<|im_start|>user\\n Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>.<|im_end|>\\n<|im_start|>assistant\\nLet me solve this step by step.\\n<think>\"\"\"\n",
    "    return prefix\n",
    "\n",
    "make_prefix(train_dataset[0], 'base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_map_fn(split):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‰å‡¦ç†ã«ä½¿ç”¨ã™ã‚‹ãƒãƒƒãƒ—é–¢æ•°ã‚’ä½œæˆã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        split (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å‰²åï¼ˆä¾‹: 'train', 'test'ï¼‰\n",
    "    Returns:\n",
    "        function: ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’å‡¦ç†ã™ã‚‹ãƒãƒƒãƒ—é–¢æ•°\n",
    "    \"\"\"\n",
    "\n",
    "    def process_fn(example, idx):\n",
    "        \"\"\"\n",
    "        ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’å‰å‡¦ç†ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            example (dict): ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ\n",
    "            idx (int): ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "        Returns:\n",
    "            dict: å‰å‡¦ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ\n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ\n",
    "        question = make_prefix(example, template_type=args.template_type)\n",
    "\n",
    "        solution = {\n",
    "            \"target\": example['target'],\n",
    "            \"numbers\": example['nums']\n",
    "        }\n",
    "\n",
    "        # å‰å‡¦ç†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸å½¢å¼ã§è¿”ã™\n",
    "        data = {\n",
    "            \"data_source\": data_source,\n",
    "            \"prompt\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            }],\n",
    "            \"ability\": \"math\",\n",
    "            \"reward_model\": {\n",
    "                \"style\": \"rule\",\n",
    "                \"ground_truth\": solution\n",
    "            },\n",
    "            \"extra_info\": {\n",
    "                'split': split,\n",
    "                'index': idx,\n",
    "            }\n",
    "        }\n",
    "        return data\n",
    "    return process_fn\n",
    "\n",
    "# æ¤œè¨¼\n",
    "make_map_fn(\"train\")(train_dataset[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb883d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‰å‡¦ç†ã¨Parquetå½¢å¼ã§ã®ä¿å­˜\n",
    "\n",
    "local_dir = args.local_dir\n",
    "\n",
    "if not os.path.exists(os.path.join(local_dir, \"train.parquet\")):\n",
    "    train_dataset = train_dataset.map(function=make_map_fn('train'), with_indices=True)\n",
    "    train_dataset.to_parquet(os.path.join(local_dir, 'train.parquet'))\n",
    "\n",
    "if not os.path.exists(os.path.join(local_dir, \"test.parquet\")):\n",
    "    test_dataset = test_dataset.map(function=make_map_fn('test'), with_indices=True)\n",
    "    test_dataset.to_parquet(os.path.join(local_dir, 'test.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06016cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hadoop File System (HDFS) ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼\n",
    "# åˆ†æ•£å‡¦ç†ç”¨\n",
    "\n",
    "hdfs_dir = args.hdfs_dir\n",
    "\n",
    "if hdfs_dir is not None:\n",
    "    makedirs(hdfs_dir)\n",
    "    copy(src=local_dir, dst=hdfs_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cfd477",
   "metadata": {},
   "source": [
    "## è¨“ç·´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe9a36e",
   "metadata": {},
   "source": [
    "```sh\n",
    "N_GPUS=1 BASE_MODEL=\"qwen/qwen2.5-0.5b\" DATA_DIR=\"countdown\" ROLLOUT_TP_SIZE=1 VLLM_ATTENTION_BACKEND=\"XFORMERS\" bash ./scripts/train_tiny_zero.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2931326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from verl import DataProto\n",
    "import torch\n",
    "from verl.utils.reward_score import gsm8k, math, multiply, countdown\n",
    "from verl.trainer.ppo.ray_trainer import RayPPOTrainer\n",
    "import ray\n",
    "import hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã®ç’°å¢ƒå¤‰æ•°ã‚’å¤‰æ•°åŒ–\n",
    "\n",
    "BASE_MODEL=\"qwen/qwen2.5-0.5b\"\n",
    "DATA_DIR = \"countdown\"\n",
    "EXPERIMENT_NAME = \"countdown_ppo_demo\"\n",
    "N_GPUS = 1\n",
    "ROLLOUT_TP_SIZE = 1\n",
    "VLLM_ATTENTION_BACKEND = \"XFORMERS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPOãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’èª­ã¿è¾¼ã¿\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# TinyZero/verl/trainer/config/ppo_trainer.yaml\n",
    "ppo_trainer_yaml = \"\"\"\n",
    "data:\n",
    "  tokenizer: null\n",
    "  train_files: ~/data/rlhf/gsm8k/train.parquet\n",
    "  val_files: ~/data/rlhf/gsm8k/test.parquet\n",
    "  prompt_key: prompt\n",
    "  max_prompt_length: 512\n",
    "  max_response_length: 512\n",
    "  train_batch_size: 1024\n",
    "  val_batch_size: 1312\n",
    "  return_raw_input_ids: False  # This should be set to true when the tokenizer between policy and rm differs\n",
    "  return_raw_chat: False\n",
    "\n",
    "actor_rollout_ref:\n",
    "  hybrid_engine: True\n",
    "  model:\n",
    "    path: ~/models/deepseek-llm-7b-chat\n",
    "    external_lib: null\n",
    "    override_config: { }\n",
    "    enable_gradient_checkpointing: False\n",
    "    use_remove_padding: False\n",
    "  actor:\n",
    "    strategy: fsdp  # This is for backward-compatibility\n",
    "    ppo_mini_batch_size: 256\n",
    "    ppo_micro_batch_size: 64\n",
    "    use_dynamic_bsz: False\n",
    "    ppo_max_token_len_per_gpu: 16384 # n * ${data.max_prompt_length} + ${data.max_response_length}\n",
    "    grad_clip: 1.0\n",
    "    clip_ratio: 0.2\n",
    "    entropy_coeff: 0.001\n",
    "    use_kl_loss: False # True for GRPO\n",
    "    kl_loss_coef: 0.001 # for grpo\n",
    "    kl_loss_type: low_var_kl # for grpo\n",
    "    ppo_epochs: 1\n",
    "    shuffle: False\n",
    "    ulysses_sequence_parallel_size: 1 # sp size\n",
    "    optim:\n",
    "      lr: 1e-6\n",
    "      lr_warmup_steps_ratio: 0.  # the total steps will be injected during runtime\n",
    "      min_lr_ratio: null   # only useful for warmup with cosine\n",
    "      warmup_style: constant  # select from constant/cosine\n",
    "      total_training_steps: -1  # must be override by program\n",
    "    fsdp_config:\n",
    "      wrap_policy:\n",
    "        # transformer_layer_cls_to_wrap: None\n",
    "        min_num_params: 0\n",
    "      param_offload: False\n",
    "      grad_offload: False\n",
    "      optimizer_offload: False\n",
    "      fsdp_size: -1\n",
    "  ref:\n",
    "    fsdp_config:\n",
    "      param_offload: False\n",
    "      wrap_policy:\n",
    "        # transformer_layer_cls_to_wrap: None\n",
    "        min_num_params: 0\n",
    "      fsdp_size: -1\n",
    "    log_prob_micro_batch_size: 128\n",
    "    log_prob_use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}\n",
    "    log_prob_max_token_len_per_gpu: ${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}\n",
    "    ulysses_sequence_parallel_size: ${actor_rollout_ref.actor.ulysses_sequence_parallel_size} # sp size\n",
    "  rollout:\n",
    "    name: vllm\n",
    "    temperature: 1.0\n",
    "    top_k: -1 # 0 for hf rollout, -1 for vllm rollout\n",
    "    top_p: 1\n",
    "    prompt_length: ${data.max_prompt_length}  # not use for opensource\n",
    "    response_length: ${data.max_response_length}\n",
    "    # for vllm rollout\n",
    "    dtype: bfloat16 # should align with FSDP\n",
    "    gpu_memory_utilization: 0.5\n",
    "    ignore_eos: False\n",
    "    enforce_eager: True\n",
    "    free_cache_engine: True\n",
    "    load_format: dummy_dtensor\n",
    "    tensor_model_parallel_size: 2\n",
    "    max_num_batched_tokens: 8192\n",
    "    max_num_seqs: 1024\n",
    "    log_prob_micro_batch_size: 128\n",
    "    log_prob_use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}\n",
    "    log_prob_max_token_len_per_gpu: ${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}\n",
    "    # for hf rollout\n",
    "    do_sample: True\n",
    "    # number of responses (i.e. num sample times)\n",
    "    n: 1 # > 1 for grpo\n",
    "\n",
    "critic:\n",
    "  strategy: fsdp\n",
    "  optim:\n",
    "    lr: 1e-5\n",
    "    lr_warmup_steps_ratio: 0.  # the total steps will be injected during runtime\n",
    "    min_lr_ratio: null   # only useful for warmup with cosine\n",
    "    warmup_style: constant  # select from constant/cosine\n",
    "    total_training_steps: -1  # must be override by program\n",
    "  model:\n",
    "    path: ~/models/deepseek-llm-7b-chat\n",
    "    tokenizer_path: ${actor_rollout_ref.model.path}\n",
    "    override_config: { }\n",
    "    external_lib: ${actor_rollout_ref.model.external_lib}\n",
    "    enable_gradient_checkpointing: False\n",
    "    use_remove_padding: False\n",
    "    fsdp_config:\n",
    "      param_offload: False\n",
    "      grad_offload: False\n",
    "      optimizer_offload: False\n",
    "      wrap_policy:\n",
    "        # transformer_layer_cls_to_wrap: None\n",
    "        min_num_params: 0\n",
    "      fsdp_size: -1\n",
    "  ppo_mini_batch_size: ${actor_rollout_ref.actor.ppo_mini_batch_size}\n",
    "  ppo_micro_batch_size: 64\n",
    "  forward_micro_batch_size: ${critic.ppo_micro_batch_size}\n",
    "  use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}\n",
    "  ppo_max_token_len_per_gpu: 32768 # (${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}) * 2\n",
    "  forward_max_token_len_per_gpu: ${critic.ppo_max_token_len_per_gpu}\n",
    "  ulysses_sequence_parallel_size: 1 # sp size\n",
    "  ppo_epochs: ${actor_rollout_ref.actor.ppo_epochs}\n",
    "  shuffle: ${actor_rollout_ref.actor.shuffle}\n",
    "  grad_clip: 1.0\n",
    "  cliprange_value: 0.5\n",
    "\n",
    "reward_model:\n",
    "  enable: False\n",
    "  strategy: fsdp\n",
    "  model:\n",
    "    input_tokenizer: ${actor_rollout_ref.model.path}  # set this to null if the chat template is identical\n",
    "    path: ~/models/FsfairX-LLaMA3-RM-v0.1\n",
    "    external_lib: ${actor_rollout_ref.model.external_lib}\n",
    "    use_remove_padding: False\n",
    "    fsdp_config:\n",
    "      min_num_params: 0\n",
    "      param_offload: False\n",
    "  micro_batch_size: 64\n",
    "  max_length: null\n",
    "  ulysses_sequence_parallel_size: 1 # sp size\n",
    "  use_dynamic_bsz: ${critic.use_dynamic_bsz}\n",
    "  forward_max_token_len_per_gpu: ${critic.forward_max_token_len_per_gpu}\n",
    "\n",
    "algorithm:\n",
    "  gamma: 1.0\n",
    "  lam: 1.0\n",
    "  adv_estimator: gae\n",
    "  kl_penalty: kl  # how to estimate kl divergence\n",
    "  kl_ctrl:\n",
    "    type: fixed\n",
    "    kl_coef: 0.001\n",
    "\n",
    "trainer:\n",
    "  total_epochs: 30\n",
    "  total_training_steps: null\n",
    "  project_name: verl_examples\n",
    "  experiment_name: gsm8k\n",
    "  logger: [ 'console', 'wandb' ]\n",
    "  nnodes: 1\n",
    "  n_gpus_per_node: 8\n",
    "  save_freq: -1\n",
    "  test_freq: -1\n",
    "  critic_warmup: 0\n",
    "  default_hdfs_dir: ~/experiments/gsm8k/ppo/${trainer.experiment_name}\n",
    "  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}\n",
    "\"\"\"\n",
    "\n",
    "ppo_trainer_config = OmegaConf.create(ppo_trainer_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684dfd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä¸Šæ›¸ã\n",
    "\n",
    "# TinyZero/scripts/train_tiny_zero.shã‚ˆã‚ŠæŠœç²‹\n",
    "# python3 -m verl.trainer.main_ppo \\\n",
    "# data.train_files=$DATA_DIR/train.parquet \\\n",
    "# data.val_files=$DATA_DIR/test.parquet \\\n",
    "# data.train_batch_size=32 \\\n",
    "# data.val_batch_size=32 \\\n",
    "# data.max_prompt_length=256 \\\n",
    "# data.max_response_length=1024 \\\n",
    "# actor_rollout_ref.model.path=$BASE_MODEL \\\n",
    "# actor_rollout_ref.model.use_remove_padding=True \\\n",
    "# actor_rollout_ref.model.enable_gradient_checkpointing=True \\\n",
    "# actor_rollout_ref.actor.use_dynamic_bsz=True \\\n",
    "# actor_rollout_ref.actor.optim.lr=1e-6 \\\n",
    "# actor_rollout_ref.actor.ppo_mini_batch_size=4 \\\n",
    "# actor_rollout_ref.actor.ppo_micro_batch_size=4 \\\n",
    "# actor_rollout_ref.rollout.log_prob_micro_batch_size=1 \\\n",
    "# actor_rollout_ref.rollout.tensor_model_parallel_size=$ROLLOUT_TP_SIZE \\\n",
    "# actor_rollout_ref.rollout.gpu_memory_utilization=0.1 \\\n",
    "# actor_rollout_ref.ref.log_prob_micro_batch_size=1 \\\n",
    "# critic.optim.lr=1e-5 \\\n",
    "# critic.model.path=$BASE_MODEL \\\n",
    "# critic.ppo_micro_batch_size=1 \\\n",
    "# algorithm.kl_ctrl.kl_coef=0.001 \\\n",
    "# trainer.logger=[] \\\n",
    "# +trainer.val_before_train=False \\\n",
    "# trainer.default_hdfs_dir=null \\\n",
    "# trainer.n_gpus_per_node=$N_GPUS \\\n",
    "# trainer.nnodes=1 \\\n",
    "# trainer.save_freq=100 \\\n",
    "# trainer.test_freq=100 \\\n",
    "# trainer.project_name=TinyZero \\\n",
    "# trainer.experiment_name=$EXPERIMENT_NAME \\\n",
    "# trainer.total_epochs=15 2>&1 | tee verl_demo.log\n",
    "\n",
    "ppo_trainer_config.data.train_files = os.path.join(DATA_DIR, \"train.parquet\")\n",
    "ppo_trainer_config.data.val_files = os.path.join(DATA_DIR, \"test.parquet\")\n",
    "ppo_trainer_config.data.train_batch_size = 32\n",
    "ppo_trainer_config.data.val_batch_size = 32\n",
    "ppo_trainer_config.data.max_prompt_length = 256\n",
    "ppo_trainer_config.data.max_response_length = 1024\n",
    "ppo_trainer_config.actor_rollout_ref.model.path = BASE_MODEL\n",
    "ppo_trainer_config.actor_rollout_ref.model.use_remove_padding = True\n",
    "ppo_trainer_config.actor_rollout_ref.model.enable_gradient_checkpointing = True\n",
    "ppo_trainer_config.actor_rollout_ref.actor.use_dynamic_bsz = True\n",
    "ppo_trainer_config.actor_rollout_ref.actor.optim.lr = 1e-6\n",
    "ppo_trainer_config.actor_rollout_ref.actor.ppo_mini_batch_size = 4\n",
    "ppo_trainer_config.actor_rollout_ref.actor.ppo_micro_batch_size = 4\n",
    "ppo_trainer_config.actor_rollout_ref.rollout.log_prob_micro_batch_size = 1\n",
    "ppo_trainer_config.actor_rollout_ref.rollout.tensor_model_parallel_size = ROLLOUT_TP_SIZE\n",
    "ppo_trainer_config.actor_rollout_ref.rollout.gpu_memory_utilization = 0.1\n",
    "ppo_trainer_config.actor_rollout_ref.ref.log_prob_micro_batch_size = 1\n",
    "ppo_trainer_config.critic.optim.lr = 1e-5\n",
    "ppo_trainer_config.critic.model.path = BASE_MODEL\n",
    "ppo_trainer_config.critic.ppo_micro_batch_size = 1\n",
    "ppo_trainer_config.algorithm.kl_ctrl.kl_coef = 0.001\n",
    "ppo_trainer_config.trainer.logger = []\n",
    "ppo_trainer_config.trainer.val_before_train = False\n",
    "ppo_trainer_config.trainer.default_hdfs_dir = None\n",
    "ppo_trainer_config.trainer.n_gpus_per_node = N_GPUS\n",
    "ppo_trainer_config.trainer.nnodes = 1\n",
    "ppo_trainer_config.trainer.save_freq = 100\n",
    "ppo_trainer_config.trainer.test_freq = 100\n",
    "ppo_trainer_config.trainer.project_name = \"TinyZero\"\n",
    "ppo_trainer_config.trainer.experiment_name = EXPERIMENT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1c62c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_rm_score_fn(data_source):\n",
    "    if data_source == 'openai/gsm8k':\n",
    "        return gsm8k.compute_score\n",
    "    elif data_source == 'lighteval/MATH':\n",
    "        return math.compute_score\n",
    "    elif \"multiply\" in data_source or \"arithmetic\" in data_source:\n",
    "        return multiply.compute_score\n",
    "    elif \"countdown\" in data_source:\n",
    "        return countdown.compute_score\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5be65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardManager():\n",
    "    \"\"\"The reward manager.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, num_examine) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_examine = num_examine  # the number of batches of decoded responses to print to the console\n",
    "\n",
    "    def __call__(self, data: DataProto):\n",
    "        \"\"\"We will expand this function gradually based on the available datasets\"\"\"\n",
    "\n",
    "        # If there is rm score, we directly return rm score. Otherwise, we compute via rm_score_fn\n",
    "        if 'rm_scores' in data.batch.keys():\n",
    "            return data.batch['rm_scores']\n",
    "\n",
    "        reward_tensor = torch.zeros_like(data.batch['responses'], dtype=torch.float32)\n",
    "\n",
    "        already_print_data_sources = {}\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            data_item = data[i]  # DataProtoItem\n",
    "\n",
    "            prompt_ids = data_item.batch['prompts']\n",
    "\n",
    "            prompt_length = prompt_ids.shape[-1]\n",
    "\n",
    "            valid_prompt_length = data_item.batch['attention_mask'][:prompt_length].sum()\n",
    "            valid_prompt_ids = prompt_ids[-valid_prompt_length:]\n",
    "\n",
    "            response_ids = data_item.batch['responses']\n",
    "            valid_response_length = data_item.batch['attention_mask'][prompt_length:].sum()\n",
    "            valid_response_ids = response_ids[:valid_response_length]\n",
    "\n",
    "            # decode\n",
    "            sequences = torch.cat((valid_prompt_ids, valid_response_ids))\n",
    "            sequences_str = self.tokenizer.decode(sequences)\n",
    "\n",
    "            ground_truth = data_item.non_tensor_batch['reward_model']['ground_truth']\n",
    "\n",
    "            # select rm_score\n",
    "            data_source = data_item.non_tensor_batch['data_source']\n",
    "            compute_score_fn = _select_rm_score_fn(data_source)\n",
    "\n",
    "            score = compute_score_fn(solution_str=sequences_str, ground_truth=ground_truth)\n",
    "            reward_tensor[i, valid_response_length - 1] = score\n",
    "\n",
    "            if data_source not in already_print_data_sources:\n",
    "                already_print_data_sources[data_source] = 0\n",
    "\n",
    "            if already_print_data_sources[data_source] < self.num_examine:\n",
    "                already_print_data_sources[data_source] += 1\n",
    "                print(sequences_str)\n",
    "\n",
    "        return reward_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff4926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ray.remote\n",
    "def main_task(config):\n",
    "    logger.info(f\"ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹\")\n",
    "\n",
    "    from verl.utils.fs import copy_local_path_from_hdfs\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    # 1) åˆæœŸåŒ–\n",
    "\n",
    "    # 1-1) è¨­å®šã‚’è¡¨ç¤º\n",
    "    from pprint import pprint\n",
    "    from omegaconf import OmegaConf\n",
    "    # pprint(OmegaConf.to_container(config, resolve=True))\n",
    "    OmegaConf.resolve(config)\n",
    "    logger.debug(f\"è¨­å®š: {config}\")\n",
    "\n",
    "    # 1-2) Hadoop File System (HDFS) ã‹ã‚‰ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "    local_path = copy_local_path_from_hdfs(config.actor_rollout_ref.model.path)\n",
    "    logger.debug(f\"ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹: {local_path}\")\n",
    "\n",
    "    # 1-3) ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–\n",
    "    from verl.utils import hf_tokenizer\n",
    "    tokenizer = hf_tokenizer(local_path)\n",
    "    logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼: {tokenizer}\")\n",
    "\n",
    "    # 1-4) Rayã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã®åˆæœŸåŒ–\n",
    "    if config.actor_rollout_ref.actor.strategy == 'fsdp':\n",
    "        logger.debug(\"FSDPãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨\")\n",
    "        assert config.actor_rollout_ref.actor.strategy == config.critic.strategy\n",
    "        from verl.workers.fsdp_workers import ActorRolloutRefWorker, CriticWorker\n",
    "        from verl.single_controller.ray import RayWorkerGroup\n",
    "        ray_worker_group_cls = RayWorkerGroup\n",
    "\n",
    "    elif config.actor_rollout_ref.actor.strategy == 'megatron':\n",
    "        logger.debug(\"Megatronãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨\")\n",
    "        assert config.actor_rollout_ref.actor.strategy == config.critic.strategy\n",
    "        from verl.workers.megatron_workers import ActorRolloutRefWorker, CriticWorker\n",
    "        from verl.single_controller.ray.megatron import NVMegatronRayWorkerGroup\n",
    "        ray_worker_group_cls = NVMegatronRayWorkerGroup\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    from verl.trainer.ppo.ray_trainer import ResourcePoolManager, Role\n",
    "\n",
    "    role_worker_mapping = {\n",
    "        Role.ActorRollout: ray.remote(ActorRolloutRefWorker),\n",
    "        Role.Critic: ray.remote(CriticWorker),\n",
    "        Role.RefPolicy: ray.remote(ActorRolloutRefWorker)\n",
    "    }\n",
    "\n",
    "    global_pool_id = 'global_pool'\n",
    "\n",
    "    resource_pool_spec = {\n",
    "        global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes,\n",
    "    }\n",
    "\n",
    "    mapping = {\n",
    "        Role.ActorRollout: global_pool_id,\n",
    "        Role.Critic: global_pool_id,\n",
    "        Role.RefPolicy: global_pool_id,\n",
    "    }\n",
    "\n",
    "    # we should adopt a multi-source reward function here\n",
    "    # - for rule-based rm, we directly call a reward score\n",
    "    # - for model-based rm, we call a model\n",
    "    # - for code related prompt, we send to a sandbox if there are test cases\n",
    "    # - finally, we combine all the rewards together\n",
    "    # - The reward type depends on the tag of the data\n",
    "    if config.reward_model.enable:\n",
    "        if config.reward_model.strategy == 'fsdp':\n",
    "            logger.debug(\"FSDPå ±é…¬ãƒ¢ãƒ‡ãƒ«ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½¿ç”¨\")\n",
    "            from verl.workers.fsdp_workers import RewardModelWorker\n",
    "        elif config.reward_model.strategy == 'megatron':\n",
    "            logger.debug(\"Megatronå ±é…¬ãƒ¢ãƒ‡ãƒ«ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½¿ç”¨\")\n",
    "            from verl.workers.megatron_workers import RewardModelWorker\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        role_worker_mapping[Role.RewardModel] = ray.remote(RewardModelWorker)\n",
    "\n",
    "        mapping[Role.RewardModel] = global_pool_id\n",
    "    else:\n",
    "        logger.debug(\"é–¢æ•°ãƒ™ãƒ¼ã‚¹ã®å ±é…¬ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½¿ç”¨\")\n",
    "\n",
    "    reward_fn = RewardManager(tokenizer=tokenizer, num_examine=0)\n",
    "\n",
    "    # Note that we always use function-based RM for validation\n",
    "    val_reward_fn = RewardManager(tokenizer=tokenizer, num_examine=1)\n",
    "\n",
    "    resource_pool_manager = ResourcePoolManager(\n",
    "        resource_pool_spec=resource_pool_spec,\n",
    "        mapping=mapping\n",
    "    )\n",
    "\n",
    "    trainer = RayPPOTrainer(\n",
    "        config=config,\n",
    "        tokenizer=tokenizer,\n",
    "        role_worker_mapping=role_worker_mapping,\n",
    "        resource_pool_manager=resource_pool_manager,\n",
    "        ray_worker_group_cls=ray_worker_group_cls,\n",
    "        reward_fn=reward_fn,\n",
    "        val_reward_fn=val_reward_fn\n",
    "    )\n",
    "\n",
    "    trainer.init_workers()\n",
    "\n",
    "    trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7faacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ã‚’é–‹å§‹\n",
    "\n",
    "# @hydra.main(config_path='config', config_name='ppo_trainer', version_base=None)\n",
    "def main(config):\n",
    "\n",
    "    # RayãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆ\n",
    "    if not ray.is_initialized():\n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«ã®Rayã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–\n",
    "        logger.debug(f\"Rayã‚’åˆæœŸåŒ–\")\n",
    "        ray.init(runtime_env={\n",
    "            'env_vars': {'TOKENIZERS_PARALLELISM': 'true', 'NCCL_DEBUG': 'WARN'}\n",
    "        })\n",
    "\n",
    "    # ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ\n",
    "    # ray.get(main_task.remote(config))\n",
    "    main_task(config)\n",
    "\n",
    "main(ppo_trainer_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
