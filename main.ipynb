{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a35361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"ðŸŸ¦\"\n",
    "        case logging.INFO:\n",
    "            level = \"ðŸŸ©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"ðŸŸ¨\"\n",
    "        case logging.ERROR:\n",
    "            level = \"ðŸŸ¥\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"ðŸ›‘\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(\"debug.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "NVIDIA_SMI = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True).stdout\n",
    "logging.info(NVIDIA_SMI)\n",
    "logging.info(f\"Python {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) PyTorchã¨Transformersã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "%pip install torch==2.4.0\n",
    "\n",
    "# 2) vLLMã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "%pip install vllm==0.6.3\n",
    "\n",
    "# 3) Flash Attentionã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "# 2.8.3ã¯undefined symbolã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ãŸã‚2.7.3ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "# https://github.com/Dao-AILab/flash-attention/issues/1832\n",
    "# %pip install \"https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl\" --no-build-isolation\n",
    "%pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0307c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspaces/tinyzero/TinyZero\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ff02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./examples/data_preprocess/countdown.py --local_dir countdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f60e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m verl.trainer.main_ppo \\\n",
    "data.train_files=$DATA_DIR/train.parquet \\\n",
    "data.val_files=$DATA_DIR/test.parquet \\\n",
    "data.train_batch_size=32 \\\n",
    "data.val_batch_size=32 \\\n",
    "data.max_prompt_length=256 \\\n",
    "data.max_response_length=1024 \\\n",
    "actor_rollout_ref.model.path=$BASE_MODEL \\\n",
    "actor_rollout_ref.model.use_remove_padding=True \\\n",
    "actor_rollout_ref.model.enable_gradient_checkpointing=True \\\n",
    "actor_rollout_ref.actor.use_dynamic_bsz=True \\\n",
    "actor_rollout_ref.actor.optim.lr=1e-6 \\\n",
    "actor_rollout_ref.actor.ppo_mini_batch_size=4 \\\n",
    "actor_rollout_ref.actor.ppo_micro_batch_size=4 \\\n",
    "actor_rollout_ref.rollout.log_prob_micro_batch_size=1 \\\n",
    "actor_rollout_ref.rollout.tensor_model_parallel_size=$ROLLOUT_TP_SIZE \\\n",
    "actor_rollout_ref.rollout.gpu_memory_utilization=0.1 \\\n",
    "actor_rollout_ref.ref.log_prob_micro_batch_size=1 \\\n",
    "critic.optim.lr=1e-5 \\\n",
    "critic.model.path=$BASE_MODEL \\\n",
    "critic.ppo_micro_batch_size=1 \\\n",
    "algorithm.kl_ctrl.kl_coef=0.001 \\\n",
    "trainer.logger=[] \\\n",
    "+trainer.val_before_train=False \\\n",
    "trainer.default_hdfs_dir=null \\\n",
    "trainer.n_gpus_per_node=$N_GPUS \\\n",
    "trainer.nnodes=1 \\\n",
    "trainer.save_freq=100 \\\n",
    "trainer.test_freq=100 \\\n",
    "trainer.project_name=TinyZero \\\n",
    "trainer.experiment_name=$EXPERIMENT_NAME \\\n",
    "trainer.total_epochs=15 2>&1 | tee verl_demo.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2931326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export N_GPUS=1\n",
    "# export BASE_MODEL={path_to_your_model}\n",
    "# export DATA_DIR={path_to_your_dataset}\n",
    "# export ROLLOUT_TP_SIZE=1\n",
    "# export EXPERIMENT_NAME=countdown-qwen2.5-0.5b\n",
    "# export VLLM_ATTENTION_BACKEND=XFORMERS\n",
    "# bash ./scripts/train_tiny_zero.sh\n",
    "!N_GPUS=1 BASE_MODEL=\"qwen/qwen2.5-0.5b\" DATA_DIR=\"countdown\" ROLLOUT_TP_SIZE=1 VLLM_ATTENTION_BACKEND=\"XFORMERS\" bash ./scripts/train_tiny_zero.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ec1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export N_GPUS=2\n",
    "# export BASE_MODEL={path_to_your_model}\n",
    "# export DATA_DIR={path_to_your_dataset}\n",
    "# export ROLLOUT_TP_SIZE=2\n",
    "# export EXPERIMENT_NAME=countdown-qwen2.5-3b\n",
    "# export VLLM_ATTENTION_BACKEND=XFORMERS\n",
    "# bash ./scripts/train_tiny_zero.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
