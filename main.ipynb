{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f106fd6d",
   "metadata": {},
   "source": [
    "## ç’°å¢ƒæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b9e317",
   "metadata": {},
   "source": [
    "### ãƒ­ã‚°åˆæœŸåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a35361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"ğŸŸ¦\"\n",
    "        case logging.INFO:\n",
    "            level = \"ğŸŸ©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"ğŸŸ¨\"\n",
    "        case logging.ERROR:\n",
    "            level = \"ğŸŸ¥\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"ğŸ›‘\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(\"debug.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "NVIDIA_SMI = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True).stdout\n",
    "logging.info(NVIDIA_SMI)\n",
    "logging.info(f\"Python {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d22384",
   "metadata": {},
   "source": [
    "### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) PyTorchã¨Transformersã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "%pip install torch==2.4.0\n",
    "\n",
    "# 2) vLLMã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "%pip install vllm==0.6.3\n",
    "\n",
    "# 3) Flash Attentionã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "# 2.8.3ã¯undefined symbolã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ãŸã‚2.7.3ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "# https://github.com/Dao-AILab/flash-attention/issues/1832\n",
    "# %pip install \"https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl\" --no-build-isolation\n",
    "%pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0307c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspaces/tinyzero/TinyZero\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b6646",
   "metadata": {},
   "source": [
    "### ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a471fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codetiming import Timer\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass, field\n",
    "from datasets import Dataset, load_dataset\n",
    "from enum import Enum\n",
    "from filelock import FileLock\n",
    "from flash_attn.bert_padding import pad_input, unpad_input, index_first_axis, rearrange\n",
    "from omegaconf import DictConfig, open_dict, OmegaConf\n",
    "from pprint import pprint\n",
    "from random import randint, seed, choice\n",
    "from ray.experimental.state.api import get_actor\n",
    "from ray.util import list_named_actors\n",
    "from ray.util.placement_group import placement_group, PlacementGroup\n",
    "from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy, NodeAffinitySchedulingStrategy\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy, CPUOffload, MixedPrecision, StateDictType, FullStateDictConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoConfig, AutoModelForTokenClassification, AutoModelForCausalLM\n",
    "from typing import Dict, List, Any, Tuple, Type\n",
    "import argparse\n",
    "import ast\n",
    "import hashlib\n",
    "import hydra\n",
    "import itertools\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import random\n",
    "import ray\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tempfile\n",
    "import time\n",
    "import torch\n",
    "import torch.distributed\n",
    "import uuid\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from verl import DataProto\n",
    "from verl.models.registry import check_model_support_rmpad\n",
    "from verl.models.transformers.monkey_patch import apply_monkey_patch\n",
    "from verl.protocol import pad_dataproto_to_divisor, unpad_dataproto\n",
    "from verl.single_controller.base import WorkerGroup, ResourcePool, ClassWithInitArgs, Worker\n",
    "from verl.single_controller.base.decorator import register, Dispatch\n",
    "from verl.single_controller.ray.base import create_colocated_worker_cls\n",
    "from verl.trainer.ppo import core_algos\n",
    "from verl.utils import hf_tokenizer\n",
    "from verl.utils.dataset.rl_dataset import RLHFDataset, collate_fn\n",
    "from verl.utils.debug import log_gpu_memory_usage\n",
    "from verl.utils.flops_counter import FlopsCounter\n",
    "# from verl.utils.fs import copy_local_path_from_hdfs\n",
    "from verl.utils.fsdp_utils import get_fsdp_wrap_policy, offload_fsdp_grad, init_fn, get_init_weight_context_manager, offload_fsdp_optimizer, offload_fsdp_param_and_grad, load_fsdp_optimizer, load_fsdp_param_and_grad\n",
    "from verl.utils.hdfs_io import copy, makedirs\n",
    "from verl.utils.import_utils import import_external_libs\n",
    "from verl.utils.model import compute_position_id_with_mask, LambdaLayer, print_model_size, squeeze, update_model_config\n",
    "from verl.utils.seqlen_balancing import get_seqlen_balanced_partitions, log_seqlen_unbalance, rearrange_micro_batches, get_reverse_idx\n",
    "from verl.utils.torch_dtypes import PrecisionType\n",
    "from verl.utils.torch_functional import get_constant_schedule_with_warmup\n",
    "from verl.utils.tracking import Tracking\n",
    "from verl.utils.ulysses import ulysses_pad_and_slice_inputs, gather_outpus_and_unpad\n",
    "from verl.workers.actor import DataParallelPPOActor\n",
    "from verl.workers.critic import DataParallelPPOCritic\n",
    "from verl.workers.rollout import HFRollout\n",
    "from verl.workers.rollout.vllm_rollout import vLLMRollout\n",
    "from verl.workers.sharding_manager import BaseShardingManager, FSDPVLLMShardingManager\n",
    "from verl.workers.sharding_manager.fsdp_ulysses import FSDPUlyssesShardingManager\n",
    "import verl.utils.hdfs_io as hdfs_io\n",
    "import verl.utils.torch_functional as verl_F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b337fc",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5f272b",
   "metadata": {},
   "source": [
    "[Countdown-Task-3to4][1]ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒãƒ£ãƒƒãƒˆå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹:\n",
    "\n",
    "```sh\n",
    "python ./examples/data_preprocess/countdown.py --local_dir countdown\n",
    "```\n",
    "\n",
    "[1]: https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--local_dir', default='~/data/countdown')\n",
    "parser.add_argument('--hdfs_dir', default=None)\n",
    "parser.add_argument('--num_samples', type=int, default=100000)\n",
    "parser.add_argument('--num_operands', type=int, default=6)\n",
    "parser.add_argument('--max_target', type=int, default=1000)\n",
    "parser.add_argument('--min_number', type=int, default=1)\n",
    "parser.add_argument('--max_number', type=int, default=100)\n",
    "parser.add_argument('--train_size', type=int, default=327680)\n",
    "parser.add_argument('--test_size', type=int, default=1024)\n",
    "parser.add_argument('--template_type', type=str, default='base')\n",
    "\n",
    "args = parser.parse_args([\"--local_dir\", \"countdown\"])\n",
    "\n",
    "# ãƒ‡ãƒãƒƒã‚°ç”¨ã«ä¸Šæ›¸ã\n",
    "args.train_size = 32 \n",
    "args.test_size = 32\n",
    "\n",
    "data_source = 'countdown'\n",
    "TRAIN_SIZE = args.train_size\n",
    "TEST_SIZE = args.test_size\n",
    "\n",
    "TRAIN_SIZE, TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d509182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4\n",
    "raw_dataset = load_dataset('Jiayi-Pan/Countdown-Tasks-3to4', split='train')\n",
    "logger.info(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚º: {len(raw_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e30bb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(raw_dataset) > TRAIN_SIZE + TEST_SIZE\n",
    "train_dataset = raw_dataset.select(range(TRAIN_SIZE))\n",
    "test_dataset = raw_dataset.select(range(TRAIN_SIZE, TRAIN_SIZE + TEST_SIZE))\n",
    "\n",
    "logger.info(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚º: {len(train_dataset)}\")\n",
    "logger.info(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚º: {len(test_dataset)}\")\n",
    "logger.info(f\"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿: {train_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prefix(dp, template_type):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        dp (dict): targetã¨numsã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ\n",
    "        template_type (str): ä½¿ç”¨ã™ã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ç¨®é¡\n",
    "            - base: ä¸€èˆ¬çš„ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ç”¨\n",
    "            - qwen-instruct: Qwen Instructãƒ¢ãƒ‡ãƒ«ç”¨\n",
    "\n",
    "    Returns:\n",
    "        str: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹\n",
    "    \"\"\"\n",
    "    target = dp['target']\n",
    "    numbers = dp['nums']\n",
    "\n",
    "    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ\n",
    "    if template_type == 'base':\n",
    "        # User: {numbers} ã‚’ä½¿ã£ã¦ {target} ã«ãªã‚‹æ•°å¼ã‚’ä½œã£ã¦ãã ã•ã„ã€‚\n",
    "        # å››å‰‡æ¼”ç®—ï¼ˆ+, -, *, /ï¼‰ãŒä½¿ãˆã€å„æ•°å­—ã¯ä¸€åº¦ã ã‘ä½¿ãˆã¾ã™ã€‚\n",
    "        # æ€è€ƒéç¨‹ã¯ <think> </think> ã‚¿ã‚°ã§ç¤ºã—ã€æœ€çµ‚çš„ãªç­”ãˆã¯ <answer> </answer> ã‚¿ã‚°ã§\n",
    "        # è¿”ã—ã¦ãã ã•ã„ã€‚ä¾‹: <answer> (1 + 2) / 3 </answer>ã€‚\n",
    "        # Assistant: é †ã‚’è¿½ã£ã¦è§£ã„ã¦ã„ãã¾ã™ã€‚<think>\n",
    "        prefix = f\"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\n",
    "User: Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>.\n",
    "Assistant: Let me solve this step by step.\n",
    "<think>\"\"\"\n",
    "\n",
    "    # Qwen Instructãƒ¢ãƒ‡ãƒ«ã®å ´åˆ\n",
    "    elif template_type == 'qwen-instruct':\n",
    "\n",
    "        # <|im_start|>system\n",
    "        # ã‚ãªãŸã¯æœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ã¾ãšé ­ã®ä¸­ã§æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã‚’è€ƒãˆã€ãã®å¾Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ç­”ãˆã‚’ä¼ãˆã¾ã™ã€‚\n",
    "        # <|im_end|>\n",
    "        # <|im_start|>user\n",
    "        # {numbers} ã‚’ä½¿ã£ã¦ {target} ã«ãªã‚‹æ•°å¼ã‚’ä½œã£ã¦ãã ã•ã„ã€‚\n",
    "        # å››å‰‡æ¼”ç®—ï¼ˆ+, -, *, /ï¼‰ãŒä½¿ãˆã€å„æ•°å­—ã¯ä¸€åº¦ã ã‘ä½¿ãˆã¾ã™ã€‚\n",
    "        # æ€è€ƒéç¨‹ã¯ <think> </think> ã‚¿ã‚°ã§ç¤ºã—ã€æœ€çµ‚çš„ãªç­”ãˆã¯\n",
    "        #  <answer> </answer> ã‚¿ã‚°ã§è¿”ã—ã¦ãã ã•ã„ã€‚\n",
    "        # ä¾‹: <answer> (1 + 2) / 3 </answer>ã€‚\n",
    "        # <|im_end|>\n",
    "        # <|im_start|>assistant\n",
    "        # é †ã‚’è¿½ã£ã¦è§£ã„ã¦ã„ãã¾ã™ã€‚\n",
    "        # <think>\n",
    "        prefix = f\"\"\"<|im_start|>system\\nYou are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.<|im_end|>\\n<|im_start|>user\\n Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>.<|im_end|>\\n<|im_start|>assistant\\nLet me solve this step by step.\\n<think>\"\"\"\n",
    "    return prefix\n",
    "\n",
    "make_prefix(train_dataset[0], 'base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_map_fn(split):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‰å‡¦ç†ã«ä½¿ç”¨ã™ã‚‹ãƒãƒƒãƒ—é–¢æ•°ã‚’ä½œæˆã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        split (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å‰²åï¼ˆä¾‹: 'train', 'test'ï¼‰\n",
    "    Returns:\n",
    "        function: ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’å‡¦ç†ã™ã‚‹ãƒãƒƒãƒ—é–¢æ•°\n",
    "    \"\"\"\n",
    "\n",
    "    def process_fn(example, idx):\n",
    "        \"\"\"\n",
    "        ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’å‰å‡¦ç†ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            example (dict): ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ\n",
    "            idx (int): ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "        Returns:\n",
    "            dict: å‰å‡¦ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ\n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ\n",
    "        question = make_prefix(example, template_type=args.template_type)\n",
    "\n",
    "        solution = {\n",
    "            \"target\": example['target'],\n",
    "            \"numbers\": example['nums']\n",
    "        }\n",
    "\n",
    "        # å‰å‡¦ç†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸å½¢å¼ã§è¿”ã™\n",
    "        data = {\n",
    "            \"data_source\": data_source,\n",
    "            \"prompt\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            }],\n",
    "            \"ability\": \"math\",\n",
    "            \"reward_model\": {\n",
    "                \"style\": \"rule\",\n",
    "                \"ground_truth\": solution\n",
    "            },\n",
    "            \"extra_info\": {\n",
    "                'split': split,\n",
    "                'index': idx,\n",
    "            }\n",
    "        }\n",
    "        return data\n",
    "    return process_fn\n",
    "\n",
    "# æ¤œè¨¼\n",
    "make_map_fn(\"train\")(train_dataset[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb883d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‰å‡¦ç†ã¨Parquetå½¢å¼ã§ã®ä¿å­˜\n",
    "\n",
    "local_dir = args.local_dir\n",
    "OVERWRITE = False\n",
    "\n",
    "if not os.path.exists(os.path.join(local_dir, \"train.parquet\")) or OVERWRITE:\n",
    "    logger.info(\"train.parquetã‚’ä½œæˆ\")\n",
    "    train_dataset = train_dataset.map(function=make_map_fn('train'), with_indices=True)\n",
    "    train_dataset.to_parquet(os.path.join(local_dir, 'train.parquet'))\n",
    "else:\n",
    "    logger.info(f\"æ—¢ã«train.parquetãŒå­˜åœ¨ã™ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {os.path.join(local_dir, 'train.parquet')}\")\n",
    "\n",
    "if not os.path.exists(os.path.join(local_dir, \"test.parquet\")) or OVERWRITE:\n",
    "    logger.info(\"test.parquetã‚’ä½œæˆ\")\n",
    "    test_dataset = test_dataset.map(function=make_map_fn('test'), with_indices=True)\n",
    "    test_dataset.to_parquet(os.path.join(local_dir, 'test.parquet'))\n",
    "else:\n",
    "    logger.info(f\"æ—¢ã«test.parquetãŒå­˜åœ¨ã™ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {os.path.join(local_dir, 'test.parquet')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06016cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hadoop File System (HDFS) ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼\n",
    "# åˆ†æ•£å‡¦ç†ç”¨\n",
    "\n",
    "hdfs_dir = args.hdfs_dir\n",
    "\n",
    "if hdfs_dir is not None:\n",
    "    makedirs(hdfs_dir)\n",
    "    copy(src=local_dir, dst=hdfs_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c09d6d3",
   "metadata": {},
   "source": [
    "## HDFSã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f28d1",
   "metadata": {},
   "source": [
    "TinyZeroã§ã¯ã€HDFSï¼ˆHadoop Distributed File Systemï¼‰ã‚’ä½¿ç”¨\n",
    "\n",
    "ãƒªãƒ¢ãƒ¼ãƒˆã®HDFSã«ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã‚’ã€å„ãƒ—ãƒ­ã‚»ã‚¹ãŒãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ©Ÿèƒ½ã‚’æä¾›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFSã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒ—ãƒªãƒ•ã‚£ãƒƒã‚¯ã‚¹\n",
    "_HDFS_PREFIX = \"hdfs://\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67eca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_HDFS_BIN_PATH = shutil.which('hdfs')\n",
    "logger.info(f\"HDFSãƒã‚¤ãƒŠãƒªã®ãƒ‘ã‚¹: {_HDFS_BIN_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cdab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_non_local(path):\n",
    "    return path.startswith(_HDFS_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88074a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md5_encode(path: str) -> str:\n",
    "    return hashlib.md5(path.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d06dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_temp_path(hdfs_path: str, cache_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    HDFSãƒ‘ã‚¹ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ã®ä¸€æ™‚ãƒ‘ã‚¹ã‚’å–å¾—ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        hdfs_path (str): HDFSä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹\n",
    "        cache_dir (str): ãƒ­ãƒ¼ã‚«ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "    Returns:\n",
    "        str: ãƒ­ãƒ¼ã‚«ãƒ«ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹\n",
    "    \"\"\"\n",
    "    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç«¶åˆã‚’é¿ã‘ã‚‹ãŸã‚ã«hdfs_pathã®base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆ\n",
    "    encoded_hdfs_path = md5_encode(hdfs_path)\n",
    "\n",
    "    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸHDFSãƒ‘ã‚¹ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "    temp_dir = os.path.join(cache_dir, encoded_hdfs_path)\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    # HDFSãƒ‘ã‚¹ã®ãƒ™ãƒ¼ã‚¹åã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã®ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«çµåˆ\n",
    "    dst = os.path.join(temp_dir, os.path.basename(hdfs_path))\n",
    "\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89282bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hdfs_cmd(cmd: str) -> str:\n",
    "    \"\"\"\n",
    "    HDFSã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        cmd (str): HDFSã®ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ï¼ˆä¾‹: -cp -f src dstï¼‰\n",
    "    Returns:\n",
    "        str: å®Œå…¨ãªHDFSã‚³ãƒãƒ³ãƒ‰ï¼ˆä¾‹: hdfs dfs -cp -f src dstï¼‰\n",
    "    \"\"\"\n",
    "    return f\"{_HDFS_BIN_PATH} dfs {cmd}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f9451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_cmd(cmd: str, timeout=None):\n",
    "    \"\"\"\n",
    "    ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹\n",
    "    ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¯æœªå®Ÿè£…...\n",
    "\n",
    "    Args:\n",
    "        cmd (str): å®Ÿè¡Œã™ã‚‹ã‚³ãƒãƒ³ãƒ‰\n",
    "        timeout (int, optional): ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰\n",
    "    Returns:\n",
    "        int: ã‚³ãƒãƒ³ãƒ‰ã®çµ‚äº†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹\n",
    "    \"\"\"\n",
    "    return os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _copy(from_path: str, to_path: str, timeout: int = None) -> bool:\n",
    "    \"\"\"\n",
    "    HDFSã¨ãƒ­ãƒ¼ã‚«ãƒ«é–“ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹å†…éƒ¨é–¢æ•°\n",
    "\n",
    "    Args:\n",
    "        from_path (str): ã‚³ãƒ”ãƒ¼å…ƒã®ãƒ‘ã‚¹ï¼ˆHDFSã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰\n",
    "        to_path (str): ã‚³ãƒ”ãƒ¼å…ˆã®ãƒ‘ã‚¹ï¼ˆHDFSã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰\n",
    "        timeout (int, optional): ã‚³ãƒ”ãƒ¼æ“ä½œã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã€‚\n",
    "    Returns:\n",
    "        bool: ã‚³ãƒ”ãƒ¼ãŒæˆåŠŸã—ãŸå ´åˆã¯Trueã€å¤±æ•—ã—ãŸå ´åˆã¯False\n",
    "    \"\"\"\n",
    "\n",
    "    # ã‚³ãƒ”ãƒ¼å…ˆãŒHDFSã®å ´åˆ\n",
    "    if to_path.startswith(\"hdfs\"):\n",
    "\n",
    "        # ã‚³ãƒ”ãƒ¼å…ƒãŒHDFSã®å ´åˆ\n",
    "        if from_path.startswith(\"hdfs\"):\n",
    "\n",
    "            # HDFSé–“ã®ã‚³ãƒ”ãƒ¼ã‚’å®Ÿè¡Œ\n",
    "            returncode = _run_cmd(\n",
    "                _hdfs_cmd(f\"-cp -f {from_path} {to_path}\"), timeout=timeout\n",
    "            )\n",
    "\n",
    "        # ã‚³ãƒ”ãƒ¼å…ƒãŒãƒ­ãƒ¼ã‚«ãƒ«ã®å ´åˆ\n",
    "        else:\n",
    "\n",
    "            # ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰HDFSã¸ã®ã‚³ãƒ”ãƒ¼ã‚’å®Ÿè¡Œ\n",
    "            returncode = _run_cmd(\n",
    "                _hdfs_cmd(f\"-put -f {from_path} {to_path}\"), timeout=timeout\n",
    "            )\n",
    "\n",
    "    # ã‚³ãƒ”ãƒ¼å…ˆãŒãƒ­ãƒ¼ã‚«ãƒ«ã®å ´åˆ\n",
    "    else:\n",
    "\n",
    "        # ã‚³ãƒ”ãƒ¼å…ƒãŒHDFSã®å ´åˆ\n",
    "        if from_path.startswith(\"hdfs\"):\n",
    "\n",
    "            # HDFSã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ã¸ã®ã‚³ãƒ”ãƒ¼ã‚’å®Ÿè¡Œ\n",
    "            returncode = _run_cmd(\n",
    "                _hdfs_cmd(f\"-get {from_path} {to_path}\"), timeout=timeout\n",
    "            )\n",
    "\n",
    "        # ã‚³ãƒ”ãƒ¼å…ƒãŒãƒ­ãƒ¼ã‚«ãƒ«ã®å ´åˆ\n",
    "        else:\n",
    "            try:\n",
    "                # ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ã¸ã®ã‚³ãƒ”ãƒ¼ã‚’å®Ÿè¡Œ\n",
    "                shutil.copy(from_path, to_path)\n",
    "                returncode = 0\n",
    "            except shutil.SameFileError:\n",
    "                returncode = 0\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"copy {from_path} {to_path} failed: {e}\")\n",
    "                returncode = -1\n",
    "\n",
    "    return returncode == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy(src: str, dst: str, **kwargs) -> bool:\n",
    "    r\"\"\"\n",
    "    HDFSã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‚³ãƒ”ãƒ¼é–¢æ•°\n",
    "    shutil.copy()ã‚„shutil.copytree()ã®ã‚ˆã†ã«å‹•ä½œã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        src (str): ã‚³ãƒ”ãƒ¼å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹\n",
    "        dst (str): ã‚³ãƒ”ãƒ¼å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹\n",
    "        kwargs: hdfsã‚³ãƒ”ãƒ¼ã®ãŸã‚ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°\n",
    "    Returns:\n",
    "        str: ã‚³ãƒ”ãƒ¼å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹\n",
    "    \"\"\"\n",
    "    logger.info(f\"ã‚³ãƒ”ãƒ¼é–‹å§‹: {src} -> {dst}\")\n",
    "\n",
    "    # srcã‹dstãŒHDFSãƒ‘ã‚¹ã®å ´åˆ\n",
    "    if _is_non_local(src) or _is_non_local(dst):\n",
    "        return _copy(src, dst)\n",
    "\n",
    "    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®å ´åˆ\n",
    "    else:\n",
    "        if os.path.isdir(src):\n",
    "            return shutil.copytree(src, dst, **kwargs)\n",
    "        else:\n",
    "            return shutil.copy(src, dst, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf74c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_local_path_from_hdfs(src: str, cache_dir=None, filelock='.file.lock', verbose=False) -> str:\n",
    "    \"\"\"\n",
    "    HDFSï¼ˆHadoop Distributed File Systemï¼‰ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’\n",
    "    ãƒ­ãƒ¼ã‚«ãƒ«ãƒã‚·ãƒ³ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼ã—ã€ãã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’è¿”ã™\n",
    "\n",
    "    HDFSã«å¯¾å¿œã—ã¦ã„ãªã„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã‚‚ã€ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã‚’é€éçš„ã«åˆ©ç”¨å¯èƒ½ã«ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        src (str): HDFSä¸Šã®ã‚½ãƒ¼ã‚¹ãƒ‘ã‚¹\n",
    "        cache_dir (str, optional):\n",
    "            ãƒ­ãƒ¼ã‚«ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "            æŒ‡å®šã—ãªã„å ´åˆã¯ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨\n",
    "        filelock (str, optional):\n",
    "            ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ã®åå‰\n",
    "            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯'.file.lock'\n",
    "        verbose (bool, optional):\n",
    "            ã‚³ãƒ”ãƒ¼æ™‚ã«è©³ç´°æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ã‹ã©ã†ã‹\n",
    "            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯False\n",
    "    \"\"\"\n",
    "    logger.info(f\"HDFSã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ã¸ã®ã‚³ãƒ”ãƒ¼é–‹å§‹: {src}\")\n",
    "\n",
    "    # 1) å…¥åŠ›å€¤ã®æ¤œè¨¼\n",
    "\n",
    "    # srcã®æœ€å¾Œã®æ–‡å­—ãŒ'/'ã§ãªã„ã“ã¨ã‚’ç¢ºèª\n",
    "    assert src[-1] != '/', f'Make sure the last char in src is not / because it will cause error. Got {src}'\n",
    "\n",
    "    # 2) HDFSã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ã¸ã®ã‚³ãƒ”ãƒ¼å‡¦ç†\n",
    "\n",
    "    # ã‚½ãƒ¼ã‚¹ãŒhdfs://ã§å§‹ã¾ã‚‹å ´åˆ\n",
    "    if _is_non_local(src):\n",
    "        logger.debug(f\"HDFSã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚³ãƒ”ãƒ¼: {src}\")\n",
    "\n",
    "        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "        if cache_dir is None:\n",
    "            cache_dir = tempfile.gettempdir()\n",
    "\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "        assert os.path.exists(cache_dir)\n",
    "\n",
    "        local_path = get_local_temp_path(src, cache_dir)\n",
    "\n",
    "        # md5ãƒãƒƒã‚·ãƒ¥ã‚’ä½¿ã£ã¦ä¸€æ„ã®ãƒ­ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ\n",
    "        filelock = md5_encode(src) + '.lock'\n",
    "        lock_file = os.path.join(cache_dir, filelock)\n",
    "\n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦åŒæ™‚ã‚³ãƒ”ãƒ¼ã‚’é˜²æ­¢\n",
    "        with FileLock(lock_file=lock_file):\n",
    "            if not os.path.exists(local_path):\n",
    "                if verbose:\n",
    "                    logger.debug(f\"ã‚³ãƒ”ãƒ¼ä¸­: {src} -> {local_path}\")\n",
    "\n",
    "                # HDFSã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚³ãƒ”ãƒ¼\n",
    "                copy(src, local_path)\n",
    "\n",
    "        return local_path\n",
    "    else:\n",
    "        logger.debug(f\"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’ãã®ã¾ã¾ä½¿ç”¨: {src}\")\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af587c0d",
   "metadata": {},
   "source": [
    "### FSDPãƒ¯ãƒ¼ã‚«ãƒ¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ceada",
   "metadata": {},
   "source": [
    "#### ActorRolloutRefWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0ba653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorRolloutRefWorker(Worker):\n",
    "    \"\"\"\n",
    "    TinyZeroï¼ˆåŸºç›¤ã®VeRLï¼‰ã®åˆ†æ•£å¼·åŒ–å­¦ç¿’ã«ãŠã„ã¦\n",
    "    å­¦ç¿’ï¼ˆActorï¼‰ã€ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆï¼ˆRolloutï¼‰ã€å‚ç…§ãƒ¢ãƒ‡ãƒ«ï¼ˆRef Modelï¼‰ã®3ã¤ã®å½¹å‰²ã‚’\n",
    "    1ã¤ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã§æ‹…ã†ã€ã‚‚ã—ãã¯å€‹åˆ¥ã«æ‰±ã†ãŸã‚ã®ã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: DictConfig, role: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config (DictConfig): ãƒ¯ãƒ¼ã‚«ãƒ¼ã®è¨­å®š\n",
    "            role (str): ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å½¹å‰²\n",
    "                - 'actor': å­¦ç¿’ã®ã¿\n",
    "                - 'rollout': ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã®ã¿\n",
    "                - 'ref': å‚ç…§ãƒ¢ãƒ‡ãƒ«ã®ã¿\n",
    "                - 'actor_rollout': å­¦ç¿’ã¨ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã®ä¸¡æ–¹\n",
    "                - 'actor_rollout_ref': å­¦ç¿’ã€ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã€å‚ç…§ãƒ¢ãƒ‡ãƒ«ã®å…¨ã¦\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"ActorRolloutRefWorkerã‚’åˆæœŸåŒ– {config=} {role=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # 1) åˆ†æ•£ç’°å¢ƒãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã€NCCLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§åˆæœŸåŒ–\n",
    "\n",
    "        if not torch.distributed.is_initialized():\n",
    "            torch.distributed.init_process_group(backend=\"nccl\")\n",
    "\n",
    "        # 2) FSDPï¼ˆFully Sharded Data Parallelï¼‰ç”¨ã®ãƒ‡ãƒã‚¤ã‚¹ãƒ¡ãƒƒã‚·ãƒ¥ã‚’æ§‹ç¯‰\n",
    "\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "\n",
    "        self.device_mesh = init_device_mesh(\n",
    "            'cuda',\n",
    "            mesh_shape=(world_size,),\n",
    "            mesh_dim_names=['fsdp']\n",
    "        )\n",
    "\n",
    "        # 3) Ulysses Sequence Parallelç”¨ã®ãƒ‡ãƒã‚¤ã‚¹ãƒ¡ãƒƒã‚·ãƒ¥ã‚’æ§‹ç¯‰\n",
    "\n",
    "        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—åŒ–ã¨ã¯ã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’è¤‡æ•°ã®ãƒ‡ãƒã‚¤ã‚¹ã«åˆ†å‰²ã—ã¦å‡¦ç†ã™ã‚‹æ‰‹æ³•\n",
    "\n",
    "        self.ulysses_device_mesh = None\n",
    "\n",
    "        self.ulysses_sequence_parallel_size = \\\n",
    "            self.config.actor.get('ulysses_sequence_parallel_size', 1)\n",
    "\n",
    "        dp = world_size // self.ulysses_sequence_parallel_size\n",
    "\n",
    "        if self.ulysses_sequence_parallel_size > 1:\n",
    "            self.ulysses_device_mesh = init_device_mesh(\n",
    "                'cuda',\n",
    "                mesh_shape=(dp, self.ulysses_sequence_parallel_size),\n",
    "                mesh_dim_names=['dp', 'sp']\n",
    "            )\n",
    "\n",
    "        self.ulysses_sharding_manager = FSDPUlyssesShardingManager(\n",
    "            self.ulysses_device_mesh\n",
    "        )\n",
    "\n",
    "        # 4) ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å½¹å‰²ã‚’è¨­å®š\n",
    "\n",
    "        self.role = role\n",
    "\n",
    "        assert self.role in ['actor', 'rollout', 'ref', 'actor_rollout', 'actor_rollout_ref']\n",
    "\n",
    "        self._is_actor = self.role in ['actor', 'actor_rollout', 'actor_rollout_ref']\n",
    "\n",
    "        self._is_rollout = self.role in ['rollout', 'actor_rollout', 'actor_rollout_ref']\n",
    "\n",
    "        self._is_ref = self.role in ['ref', 'actor_rollout_ref']\n",
    "\n",
    "        self._is_offload_param = False\n",
    "        self._is_offload_grad = False\n",
    "        self._is_offload_optimizer = False\n",
    "\n",
    "        if self._is_actor:\n",
    "            self._is_offload_param = self.config.actor.fsdp_config.get('param_offload', False)\n",
    "            self._is_offload_grad = self.config.actor.fsdp_config.get('grad_offload', False)\n",
    "            self._is_offload_optimizer = self.config.actor.fsdp_config.get('optimizer_offload', False)\n",
    "\n",
    "        elif self._is_ref:\n",
    "            # TODO: it seems that manual offload is slowly than FSDP offload\n",
    "            self._is_offload_param = self.config.ref.fsdp_config.get('param_offload', False)\n",
    "\n",
    "        # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ\n",
    "        if self._is_actor:\n",
    "\n",
    "            self.config.actor.ppo_mini_batch_size //= \\\n",
    "                (self.device_mesh.shape[0] // self.ulysses_sequence_parallel_size)\n",
    "\n",
    "            self.config.actor.ppo_micro_batch_size //= \\\n",
    "                (self.device_mesh.shape[0] // self.ulysses_sequence_parallel_size)\n",
    "\n",
    "            self.config.actor.ppo_mini_batch_size *= self.config.rollout.n\n",
    "\n",
    "            self.config.actor.ppo_micro_batch_size *= self.config.rollout.n\n",
    "\n",
    "        # ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãƒ¢ãƒ‡ãƒ«ã®å ´åˆ\n",
    "        if self._is_rollout:\n",
    "\n",
    "            self.config.rollout.log_prob_micro_batch_size //= \\\n",
    "                (self.device_mesh.shape[0] // self.ulysses_sequence_parallel_size)\n",
    "\n",
    "            self.config.rollout.log_prob_micro_batch_size *= self.config.rollout.n\n",
    "\n",
    "        # å‚ç…§ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ\n",
    "        if self._is_ref:\n",
    "\n",
    "            self.config.ref.log_prob_micro_batch_size //= \\\n",
    "                (self.device_mesh.shape[0] // self.ulysses_sequence_parallel_size)\n",
    "\n",
    "            self.config.ref.log_prob_micro_batch_size *= self.config.rollout.n\n",
    "\n",
    "    def _build_model_optimizer(self, model_path, fsdp_config, optim_config, override_model_config, use_remove_padding=False, enable_gradient_checkpointing=False, trust_remote_code=False):\n",
    "        \"\"\"\n",
    "        ãƒ¢ãƒ‡ãƒ«ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "        init_modelãƒ¡ã‚½ãƒƒãƒ‰ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "        Args:\n",
    "            model_path (str): äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹\n",
    "            fsdp_config (DictConfig): FSDPã®è¨­å®š\n",
    "            optim_config (DictConfig): ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®è¨­å®š\n",
    "            override_model_config (Dict): ãƒ¢ãƒ‡ãƒ«è¨­å®šã®ä¸Šæ›¸ãç”¨è¾æ›¸\n",
    "            use_remove_padding (bool, optional): ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            enable_gradient_checkpointing (bool, optional):\n",
    "                å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            trust_remote_code (bool, optional): ãƒªãƒ¢ãƒ¼ãƒˆã‚³ãƒ¼ãƒ‰ã‚’ä¿¡é ¼ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        Returns:\n",
    "            actor_module_fsdp (FSDP): FSDPã§ãƒ©ãƒƒãƒ—ã•ã‚ŒãŸã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«\n",
    "            actor_optimizer (Optimizer): ã‚¢ã‚¯ã‚¿ãƒ¼ç”¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶\n",
    "            actor_lr_scheduler (LRScheduler): ã‚¢ã‚¯ã‚¿ãƒ¼ç”¨å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©\n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "        log_gpu_memory_usage('Before init from HF AutoModel', logger=logger)\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚³ãƒ”ãƒ¼\n",
    "        local_path = copy_local_path_from_hdfs(model_path)\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’åˆæœŸåŒ–\n",
    "        self.tokenizer = hf_tokenizer(local_path, trust_remote_code=trust_remote_code)\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’è¨­å®š\n",
    "        torch_dtype = fsdp_config.get('model_dtype', None)\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å‹ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ\n",
    "        if torch_dtype is None:\n",
    "            # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯float32ã€ãã†ã§ãªã‘ã‚Œã°bfloat16ã‚’ä½¿ç”¨\n",
    "            torch_dtype = torch.float32 if self._is_actor else torch.bfloat16\n",
    "\n",
    "        # æŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "        else:\n",
    "            # ãã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’ä½¿ç”¨\n",
    "            torch_dtype = PrecisionType.to_dtype(torch_dtype)\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€\n",
    "        actor_model_config = AutoConfig.from_pretrained(\n",
    "            local_path, trust_remote_code=trust_remote_code\n",
    "        )\n",
    "\n",
    "        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "        if use_remove_padding:\n",
    "\n",
    "            # ãƒ¢ãƒ‡ãƒ«ãŒãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "            check_model_support_rmpad(actor_model_config.model_type)\n",
    "\n",
    "        # Ulysses Sequence ParallelãŒæœ‰åŠ¹ãªå ´åˆ\n",
    "        if use_remove_padding and self.ulysses_sequence_parallel_size > 1:\n",
    "\n",
    "            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã®ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ‘ãƒƒãƒã‚’é©ç”¨\n",
    "            apply_monkey_patch(actor_model_config, verbose=True)\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«è¨­å®šã®ä¸Šæ›¸ãç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°ã‚’æº–å‚™\n",
    "        override_config_kwargs = {\n",
    "            'bos_token_id': self.tokenizer.bos_token_id,\n",
    "            'eos_token_id': self.tokenizer.eos_token_id,\n",
    "            'pad_token_id': self.tokenizer.pad_token_id,\n",
    "        }\n",
    "\n",
    "        # ãƒ¡ã‚½ãƒƒãƒ‰ã®å¼•æ•°ã§ã€è¿½åŠ ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šã®ä¸Šæ›¸ããŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä¸Šæ›¸ã\n",
    "        override_config_kwargs.update(override_model_config)\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ä¸Šæ›¸ã\n",
    "        update_model_config(\n",
    "            actor_model_config,\n",
    "            override_config_kwargs=override_config_kwargs\n",
    "        )\n",
    "\n",
    "        # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’è¡¨ç¤º\n",
    "        if self.rank == 0:\n",
    "            logger.info(f'Model config after override: {actor_model_config}')\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—\n",
    "        init_context = get_init_weight_context_manager(\n",
    "            use_meta_tensor=not actor_model_config.tie_word_embeddings\n",
    "        )\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–\n",
    "        with init_context(), warnings.catch_warnings():\n",
    "\n",
    "            # Transformersã®è­¦å‘Šã‚’ç„¡è¦–\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€\n",
    "            actor_module = AutoModelForCausalLM.from_pretrained(\n",
    "                pretrained_model_name_or_path=local_path,\n",
    "                torch_dtype=torch_dtype,\n",
    "                config=actor_model_config,\n",
    "                attn_implementation='flash_attention_2',\n",
    "                trust_remote_code=trust_remote_code\n",
    "            )\n",
    "\n",
    "            # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å‹ã«å¤‰æ›\n",
    "            actor_module.to(torch_dtype)\n",
    "\n",
    "            # å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æœ‰åŠ¹ã«ã™ã‚‹å ´åˆ\n",
    "            if enable_gradient_checkpointing:\n",
    "\n",
    "                # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã§å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æœ‰åŠ¹åŒ–\n",
    "                actor_module.gradient_checkpointing_enable(\n",
    "                    gradient_checkpointing_kwargs={'use_reentrant': False}\n",
    "                )\n",
    "\n",
    "        # å…¨ãƒ—ãƒ­ã‚»ã‚¹ã§åŒæœŸ\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "        # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã§ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’è¡¨ç¤º\n",
    "        if self.rank == 0:\n",
    "            print_model_size(actor_module)\n",
    "\n",
    "        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "        log_gpu_memory_usage('After init from HF AutoModel', logger=logger)\n",
    "\n",
    "        # FSDPã®è¨­å®šã‹ã‚‰æ··åˆç²¾åº¦ã®è¨­å®šã‚’å–å¾—\n",
    "        mixed_precision_config = fsdp_config.get('mixed_precision', None)\n",
    "\n",
    "        # æ··åˆç²¾åº¦ã®è¨­å®šãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "        if mixed_precision_config is not None:\n",
    "\n",
    "            # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’å–å¾—ã€ãªã‘ã‚Œã°bfloat16ã‚’ä½¿ç”¨\n",
    "            param_dtype = PrecisionType.to_dtype(\n",
    "                mixed_precision_config.get('param_dtype', 'bf16')\n",
    "            )\n",
    "\n",
    "            # å‹¾é…ã®é›†ç´„æ“ä½œï¼ˆreduceï¼‰ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’å–å¾—ã€ãªã‘ã‚Œã°float32ã‚’ä½¿ç”¨\n",
    "            reduce_dtype = PrecisionType.to_dtype(\n",
    "                mixed_precision_config.get('reduce_dtype', 'fp32')\n",
    "            )\n",
    "\n",
    "            # ãƒãƒƒãƒ•ã‚¡ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’å–å¾—ã€ãªã‘ã‚Œã°float32ã‚’ä½¿ç”¨\n",
    "            buffer_dtype = PrecisionType.to_dtype(\n",
    "                mixed_precision_config.get('buffer_dtype', 'fp32')\n",
    "            )\n",
    "\n",
    "        # æ··åˆç²¾åº¦ã®è¨­å®šãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ\n",
    "        else:\n",
    "            param_dtype = torch.bfloat16\n",
    "            reduce_dtype = torch.float32\n",
    "            buffer_dtype = torch.float32\n",
    "\n",
    "        # FSDPã®æ··åˆç²¾åº¦è¨­å®šã‚’ä½œæˆ\n",
    "        mixed_precision = MixedPrecision(\n",
    "            param_dtype=param_dtype,\n",
    "            reduce_dtype=reduce_dtype,\n",
    "            buffer_dtype=buffer_dtype\n",
    "        )\n",
    "\n",
    "        # å‚ç…§ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€å‹¾é…è¨ˆç®—ã‚’ã—ãªã„ãŸã‚ã€æ··åˆç²¾åº¦ã‚’ç„¡åŠ¹åŒ–\n",
    "        if self._is_ref:\n",
    "            mixed_precision = None\n",
    "\n",
    "        # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®FSDPãƒ©ãƒƒãƒ—ç”¨ã®è‡ªå‹•ãƒ©ãƒƒãƒ—ãƒãƒªã‚·ãƒ¼ã‚’å–å¾—\n",
    "        auto_wrap_policy = get_fsdp_wrap_policy(\n",
    "            module=actor_module,\n",
    "            config=fsdp_config.get('wrap_policy', None)\n",
    "        )\n",
    "\n",
    "        # HFRolloutã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "        if self._is_rollout and self.config.rollout.name == 'hf':\n",
    "            # Gemmaç’°å¢ƒã§ãƒãƒ³ã‚°ã™ã‚‹å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã€è‡ªå‹•ãƒ©ãƒƒãƒ—ãƒãƒªã‚·ãƒ¼ã‚’ç„¡åŠ¹åŒ–\n",
    "            auto_wrap_policy = None\n",
    "\n",
    "        logger.debug(f'wrap_policy: {auto_wrap_policy}')\n",
    "\n",
    "        # TODO(sgm): support hybrid\n",
    "        # è‡ªå‹•ãƒ©ãƒƒãƒ—ãƒãƒªã‚·ãƒ¼ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ\n",
    "        if auto_wrap_policy is None:\n",
    "\n",
    "            # FSDPã®ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥ã‚’SHARD_GRAD_OPã«è¨­å®š\n",
    "            sharding_strategy = ShardingStrategy.SHARD_GRAD_OP\n",
    "        else:\n",
    "            # FSDPã®ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥ã‚’FULL_SHARDã«è¨­å®š\n",
    "            sharding_strategy = ShardingStrategy.FULL_SHARD\n",
    "\n",
    "        # FSDPã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–é–¢æ•°ã‚’å®šç¾©\n",
    "        actor_module_fsdp = FSDP(\n",
    "            actor_module,\n",
    "            param_init_fn=init_fn,\n",
    "            use_orig_params=False,\n",
    "            auto_wrap_policy=auto_wrap_policy,\n",
    "            device_id=torch.cuda.current_device(),\n",
    "            sharding_strategy=sharding_strategy,  # zero3\n",
    "            mixed_precision=mixed_precision,\n",
    "            sync_module_states=True,\n",
    "            device_mesh=self.device_mesh,\n",
    "            forward_prefetch=False\n",
    "        )\n",
    "\n",
    "        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "        log_gpu_memory_usage('After Actor FSDP init', logger=logger)\n",
    "\n",
    "\n",
    "        # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ\n",
    "        if self._is_actor:\n",
    "\n",
    "            # AdamWã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’åˆæœŸåŒ–\n",
    "            actor_optimizer = optim.AdamW(\n",
    "                actor_module_fsdp.parameters(),\n",
    "                lr=optim_config.lr,\n",
    "                betas=optim_config.get('betas', (0.9, 0.999)),\n",
    "                weight_decay=optim_config.get('weight_decay', 1e-2)\n",
    "            )\n",
    "\n",
    "            # æœ€é©åŒ–é–¢æ•°ã®è¨­å®šã‹ã‚‰ç·ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’å–å¾—\n",
    "            total_steps = optim_config.get('total_training_steps', 0)\n",
    "            logger.info(f\"å…¨ã¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—æ•°: {total_steps}\")\n",
    "\n",
    "            # æœ€é©åŒ–é–¢æ•°ã®è¨­å®šã‹ã‚‰ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®æ¯”ç‡ã‚’å–å¾—\n",
    "            num_warmup_steps_ratio = optim_config.get('lr_warmup_steps_ratio', 0.)\n",
    "            logger.info(f\"ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®æ¯”ç‡: {num_warmup_steps_ratio}\")\n",
    "\n",
    "            # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è¨ˆç®—\n",
    "            num_warmup_steps = int(num_warmup_steps_ratio * total_steps)\n",
    "            logger.info(f\"ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—æ•°: {num_warmup_steps}\")\n",
    "\n",
    "            # å®šæ•°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’åˆæœŸåŒ–\n",
    "            actor_lr_scheduler = get_constant_schedule_with_warmup(\n",
    "                optimizer=actor_optimizer,\n",
    "                num_warmup_steps=num_warmup_steps\n",
    "            )\n",
    "\n",
    "        # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ãªã„å ´åˆã€å‹¾é…è¨ˆç®—ã‚’è¡Œã‚ãªã„\n",
    "        else:\n",
    "            actor_optimizer = None\n",
    "            actor_lr_scheduler = None\n",
    "\n",
    "        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "        log_gpu_memory_usage('After actor optimizer init', logger=logger)\n",
    "\n",
    "        # æ§‹ç¯‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã€ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’è¿”ã™\n",
    "        return actor_module_fsdp, actor_optimizer, actor_lr_scheduler, actor_model_config\n",
    "\n",
    "    def _build_rollout(self):\n",
    "        \"\"\"\n",
    "        ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "        ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ã¯FSDPã§åˆ†æ•£ã•ã‚ŒãŸé‡ã¿ã‚’vLLMã«å¯¾å¿œã•ã›ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹\n",
    "        init_modelãƒ¡ã‚½ãƒƒãƒ‰ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "        Returns:\n",
    "            rollout (Rollout): ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "            rollout_sharding_manager (BaseShardingManager):\n",
    "                ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆç”¨ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£\n",
    "        \"\"\"\n",
    "        # ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆç”¨ã®ãƒ†ãƒ³ã‚½ãƒ«ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—ã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "        infer_tp = self.config.rollout.tensor_model_parallel_size\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã‚µã‚¤ã‚ºã‚’è¨ˆç®—\n",
    "        dp = self.world_size // infer_tp\n",
    "\n",
    "        # ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆç”¨ã®ãƒ†ãƒ³ã‚½ãƒ«ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—ã‚µã‚¤ã‚ºãŒãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºã§å‰²ã‚Šåˆ‡ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª\n",
    "        assert self.world_size % infer_tp == 0, f'rollout world_size: {self.world_size} is not divisible by infer_tp: {infer_tp}'\n",
    "\n",
    "        # ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆç”¨ã®ãƒ‡ãƒã‚¤ã‚¹ãƒ¡ãƒƒã‚·ãƒ¥ã‚’åˆæœŸåŒ–\n",
    "        rollout_device_mesh = init_device_mesh(\n",
    "            'cuda',\n",
    "            mesh_shape=(dp, infer_tp),\n",
    "            mesh_dim_names=['dp', 'infer_tp']\n",
    "        )\n",
    "\n",
    "        # HFãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã®å ´åˆ\n",
    "        # False\n",
    "        if self.config.rollout.name == 'hf':\n",
    "\n",
    "            # HFãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’åˆæœŸåŒ–\n",
    "            rollout = HFRollout(\n",
    "                module=self.actor_module_fsdp,\n",
    "                config=self.config.rollout\n",
    "            )\n",
    "\n",
    "            # ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã®ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ã‚’åˆæœŸåŒ–\n",
    "            rollout_sharding_manager = BaseShardingManager()\n",
    "\n",
    "        # vLLMãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã®å ´åˆ\n",
    "        # True\n",
    "        elif self.config.rollout.name == 'vllm':\n",
    "\n",
    "            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "            log_gpu_memory_usage('Before building vllm rollout', logger=None)\n",
    "\n",
    "            # vLLMãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’åˆæœŸåŒ–\n",
    "            rollout = vLLMRollout(\n",
    "                actor_module=self.actor_module_fsdp,\n",
    "                config=self.config.rollout,\n",
    "                tokenizer=self.tokenizer,\n",
    "                model_hf_config=self.actor_model_config\n",
    "            )\n",
    "\n",
    "            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "            log_gpu_memory_usage('After building vllm rollout', logger=None)\n",
    "\n",
    "            # ãƒ‡ãƒãƒƒã‚°ç›®çš„ã§ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºãŒ1ã®å ´åˆã€ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’'dummy_hf'ã«è¨­å®š\n",
    "            if torch.distributed.get_world_size() == 1:\n",
    "                self.config.rollout.load_format = 'dummy_hf'\n",
    "\n",
    "            # vLLMç”¨ã®FSDPã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ã‚’åˆæœŸåŒ–\n",
    "            rollout_sharding_manager = FSDPVLLMShardingManager(\n",
    "                module=self.actor_module_fsdp,\n",
    "                inference_engine=rollout.inference_engine,\n",
    "                model_config=self.actor_model_config,\n",
    "                full_params='hf' in self.config.rollout.load_format,\n",
    "                device_mesh=rollout_device_mesh\n",
    "            )\n",
    "\n",
    "            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "            log_gpu_memory_usage('After building sharding manager', logger=None)\n",
    "\n",
    "        # æ§‹ç¯‰ã—ãŸãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ã‚’è¿”ã™\n",
    "        return rollout, rollout_sharding_manager\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.ONE_TO_ALL)\n",
    "    def init_model(self):\n",
    "        \"\"\"\n",
    "        ãƒ¢ãƒ‡ãƒ«ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã€ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "        ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å½¹å‰²ã«å¿œã˜ã¦é©åˆ‡ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "        \"\"\"\n",
    "        logger.info(\"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚’é–‹å§‹\")\n",
    "\n",
    "        # è¨­å®šã§æŒ‡å®šã•ã‚ŒãŸå¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "        # None\n",
    "        import_external_libs(self.config.model.get('external_lib', None))\n",
    "\n",
    "        # ä¸Šæ›¸ãç”¨ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’å–å¾—\n",
    "        # {}\n",
    "        override_model_config = OmegaConf.to_container(\n",
    "            self.config.model.get('override_config', OmegaConf.create())\n",
    "        )\n",
    "\n",
    "        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã‚’å–å¾—\n",
    "        # False\n",
    "        use_remove_padding = self.config.model.get('use_remove_padding', False)\n",
    "\n",
    "        # ã‚¢ã‚¯ã‚¿ãƒ¼ã¾ãŸã¯ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã®å ´åˆ\n",
    "        if self._is_actor or self._is_rollout:\n",
    "\n",
    "            # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ\n",
    "            if self._is_actor:\n",
    "\n",
    "                # æœ€é©åŒ–é–¢æ•°ã®è¨­å®šã‚’å–å¾—\n",
    "                # {'lr': 1e-06, 'lr_warmup_steps_ratio': 0.0, 'min_lr_ratio': None, 'warmup_style': 'constant', 'total_training_steps': -1},\n",
    "                optim_config = self.config.actor.optim\n",
    "\n",
    "                # FSDPã®è¨­å®šã‚’å–å¾—\n",
    "                # {'wrap_policy': {'min_num_params': 0}, 'param_offload': False, 'grad_offload': False, 'optimizer_offload': False, 'fsdp_size': -1}}\n",
    "                fsdp_config = self.config.actor.fsdp_config\n",
    "\n",
    "            # ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã®ã¿ã®å ´åˆ\n",
    "            else:\n",
    "\n",
    "                # æœ€é©åŒ–é–¢æ•°ã®è¨­å®šã¯None\n",
    "                optim_config = None\n",
    "\n",
    "                # FSDPã®è¨­å®šã‚’ç©ºã«ã™ã‚‹\n",
    "                fsdp_config = OmegaConf.create()\n",
    "\n",
    "            # ã‚¢ã‚¯ã‚¿ãƒ¼ã®ãƒ¢ãƒ‡ãƒ«ã€æœ€é©åŒ–é–¢æ•°ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã€è¨­å®šã‚’æ§‹ç¯‰\n",
    "            self.actor_module_fsdp, \\\n",
    "            self.actor_optimizer, \\\n",
    "            self.actor_lr_scheduler, \\\n",
    "            self.actor_model_config = \\\n",
    "                self._build_model_optimizer(\n",
    "                model_path=self.config.model.path,\n",
    "                fsdp_config=fsdp_config,\n",
    "                optim_config=optim_config,\n",
    "                override_model_config=override_model_config,\n",
    "                use_remove_padding=use_remove_padding,\n",
    "                enable_gradient_checkpointing=\n",
    "                    self.config.model.get('enable_gradient_checkpointing', False),\n",
    "                trust_remote_code=self.config.model.get('trust_remote_code', False)\n",
    "            )\n",
    "\n",
    "            # ãƒ©ãƒƒãƒ—ã•ã‚Œã¦ã„ãªã„å…ƒã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å–å¾—\n",
    "            self.actor_module = self.actor_module_fsdp._fsdp_wrapped_module\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ\n",
    "            if self._is_offload_param:\n",
    "\n",
    "                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰\n",
    "                offload_fsdp_grad(module=self.actor_module_fsdp)\n",
    "\n",
    "                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "                log_gpu_memory_usage(\n",
    "                    'After offload actor grad during init', logger=logger\n",
    "                )\n",
    "\n",
    "            # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ\n",
    "            if self._is_offload_optimizer:\n",
    "\n",
    "                # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰\n",
    "                offload_fsdp_optimizer(optimizer=self.actor_optimizer)\n",
    "\n",
    "                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "                log_gpu_memory_usage(\n",
    "                    'After offload actor optimizer during init', logger=logger\n",
    "                )\n",
    "\n",
    "        # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ\n",
    "        if self._is_actor:\n",
    "\n",
    "            # ã‚¢ã‚¯ã‚¿ãƒ¼è¨­å®šã®æ§‹é€ ã‚’å›ºå®š\n",
    "            OmegaConf.set_struct(self.config.actor, True)\n",
    "\n",
    "            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã®ä½¿ç”¨è¨­å®šã‚’ä¿å­˜\n",
    "            with open_dict(self.config.actor):\n",
    "                self.config.actor.use_remove_padding = use_remove_padding\n",
    "\n",
    "            # PPOã‚¢ã‚¯ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–\n",
    "            self.actor = DataParallelPPOActor(\n",
    "                config=self.config.actor,\n",
    "                actor_module=self.actor_module_fsdp,\n",
    "                actor_optimizer=self.actor_optimizer\n",
    "            )\n",
    "\n",
    "        # ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å ´åˆ\n",
    "        if self._is_rollout:\n",
    "\n",
    "            # ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ã‚’æ§‹ç¯‰\n",
    "            self.rollout, self.rollout_sharding_manager = self._build_rollout()\n",
    "\n",
    "        # å‚ç…§ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ\n",
    "        if self._is_ref:\n",
    "\n",
    "            # å‚ç…§ãƒ¢ãƒ‡ãƒ«ã®FSDPãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æ§‹ç¯‰\n",
    "            self.ref_module_fsdp = self._build_model_optimizer(\n",
    "                model_path=self.config.model.path,\n",
    "                fsdp_config=self.config.ref.fsdp_config,\n",
    "                optim_config=None,\n",
    "                override_model_config=override_model_config,\n",
    "                use_remove_padding=use_remove_padding,\n",
    "                trust_remote_code=self.config.model.get(\n",
    "                    'trust_remote_code', False)\n",
    "                )[0]\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ\n",
    "            if self._is_offload_param:\n",
    "\n",
    "                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰\n",
    "                offload_fsdp_param_and_grad(\n",
    "                    module=self.ref_module_fsdp,\n",
    "                    offload_grad=self._is_offload_grad\n",
    "                )\n",
    "\n",
    "            # å‚ç…§ãƒ¢ãƒ‡ãƒ«è¨­å®šã®æ§‹é€ ã‚’å›ºå®š\n",
    "            OmegaConf.set_struct(self.config.ref, True)\n",
    "\n",
    "            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã®ä½¿ç”¨è¨­å®šã‚’ä¿å­˜\n",
    "            with open_dict(self.config.ref):\n",
    "                self.config.ref.use_remove_padding = use_remove_padding\n",
    "\n",
    "            # å‚ç…§ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–\n",
    "            self.ref_policy = DataParallelPPOActor(\n",
    "                config=self.config.ref,\n",
    "                actor_module=self.ref_module_fsdp\n",
    "            )\n",
    "\n",
    "        # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€FLOPSã‚«ã‚¦ãƒ³ã‚¿ã‚’åˆæœŸåŒ–\n",
    "        # FLOPSã‚«ã‚¦ãƒ³ã‚¿ã¯ãƒ¢ãƒ‡ãƒ«ã®è¨ˆç®—é‡ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "        if self._is_actor:\n",
    "            self.flops_counter = FlopsCounter(self.actor_model_config)\n",
    "\n",
    "        # ä¸è¦ãªGPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\n",
    "    def update_actor(self, data: DataProto):\n",
    "        \"\"\"\n",
    "        ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            data (DataProto): ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿\n",
    "        Returns:\n",
    "            output (DataProto): ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒˆã‚³ãƒ«\n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«è»¢é€\n",
    "        data = data.to('cuda')\n",
    "\n",
    "        # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n",
    "        assert self._is_actor\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã‚‹å ´åˆ\n",
    "        if self._is_offload_param:\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "            load_fsdp_param_and_grad(\n",
    "                module=self.actor_module_fsdp,\n",
    "                device_id=torch.cuda.current_device(),\n",
    "                load_grad=self._is_offload_grad\n",
    "            )\n",
    "\n",
    "        # æœ€é©åŒ–é–¢æ•°ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã‚‹å ´åˆ\n",
    "        if self._is_offload_optimizer:\n",
    "\n",
    "            # æœ€é©åŒ–é–¢æ•°ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "            load_fsdp_optimizer(\n",
    "                optimizer=self.actor_optimizer,\n",
    "                device_id=torch.cuda.current_device()\n",
    "            )\n",
    "\n",
    "        # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’GPUã«è»¢é€\n",
    "        data.batch = data.batch.cuda()\n",
    "\n",
    "        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "        log_gpu_memory_usage('Before update policy', logger=logger)\n",
    "\n",
    "        # Ulyssesã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§å‡¦ç†\n",
    "        with self.ulysses_sharding_manager:\n",
    "\n",
    "            # ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†\n",
    "            data = self.ulysses_sharding_manager.preprocess_data(data=data)\n",
    "\n",
    "            # ã‚¿ã‚¤ãƒãƒ¼ã‚’é–‹å§‹ã—ã¦ã€ãƒãƒªã‚·ãƒ¼ã‚’æ›´æ–°\n",
    "            with Timer(name='update_policy', logger=None) as timer:\n",
    "                metrics = self.actor.update_policy(data=data)\n",
    "\n",
    "            # çµŒéæ™‚é–“ã‚’å–å¾—\n",
    "            delta_time = timer.last\n",
    "\n",
    "            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å–å¾—\n",
    "            global_num_tokens = data.meta_info['global_token_num']\n",
    "\n",
    "            # FLOPSã‚’è¨ˆç®—\n",
    "            estimated_flops, promised_flops = \\\n",
    "                self.flops_counter.estimate_flops(global_num_tokens, delta_time)\n",
    "\n",
    "            # MFUï¼ˆModel FLOPS Utilizationï¼‰ã‚’è¨ˆç®—ã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ \n",
    "            metrics['mfu/actor'] = \\\n",
    "                estimated_flops * self.config.actor.ppo_epochs \\\n",
    "                / promised_flops / self.world_size\n",
    "\n",
    "            # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ã‚¹ãƒ†ãƒƒãƒ—\n",
    "            self.actor_lr_scheduler.step()\n",
    "\n",
    "            # ç¾åœ¨ã®å­¦ç¿’ç‡ã‚’å–å¾—\n",
    "            lr = self.actor_lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "            # å­¦ç¿’ç‡ã‚’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ \n",
    "            metrics['actor/lr'] = lr\n",
    "\n",
    "            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "            log_gpu_memory_usage('After update policy', logger=logger)\n",
    "\n",
    "            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ä½œæˆ\n",
    "            output = DataProto(meta_info={'metrics': metrics})\n",
    "\n",
    "            # ãƒ‡ãƒ¼ã‚¿ã‚’å¾Œå‡¦ç†\n",
    "            output = self.ulysses_sharding_manager.postprocess_data(data=output)\n",
    "\n",
    "            # ãƒ‡ãƒ¼ã‚¿ã‚’CPUã«è»¢é€\n",
    "            output = output.to('cpu')\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ\n",
    "        if self._is_offload_param:\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰\n",
    "            offload_fsdp_param_and_grad(\n",
    "                module=self.actor_module_fsdp,\n",
    "                offload_grad=self._is_offload_grad\n",
    "            )\n",
    "\n",
    "        # æœ€é©åŒ–é–¢æ•°ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ\n",
    "        if self._is_offload_optimizer:\n",
    "\n",
    "            # æœ€é©åŒ–é–¢æ•°ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰\n",
    "            offload_fsdp_optimizer(optimizer=self.actor_optimizer)\n",
    "\n",
    "        # ä¸è¦ãªGPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return output\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\n",
    "    def generate_sequences(self, prompts: DataProto):\n",
    "        \"\"\"\n",
    "        ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            prompts (DataProto): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‡ãƒ¼ã‚¿\n",
    "        Returns:\n",
    "            output (DataProto): ç”Ÿæˆã•ã‚ŒãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿\n",
    "        \"\"\"\n",
    "\n",
    "         # ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«è»¢é€\n",
    "        prompts = prompts.to('cuda')\n",
    "\n",
    "        # å¯¾æ•°ç¢ºç‡ã‚’å†è¨ˆç®—ã™ã‚‹ã‹ã©ã†ã‹ã‚’å–å¾—\n",
    "        # True\n",
    "        recompute_log_prob = prompts.meta_info.get('recompute_log_prob', True)\n",
    "\n",
    "        # ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n",
    "        assert self._is_rollout\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã‚‹å ´åˆ\n",
    "        if self._is_offload_param:\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "            load_fsdp_param_and_grad(\n",
    "                module=self.actor_module_fsdp,\n",
    "                device_id=torch.cuda.current_device(),\n",
    "                load_grad=self._is_offload_grad\n",
    "            )\n",
    "\n",
    "        # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’GPUã«è»¢é€\n",
    "        prompts.batch = prompts.batch.cuda()\n",
    "\n",
    "        # EOSãƒˆãƒ¼ã‚¯ãƒ³IDã¨ãƒ‘ãƒƒãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒ¡ã‚¿æƒ…å ±ã«è¿½åŠ \n",
    "        meta_info = {\n",
    "            'eos_token_id': self.tokenizer.eos_token_id,\n",
    "            'pad_token_id': self.tokenizer.pad_token_id\n",
    "        }\n",
    "        prompts.meta_info.update(meta_info)\n",
    "\n",
    "\n",
    "        # ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§å‡¦ç†\n",
    "        with self.rollout_sharding_manager:\n",
    "\n",
    "            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "            log_gpu_memory_usage(\n",
    "                'After entering rollout sharding manager', logger=logger\n",
    "            )\n",
    "\n",
    "            # ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†\n",
    "            prompts = self.rollout_sharding_manager.preprocess_data(prompts)\n",
    "\n",
    "            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç”Ÿæˆ\n",
    "            output = self.rollout.generate_sequences(prompts=prompts)\n",
    "\n",
    "            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "            log_gpu_memory_usage('After rollout generation', logger=logger)\n",
    "\n",
    "            # ãƒ‡ãƒ¼ã‚¿ã‚’å¾Œå‡¦ç† \n",
    "            output = self.rollout_sharding_manager.postprocess_data(output)\n",
    "\n",
    "        # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ã‹ã¤ã€å¯¾æ•°ç¢ºç‡ã‚’å†è¨ˆç®—ã™ã‚‹å ´åˆ\n",
    "        if self._is_actor and recompute_log_prob:\n",
    "\n",
    "            # å‡ºåŠ›ã®ãƒ¡ã‚¿æƒ…å ±ã«ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¿½åŠ \n",
    "            output.meta_info['micro_batch_size'] = \\\n",
    "                self.config.rollout.log_prob_micro_batch_size\n",
    "\n",
    "            # å‡ºåŠ›ã®ãƒ¡ã‚¿æƒ…å ±ã«æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·ã‚’è¿½åŠ \n",
    "            output.meta_info['max_token_len'] = \\\n",
    "                self.config.rollout.log_prob_max_token_len_per_gpu\n",
    "\n",
    "            # å‡ºåŠ›ã®ãƒ¡ã‚¿æƒ…å ±ã«å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºä½¿ç”¨ãƒ•ãƒ©ã‚°ã‚’è¿½åŠ \n",
    "            output.meta_info['use_dynamic_bsz'] = \\\n",
    "                self.config.rollout.log_prob_use_dynamic_bsz\n",
    "\n",
    "            # å‡ºåŠ›ã®ãƒ¡ã‚¿æƒ…å ±ã«æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ \n",
    "            output.meta_info['temperature'] = \\\n",
    "                self.config.rollout.temperature\n",
    "\n",
    "            # Ulyssesã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§å‡¦ç†\n",
    "            with self.ulysses_sharding_manager:\n",
    "\n",
    "                # ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†\n",
    "                output = self.ulysses_sharding_manager.preprocess_data(output)\n",
    "\n",
    "                # å¯¾æ•°ç¢ºç‡ã‚’è¨ˆç®—\n",
    "                old_log_probs = self.actor.compute_log_prob(data=output)\n",
    "\n",
    "                # å¯¾æ•°ç¢ºç‡ã‚’å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ \n",
    "                output.batch['old_log_probs'] = old_log_probs\n",
    "\n",
    "                # ãƒ‡ãƒ¼ã‚¿ã‚’å¾Œå‡¦ç†\n",
    "                output = self.ulysses_sharding_manager.postprocess_data(output)\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚’CPUã«è»¢é€\n",
    "        output = output.to('cpu')\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ\n",
    "        if self._is_offload_param:\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰\n",
    "            offload_fsdp_param_and_grad(\n",
    "                module=self.actor_module_fsdp,\n",
    "                offload_grad=self._is_offload_grad\n",
    "            )\n",
    "\n",
    "        # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "        log_gpu_memory_usage('After recompute log prob', logger=logger)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\n",
    "    def compute_ref_log_prob(self, data: DataProto):\n",
    "        \"\"\"\n",
    "        å‚ç…§ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦å¯¾æ•°ç¢ºç‡ã‚’è¨ˆç®—ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            data (DataProto): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿\n",
    "        Returns:\n",
    "            output (DataProto): å¯¾æ•°ç¢ºç‡ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒˆã‚³ãƒ«\n",
    "        \"\"\"\n",
    "\n",
    "        # å‚ç…§ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n",
    "        assert self._is_ref\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«è»¢é€\n",
    "        data = data.to('cuda')\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã‚‹å ´åˆ\n",
    "        if self._is_offload_param:\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "            load_fsdp_param_and_grad(\n",
    "                module=self.ref_module_fsdp,\n",
    "                device_id=torch.cuda.current_device(),\n",
    "                load_grad=self._is_offload_grad\n",
    "            )\n",
    "\n",
    "        # ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "        micro_batch_size = self.config.ref.log_prob_micro_batch_size\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã®ãƒ¡ã‚¿æƒ…å ±ã«ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¿½åŠ \n",
    "        data.meta_info['micro_batch_size'] = micro_batch_size\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã®ãƒ¡ã‚¿æƒ…å ±ã«æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ \n",
    "        data.meta_info['temperature'] = self.config.rollout.temperature\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã®ãƒ¡ã‚¿æƒ…å ±ã«æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·ã‚’è¿½åŠ \n",
    "        data.meta_info['max_token_len'] = \\\n",
    "            self.config.ref.log_prob_max_token_len_per_gpu\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã®ãƒ¡ã‚¿æƒ…å ±ã«å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºä½¿ç”¨ãƒ•ãƒ©ã‚°ã‚’è¿½åŠ \n",
    "        data.meta_info['use_dynamic_bsz'] = \\\n",
    "            self.config.ref.log_prob_use_dynamic_bsz\n",
    "\n",
    "        # Ulyssesé€šä¿¡ã‚’é–‹å§‹ï¼ˆé•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä¸¦åˆ—å‡¦ç†ï¼‰\n",
    "        with self.ulysses_sharding_manager:\n",
    "\n",
    "            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã™ã‚‹\n",
    "            data = self.ulysses_sharding_manager.preprocess_data(data)\n",
    "\n",
    "            # å‚ç…§ãƒ¢ãƒ‡ãƒ«ã§å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—å‡¦ç†ã—ã¦ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®å¯¾æ•°ç¢ºç‡ã‚’è¨ˆç®—\n",
    "            output = self.ref_policy.compute_log_prob(data=data)\n",
    "\n",
    "            # å¯¾æ•°ç¢ºç‡ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«æ ¼ç´\n",
    "            output = DataProto.from_dict(tensors={'ref_log_prob': output})\n",
    "\n",
    "            # åˆ†å‰²ã•ã‚ŒãŸå¯¾æ•°ç¢ºç‡ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ã™ã‚‹\n",
    "            output = self.ulysses_sharding_manager.postprocess_data(output)\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚’CPUã«è»¢é€\n",
    "        output = output.to('cpu')\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ\n",
    "        if self._is_offload_param:\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰\n",
    "            offload_fsdp_param_and_grad(\n",
    "                module=self.ref_module_fsdp,\n",
    "                offload_grad=self._is_offload_grad\n",
    "            )\n",
    "\n",
    "        # ä¸è¦ãªGPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return output\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.ONE_TO_ALL)\n",
    "    def save_checkpoint(self, local_path, hdfs_path=None):\n",
    "        \"\"\"\n",
    "        ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹\n",
    "        HDFSãƒ‘ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€HDFSã«ã‚‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            local_path (str): ãƒ­ãƒ¼ã‚«ãƒ«ã®ä¿å­˜ãƒ‘ã‚¹\n",
    "            hdfs_path (str, optional): HDFSã®ä¿å­˜ãƒ‘ã‚¹\n",
    "        \"\"\"\n",
    "\n",
    "        # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n",
    "        assert self._is_actor\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã‚‹å ´åˆ\n",
    "        if self._is_offload_param:\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "            load_fsdp_param_and_grad(\n",
    "                module=self.actor_module_fsdp,\n",
    "                device_id=torch.cuda.current_device(),\n",
    "                load_grad=self._is_offload_grad\n",
    "            )\n",
    "\n",
    "        # stete dictï¼ˆãƒ¢ãƒ‡ãƒ«ã‚„æœ€é©åŒ–é–¢æ•°ã®çŠ¶æ…‹ï¼‰ã®é›†ç´„æ–¹æ³•ã‚’å®šç¾©\n",
    "        cfg = FullStateDictConfig(\n",
    "            offload_to_cpu=True, # CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰\n",
    "            rank0_only=True, # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®ã¿\n",
    "        )\n",
    "\n",
    "        # state dictã‚’é›†ç´„ã™ã‚‹é€šä¿¡ã‚’é–‹å§‹\n",
    "        with FSDP.state_dict_type(\n",
    "            self.actor.actor_module,\n",
    "            StateDictType.FULL_STATE_DICT,\n",
    "            cfg\n",
    "        ):\n",
    "            # state dictã‚’é›†ç´„\n",
    "            state_dict = self.actor.actor_module.state_dict()\n",
    "\n",
    "        # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®å ´åˆ\n",
    "        if self.rank == 0:\n",
    "            logger.info(f'Saving actor checkpoint to {local_path}')\n",
    "\n",
    "            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "            # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä¿å­˜\n",
    "            self.actor_module.save_pretrained(local_path, state_dict=state_dict)\n",
    "\n",
    "            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ä¿å­˜\n",
    "            self.tokenizer.save_pretrained(local_path)\n",
    "\n",
    "            # HDFSãƒ‘ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "            if hdfs_path is not None:\n",
    "                logger.info(f'Uploading actor checkpoint to {hdfs_path}')\n",
    "\n",
    "                # HDFSã®ãƒ‘ã‚¹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "                hdfs_io.makedirs(hdfs_path, exist_ok=True)\n",
    "\n",
    "                # ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰HDFSã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã‚³ãƒ”ãƒ¼\n",
    "                hdfs_io.copy(src=local_path, dst=hdfs_path)\n",
    "\n",
    "        # å…¨ãƒ—ãƒ­ã‚»ã‚¹ã§åŒæœŸ\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ\n",
    "        if self._is_offload_param:\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰\n",
    "            offload_fsdp_param_and_grad(\n",
    "                module=self.actor_module_fsdp,\n",
    "                offload_grad=self._is_offload_grad\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e5d99",
   "metadata": {},
   "source": [
    "#### CriticWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8286af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticWorker(Worker):\n",
    "    \"\"\"\n",
    "    ä¾¡å€¤é–¢æ•°ï¼ˆã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ï¼‰ã®å­¦ç¿’ãƒ»æ¨è«–ã‚’æ‹…å½“ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config (DictConfig): ãƒ¯ãƒ¼ã‚«ãƒ¼ã®è¨­å®š\n",
    "        \"\"\"\n",
    "        logger.info(f\"CriticWorkerã‚’åˆæœŸåŒ– {config=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # åˆ†æ•£å‡¦ç†ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆ\n",
    "        if not torch.distributed.is_initialized():\n",
    "\n",
    "            # NCCLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§åˆ†æ•£å‡¦ç†ã‚’åˆæœŸåŒ–\n",
    "            torch.distributed.init_process_group(backend=\"nccl\")\n",
    "\n",
    "        # è¨­å®šã‚’å±æ€§ã«ä¿å­˜\n",
    "        self.config = config\n",
    "\n",
    "        # 1) ãƒ¦ãƒªã‚·ãƒ¼ã‚ºã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—åŒ–ã®ãŸã‚ã«ãƒ‡ãƒã‚¤ã‚¹ãƒ¡ãƒƒã‚·ãƒ¥ã‚’æ§‹ç¯‰\n",
    "\n",
    "        # å…¨ã¦ã®GPUã®æ•°ã‚’å–å¾—\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "\n",
    "        # ãƒ‡ãƒã‚¤ã‚¹ãƒ¡ãƒƒã‚·ãƒ¥ã‚’åˆæœŸåŒ–\n",
    "        self.ulysses_device_mesh = None\n",
    "\n",
    "        # ãƒ¦ãƒªã‚·ãƒ¼ã‚ºã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—ã‚µã‚¤ã‚ºã‚’è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1ï¼‰\n",
    "        self.ulysses_sequence_parallel_size = \\\n",
    "            self.config.get('ulysses_sequence_parallel_size', 1)\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã‚µã‚¤ã‚ºï¼ˆdata parallelï¼‰ã‚’è¨ˆç®—\n",
    "        dp = world_size // self.ulysses_sequence_parallel_size\n",
    "\n",
    "        # ãƒ¦ãƒªã‚·ãƒ¼ã‚ºã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—ã‚µã‚¤ã‚ºãŒ1ã‚ˆã‚Šå¤§ãã„å ´åˆ\n",
    "        if self.ulysses_sequence_parallel_size > 1:\n",
    "\n",
    "            # ãƒ‡ãƒã‚¤ã‚¹ãƒ¡ãƒƒã‚·ãƒ¥ã‚’åˆæœŸåŒ–\n",
    "            # å½¢çŠ¶ã¯ (ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã‚µã‚¤ã‚º, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—ã‚µã‚¤ã‚º)\n",
    "            # ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã¯ã€ãƒãƒƒãƒã”ã¨ã«ç•°ãªã‚‹GPUã«ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¦å‡¦ç†ã™ã‚‹æ‰‹æ³•\n",
    "            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—ã¯ã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’è¤‡æ•°ã®GPUã«åˆ†å‰²ã—ã¦å‡¦ç†ã™ã‚‹æ‰‹æ³•\n",
    "            self.ulysses_device_mesh = init_device_mesh(\n",
    "                'cuda',\n",
    "                mesh_shape=(dp, self.ulysses_sequence_parallel_size),\n",
    "                mesh_dim_names=['dp', 'sp']\n",
    "            )\n",
    "\n",
    "        # ãƒ¦ãƒªã‚·ãƒ¼ã‚ºã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ã‚’åˆæœŸåŒ–\n",
    "        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®åˆ†å‰²ãƒ»é›†ç´„ãªã©ã®é€šä¿¡ã‚„ç®¡ç†ã‚’è¡Œã†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "        self.ulysses_sharding_manager = FSDPUlyssesShardingManager(\n",
    "            self.ulysses_device_mesh\n",
    "        )\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã®è¨­å®šã‚’å–å¾—\n",
    "        self._is_offload_param = self.config.model.fsdp_config.param_offload\n",
    "\n",
    "        # å‹¾é…ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã®è¨­å®šã‚’å–å¾—\n",
    "        self._is_offload_grad = self.config.model.fsdp_config.grad_offload\n",
    "\n",
    "        # æœ€é©åŒ–é–¢æ•°ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã®è¨­å®šã‚’å–å¾—\n",
    "        self._is_offload_optimizer = self.config.model.fsdp_config.optimizer_offload\n",
    "\n",
    "        # ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨ˆç®—\n",
    "        self.config.ppo_mini_batch_size //= (\n",
    "            torch.distributed.get_world_size() // self.ulysses_sequence_parallel_size\n",
    "        )\n",
    "\n",
    "        # ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨ˆç®—\n",
    "        self.config.ppo_micro_batch_size //= (\n",
    "            torch.distributed.get_world_size() // self.ulysses_sequence_parallel_size\n",
    "        )\n",
    "\n",
    "        # é †ä¼æ¬ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨ˆç®—\n",
    "        self.config.forward_micro_batch_size //= (\n",
    "            torch.distributed.get_world_size() // self.ulysses_sequence_parallel_size\n",
    "        )\n",
    "\n",
    "    def _build_critic_model_optimizer(self, config):\n",
    "        \"\"\"\n",
    "        ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            config (DictConfig): ãƒ¯ãƒ¼ã‚«ãƒ¼ã®è¨­å®š\n",
    "        Returns:\n",
    "            critic_module (FSDP): ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã®FSDPãƒ©ãƒƒãƒ—\n",
    "            critic_optimizer (Optimizer): ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶\n",
    "            critic_lr_scheduler (LRScheduler): ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©\n",
    "        \"\"\"\n",
    "\n",
    "        # ã‚³ãƒ”ãƒ¼å…ƒã®HDFSãƒ‘ã‚¹ã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ”ãƒ¼\n",
    "        local_path = copy_local_path_from_hdfs(config.model.path)\n",
    "\n",
    "        # HDFSãƒ‘ã‚¹ã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ã‚³ãƒ”ãƒ¼\n",
    "        tokenizer_path = copy_local_path_from_hdfs(config.model.tokenizer_path)\n",
    "\n",
    "        # Hugging Faceã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’åˆæœŸåŒ–\n",
    "        self.tokenizer = hf_tokenizer(\n",
    "            tokenizer_path,\n",
    "            trust_remote_code=config.model.get('trust_remote_code', False)\n",
    "        )\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®ä¸Šæ›¸ãè¨­å®šã‚’å–å¾—\n",
    "        override_config = OmegaConf.to_container(\n",
    "            self.config.model.get('override_config', OmegaConf.create())\n",
    "        )\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ä¸Šæ›¸ãè¨­å®šã«è¿½åŠ \n",
    "        override_config_kwargs = {\n",
    "            'bos_token_id': self.tokenizer.bos_token_id,\n",
    "            'eos_token_id': self.tokenizer.eos_token_id,\n",
    "            'pad_token_id': self.tokenizer.pad_token_id,\n",
    "        }\n",
    "\n",
    "        # ä¸Šæ›¸ãè¨­å®šã‚’ãƒãƒ¼ã‚¸\n",
    "        override_config_kwargs.update(override_config)\n",
    "\n",
    "        # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®å ´åˆ\n",
    "        if self.rank == 0:\n",
    "            print(f'ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®ä¸Šæ›¸ãè¨­å®š: {override_config_kwargs}')\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’å–å¾—\n",
    "        torch_dtype = self.config.model.fsdp_config.get('model_dtype', 'fp32')\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿å‹ã‚’PyTorchã®dtypeã«å¤‰æ›\n",
    "        torch_dtype = PrecisionType.to_dtype(torch_dtype)\n",
    "\n",
    "        trust_remote_code = False\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’åˆæœŸåŒ–\n",
    "        critic_model_config = AutoConfig.from_pretrained(\n",
    "            local_path,\n",
    "            trust_remote_code=trust_remote_code\n",
    "        )\n",
    "\n",
    "        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ãƒ™ãƒ«æ•°ã‚’1ã«è¨­å®š\n",
    "        critic_model_config.num_labels = 1\n",
    "\n",
    "        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã‚’å–å¾—\n",
    "        use_remove_padding = config.model.get('use_remove_padding', False)\n",
    "\n",
    "        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ãƒ¢ãƒ‡ãƒ«ãŒå¯¾å¿œã—ã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "        if use_remove_padding:\n",
    "            check_model_support_rmpad(critic_model_config.model_type)\n",
    "\n",
    "        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã‚’ä½¿ç”¨ã—ã€ã‹ã¤ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—ã‚µã‚¤ã‚ºãŒ1ã‚ˆã‚Šå¤§ãã„å ´åˆ\n",
    "        if use_remove_padding and self.ulysses_sequence_parallel_size > 1:\n",
    "            # ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ‘ãƒƒãƒã‚’é©ç”¨ã—ã¦ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã‚’æœ‰åŠ¹åŒ–\n",
    "            apply_monkey_patch(critic_model_config, verbose=True)\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ã‚’å–å¾—\n",
    "        init_context = get_init_weight_context_manager()\n",
    "\n",
    "        with init_context(), warnings.catch_warnings():\n",
    "\n",
    "            # è­¦å‘Šã‚’ç„¡è¦–\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’ç„¡åŠ¹åŒ–\n",
    "            setattr(critic_model_config, 'classifier_dropout', 0.)\n",
    "            setattr(critic_model_config, 'hidden_dropout', '0')\n",
    "\n",
    "            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–\n",
    "            critic_module = \\\n",
    "                AutoModelForTokenClassification.from_pretrained(\n",
    "                    pretrained_model_name_or_path=local_path,\n",
    "                    torch_dtype=torch_dtype,\n",
    "                    config=critic_model_config,\n",
    "                    attn_implementation='flash_attention_2',\n",
    "                    trust_remote_code=trust_remote_code\n",
    "                )\n",
    "\n",
    "            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã®ãƒ‡ãƒ¼ã‚¿å‹ã«å¤‰æ›\n",
    "            critic_module.to(torch_dtype)\n",
    "\n",
    "            # å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æœ‰åŠ¹åŒ–ã™ã‚‹å ´åˆ\n",
    "            if config.model.get('enable_gradient_checkpointing', False):\n",
    "\n",
    "                # å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æœ‰åŠ¹åŒ–\n",
    "                critic_module.gradient_checkpointing_enable(\n",
    "                    gradient_checkpointing_kwargs={'use_reentrant': False}\n",
    "                )\n",
    "\n",
    "        # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®å ´åˆ\n",
    "        if self.rank == 0:\n",
    "\n",
    "            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã‚’è¡¨ç¤º\n",
    "            print_model_size(critic_module)\n",
    "\n",
    "\n",
    "        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚’å±æ€§ã«ä¿å­˜\n",
    "        self.critic_model_config = critic_model_config\n",
    "\n",
    "        # FSDPã®åˆæœŸåŒ–é–¢æ•°ã‚’å–å¾—\n",
    "        fsdp_config = self.config.model.fsdp_config\n",
    "\n",
    "        # è‡ªå‹•æ··åˆç²¾åº¦ã®è¨­å®šã‚’å–å¾—\n",
    "        mixed_precision_config = fsdp_config.get('mixed_precision', None)\n",
    "\n",
    "        # è‡ªå‹•æ··åˆç²¾åº¦ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "        if mixed_precision_config is not None:\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’å–å¾—\n",
    "            param_dtype = PrecisionType.to_dtype(\n",
    "                mixed_precision_config.get('param_dtype', 'bf16')\n",
    "            )\n",
    "\n",
    "            # é›†ç´„ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’å–å¾—\n",
    "            reduce_dtype = PrecisionType.to_dtype(\n",
    "                mixed_precision_config.get('reduce_dtype', 'fp32')\n",
    "            )\n",
    "\n",
    "            # ãƒãƒƒãƒ•ã‚¡ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’å–å¾—\n",
    "            buffer_dtype = PrecisionType.to_dtype(\n",
    "                mixed_precision_config.get('buffer_dtype', 'fp32')\n",
    "            )\n",
    "\n",
    "        # è‡ªå‹•æ··åˆç²¾åº¦ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆ\n",
    "        else:\n",
    "            param_dtype = torch.bfloat16\n",
    "            reduce_dtype = torch.float32\n",
    "            buffer_dtype = torch.float32\n",
    "\n",
    "        # FSDPã®è‡ªå‹•æ··åˆç²¾åº¦ã‚’åˆæœŸåŒ–\n",
    "        mixed_precision = MixedPrecision(\n",
    "            param_dtype=param_dtype,\n",
    "            reduce_dtype=reduce_dtype,\n",
    "            buffer_dtype=buffer_dtype\n",
    "        )\n",
    "\n",
    "        # è‡ªå‹•ãƒ©ãƒƒãƒ—ãƒãƒªã‚·ãƒ¼ã‚’å–å¾—\n",
    "        auto_wrap_policy = get_fsdp_wrap_policy(\n",
    "            module=critic_module,\n",
    "            config=self.config.model.fsdp_config.wrap_policy\n",
    "        )\n",
    "\n",
    "        # GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "        log_gpu_memory_usage('Before critic FSDP', logger=None)\n",
    "\n",
    "        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’FSDPã§ãƒ©ãƒƒãƒ—\n",
    "        critic_module = FSDP(\n",
    "            critic_module,\n",
    "            param_init_fn=init_fn,\n",
    "            use_orig_params=False,\n",
    "            auto_wrap_policy=auto_wrap_policy,\n",
    "            device_id=torch.cuda.current_device(),\n",
    "            sharding_strategy=ShardingStrategy.FULL_SHARD,\n",
    "            mixed_precision=mixed_precision,\n",
    "            sync_module_states=True,\n",
    "            forward_prefetch=False\n",
    "        )\n",
    "\n",
    "        # GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "        log_gpu_memory_usage('After critic FSDP', logger=None)\n",
    "\n",
    "        # AdamWã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’åˆæœŸåŒ–\n",
    "        critic_optimizer = optim.AdamW(\n",
    "            critic_module.parameters(),\n",
    "            lr=config.optim.lr,\n",
    "            betas=config.optim.get('betas', (0.9, 0.999)),\n",
    "            weight_decay=config.optim.get('weight_decay', 1e-2)\n",
    "        )\n",
    "\n",
    "        # ç·ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’å–å¾—\n",
    "        total_steps = config.optim.get('total_training_steps', 0)\n",
    "        print(f\"ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã®ç·ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—æ•°: {total_steps}\")\n",
    "\n",
    "        # å­¦ç¿’ç‡ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®æ¯”ç‡ã‚’å–å¾—\n",
    "        num_warmup_steps_ratio = config.optim.get('lr_warmup_steps_ratio', 0.)\n",
    "\n",
    "        # å­¦ç¿’ç‡ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è¨ˆç®—\n",
    "        num_warmup_steps = int(num_warmup_steps_ratio * total_steps)\n",
    "        print(f\"ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ç‡ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—æ•°: {num_warmup_steps}\")\n",
    "\n",
    "        # å®šæ•°å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’åˆæœŸåŒ–\n",
    "        critic_lr_scheduler = get_constant_schedule_with_warmup(\n",
    "            optimizer=critic_optimizer,\n",
    "            num_warmup_steps=num_warmup_steps\n",
    "        )\n",
    "\n",
    "        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’è¿”ã™\n",
    "        return critic_module, critic_optimizer, critic_lr_scheduler\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.ONE_TO_ALL)\n",
    "    def init_model(self):\n",
    "        \"\"\"\n",
    "        ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "        \"\"\"\n",
    "\n",
    "        # å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "        import_external_libs(self.config.model.get('external_lib', None))\n",
    "\n",
    "        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’æ§‹ç¯‰\n",
    "        self.critic_module, self.critic_optimizer, self.critic_lr_scheduler = \\\n",
    "            self._build_critic_model_optimizer(self.config)\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ\n",
    "        if self._is_offload_param:\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰\n",
    "            offload_fsdp_param_and_grad(\n",
    "                module=self.critic_module,\n",
    "                offload_grad=self._is_offload_grad\n",
    "            )\n",
    "\n",
    "        # æœ€é©åŒ–é–¢æ•°ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ\n",
    "        if self._is_offload_optimizer:\n",
    "\n",
    "            # æœ€é©åŒ–é–¢æ•°ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰\n",
    "            offload_fsdp_optimizer(optimizer=self.critic_optimizer)\n",
    "\n",
    "        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–\n",
    "        self.critic = DataParallelPPOCritic(\n",
    "            config=self.config,\n",
    "            critic_module=self.critic_module,\n",
    "            critic_optimizer=self.critic_optimizer\n",
    "        )\n",
    "\n",
    "        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã®FLOPSã‚«ã‚¦ãƒ³ã‚¿ã‚’åˆæœŸåŒ–\n",
    "        self.flops_counter = FlopsCounter(self.critic_model_config)\n",
    "\n",
    "        # ä¸è¦ãªGPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\n",
    "    def compute_values(self, data: DataProto):\n",
    "        \"\"\"\n",
    "        ä¾¡å€¤é–¢æ•°ã‚’è¨ˆç®—ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            data (DataProto): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿\n",
    "        Returns:\n",
    "            output (DataProto): ä¾¡å€¤é–¢æ•°ã®å‡ºåŠ›ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒˆã‚³ãƒ«\n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«è»¢é€\n",
    "        data = data.to('cuda')\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã‚‹å ´åˆ\n",
    "        if self._is_offload_param:\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "            load_fsdp_param_and_grad(\n",
    "                module=self.critic_module,\n",
    "                device_id=torch.cuda.current_device(),\n",
    "                load_grad=self._is_offload_grad\n",
    "            )\n",
    "\n",
    "        # ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "        micro_batch_size = self.config.forward_micro_batch_size\n",
    "\n",
    "        # ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ãƒ‡ãƒ¼ã‚¿ã®ãƒ¡ã‚¿æƒ…å ±ã«è¿½åŠ \n",
    "        data.meta_info['micro_batch_size'] = micro_batch_size\n",
    "\n",
    "        # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·ã‚’ãƒ‡ãƒ¼ã‚¿ã®ãƒ¡ã‚¿æƒ…å ±ã«è¿½åŠ \n",
    "        data.meta_info['max_token_len'] = self.config.forward_max_token_len_per_gpu\n",
    "\n",
    "        # å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºä½¿ç”¨ãƒ•ãƒ©ã‚°ã‚’ãƒ‡ãƒ¼ã‚¿ã®ãƒ¡ã‚¿æƒ…å ±ã«è¿½åŠ \n",
    "        data.meta_info['use_dynamic_bsz'] = self.config.use_dynamic_bsz\n",
    "\n",
    "        # ãƒ¦ãƒªã‚·ãƒ¼ã‚ºé€šä¿¡ã‚’é–‹å§‹ï¼ˆé•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä¸¦åˆ—å‡¦ç†ï¼‰\n",
    "        with self.ulysses_sharding_manager:\n",
    "\n",
    "            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã™ã‚‹\n",
    "            data = self.ulysses_sharding_manager.preprocess_data(data=data)\n",
    "\n",
    "            # ä¾¡å€¤é–¢æ•°ã‚’è¨ˆç®—\n",
    "            values = self.critic.compute_values(data=data)\n",
    "\n",
    "            # ä¾¡å€¤é–¢æ•°ã®å‡ºåŠ›ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«æ ¼ç´\n",
    "            output = DataProto.from_dict(tensors={'values': values})\n",
    "\n",
    "            # åˆ†å‰²ã•ã‚ŒãŸä¾¡å€¤é–¢æ•°ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ã™ã‚‹\n",
    "            output = self.ulysses_sharding_manager.postprocess_data(data=output)\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚’CPUã«è»¢é€\n",
    "        output = output.to('cpu')\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ\n",
    "        if self._is_offload_param:\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰\n",
    "            offload_fsdp_param_and_grad(\n",
    "                module=self.critic_module,\n",
    "                offload_grad=self._is_offload_grad\n",
    "            )\n",
    "\n",
    "        # ä¸è¦ãªGPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return output\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\n",
    "    def update_critic(self, data: DataProto):\n",
    "        \"\"\"\n",
    "        ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            data (DataProto): ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿\n",
    "        Returns:\n",
    "            output (DataProto): ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒˆã‚³ãƒ«\n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«è»¢é€\n",
    "        data = data.to('cuda')\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã‚‹å ´åˆ\n",
    "        if self._is_offload_param:\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "            load_fsdp_param_and_grad(\n",
    "                module=self.critic_module,\n",
    "                device_id=torch.cuda.current_device(),\n",
    "                load_grad=self._is_offload_grad\n",
    "            )\n",
    "\n",
    "        # æœ€é©åŒ–é–¢æ•°ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã‚‹å ´åˆ\n",
    "        if self._is_offload_optimizer:\n",
    "\n",
    "            # æœ€é©åŒ–é–¢æ•°ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "            load_fsdp_optimizer(\n",
    "                optimizer=self.critic_optimizer,\n",
    "                device_id=torch.cuda.current_device()\n",
    "            )\n",
    "\n",
    "        # ãƒ¦ãƒªã‚·ãƒ¼ã‚ºé€šä¿¡ã‚’é–‹å§‹ï¼ˆé•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä¸¦åˆ—å‡¦ç†ï¼‰\n",
    "        with self.ulysses_sharding_manager:\n",
    "\n",
    "            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã™ã‚‹\n",
    "            data = self.ulysses_sharding_manager.preprocess_data(data=data)\n",
    "\n",
    "            # ã‚¿ã‚¤ãƒãƒ¼ã‚’é–‹å§‹ã—ã¦ã€ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°\n",
    "            with Timer(name='update_critic', logger=None) as timer:\n",
    "                metrics = self.critic.update_critic(data=data)\n",
    "\n",
    "            # çµŒéæ™‚é–“ã‚’å–å¾—\n",
    "            delta_time = timer.last\n",
    "\n",
    "            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å–å¾—\n",
    "            global_num_tokens = data.meta_info['global_token_num']\n",
    "\n",
    "            # FLOPSã‚’è¨ˆç®—\n",
    "            estimated_flops, promised_flops = \\\n",
    "                self.flops_counter.estimate_flops(global_num_tokens, delta_time)\n",
    "\n",
    "            # MFUï¼ˆModel FLOPS Utilizationï¼‰ã‚’è¨ˆç®—ã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ \n",
    "            metrics['mfu/critic'] = \\\n",
    "                estimated_flops * self.config.ppo_epochs / \\\n",
    "                    promised_flops / self.world_size\n",
    "\n",
    "            # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ã‚¹ãƒ†ãƒƒãƒ—\n",
    "            self.critic_lr_scheduler.step()\n",
    "\n",
    "            # ç¾åœ¨ã®å­¦ç¿’ç‡ã‚’å–å¾—\n",
    "            lr = self.critic_lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "            # å­¦ç¿’ç‡ã‚’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ \n",
    "            metrics['critic/lr'] = lr\n",
    "\n",
    "            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ä½œæˆ\n",
    "            output = DataProto(batch=None, meta_info={'metrics': metrics})\n",
    "\n",
    "            # åˆ†å‰²ã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ã™ã‚‹\n",
    "            output = self.ulysses_sharding_manager.postprocess_data(data=output)\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ\n",
    "        if self._is_offload_param:\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰\n",
    "            offload_fsdp_param_and_grad(\n",
    "                module=self.critic_module,\n",
    "                offload_grad=self._is_offload_grad\n",
    "            )\n",
    "\n",
    "        # æœ€é©åŒ–é–¢æ•°ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ\n",
    "        if self._is_offload_optimizer:\n",
    "\n",
    "            # æœ€é©åŒ–é–¢æ•°ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰\n",
    "            offload_fsdp_optimizer(optimizer=self.critic_optimizer)\n",
    "\n",
    "        # ä¸è¦ãªGPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚’CPUã«è»¢é€\n",
    "        output = output.to('cpu')\n",
    "\n",
    "        return output\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.ONE_TO_ALL)\n",
    "    def save_checkpoint(self, local_path, hdfs_path=None):\n",
    "        \"\"\"\n",
    "        ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            local_path (str): ãƒ­ãƒ¼ã‚«ãƒ«ã®ä¿å­˜ãƒ‘ã‚¹\n",
    "            hdfs_path (str, optional): HDFSã®ä¿å­˜ãƒ‘ã‚¹\n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã‚‹å ´åˆ\n",
    "        if self._is_offload_param:\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "            load_fsdp_param_and_grad(\n",
    "                module=self.critic_module,\n",
    "                device_id=torch.cuda.current_device(),\n",
    "                load_grad=self._is_offload_grad\n",
    "            )\n",
    "\n",
    "        # stete dictï¼ˆãƒ¢ãƒ‡ãƒ«ã‚„æœ€é©åŒ–é–¢æ•°ã®çŠ¶æ…‹ï¼‰ã®é›†ç´„æ–¹æ³•ã‚’å®šç¾©\n",
    "        cfg = FullStateDictConfig(\n",
    "            offload_to_cpu=True, # CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰\n",
    "            rank0_only=True # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®ã¿\n",
    "        )\n",
    "\n",
    "        # state dictã‚’é›†ç´„ã™ã‚‹é€šä¿¡ã‚’é–‹å§‹\n",
    "        with FSDP.state_dict_type(\n",
    "            self.critic_module,\n",
    "            StateDictType.FULL_STATE_DICT,\n",
    "            cfg\n",
    "        ):\n",
    "\n",
    "            # state dictã‚’é›†ç´„\n",
    "            state_dict = self.critic_module.state_dict()\n",
    "\n",
    "        # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®å ´åˆ\n",
    "        if self.rank == 0:\n",
    "            print(f'ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜: {local_path}')\n",
    "\n",
    "            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä¿å­˜\n",
    "            self.critic_module._fsdp_wrapped_module.save_pretrained(\n",
    "                local_path,\n",
    "                state_dict=state_dict\n",
    "            )\n",
    "\n",
    "            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ä¿å­˜\n",
    "            self.tokenizer.save_pretrained(local_path)\n",
    "\n",
    "            # HDFSãƒ‘ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "            if hdfs_path is not None:\n",
    "                print(f'ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’HDFSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰: {hdfs_path}')\n",
    "\n",
    "                # HDFSã®ãƒ‘ã‚¹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "                hdfs_io.makedirs(hdfs_path, exist_ok=True)\n",
    "\n",
    "                # ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰HDFSã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã‚³ãƒ”ãƒ¼\n",
    "                hdfs_io.copy(src=local_path, dst=hdfs_path)\n",
    "\n",
    "        # å…¨ãƒ—ãƒ­ã‚»ã‚¹ã§åŒæœŸ\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ\n",
    "        if self._is_offload_param:\n",
    "\n",
    "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰\n",
    "            offload_fsdp_param_and_grad(\n",
    "                module=self.critic_module,\n",
    "                offload_grad=self._is_offload_grad\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a3599c",
   "metadata": {},
   "source": [
    "#### RewardModelWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModelWorker(Worker):\n",
    "    \"\"\"\n",
    "    å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã‚’æ‹…å½“ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ã‚¹\n",
    "    ä»Šå›ã¯ä½¿ã‚ãªã„\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        print(f\"RewardModelWorkerã‚’åˆæœŸåŒ– {config=}\")\n",
    "\n",
    "        # åˆ†æ•£å‡¦ç†ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆ\n",
    "        if not torch.distributed.is_initialized():\n",
    "\n",
    "            # NCCLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§åˆ†æ•£å‡¦ç†ã‚’åˆæœŸåŒ–\n",
    "            torch.distributed.init_process_group(backend=\"nccl\")\n",
    "\n",
    "        # è¨­å®šã‚’å±æ€§ã«ä¿å­˜\n",
    "        self.config = config\n",
    "\n",
    "        # ãƒ¦ãƒªã‚·ãƒ¼ã‚ºã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—åŒ–ã®ãŸã‚ã«ãƒ‡ãƒã‚¤ã‚¹ãƒ¡ãƒƒã‚·ãƒ¥ã‚’æ§‹ç¯‰\n",
    "\n",
    "        # å…¨ã¦ã®GPUã®æ•°ã‚’å–å¾—\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "\n",
    "        # ãƒ‡ãƒã‚¤ã‚¹ãƒ¡ãƒƒã‚·ãƒ¥ã‚’åˆæœŸåŒ–\n",
    "        self.ulysses_device_mesh = None\n",
    "\n",
    "        # ãƒ¦ãƒªã‚·ãƒ¼ã‚ºã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—ã‚µã‚¤ã‚ºã‚’è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1ï¼‰\n",
    "        self.ulysses_sequence_parallel_size = \\\n",
    "            self.config.get('ulysses_sequence_parallel_size', 1)\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã‚µã‚¤ã‚ºï¼ˆdata parallelï¼‰ã‚’è¨ˆç®—\n",
    "        dp = world_size // self.ulysses_sequence_parallel_size\n",
    "\n",
    "        # ãƒ¦ãƒªã‚·ãƒ¼ã‚ºã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—ã‚µã‚¤ã‚ºãŒ1ã‚ˆã‚Šå¤§ãã„å ´åˆ\n",
    "        if self.ulysses_sequence_parallel_size > 1:\n",
    "\n",
    "            # ãƒ‡ãƒã‚¤ã‚¹ãƒ¡ãƒƒã‚·ãƒ¥ã‚’åˆæœŸåŒ–\n",
    "            self.ulysses_device_mesh = init_device_mesh(\n",
    "                'cuda',\n",
    "                mesh_shape=(dp, self.ulysses_sequence_parallel_size),\n",
    "                mesh_dim_names=['dp', 'sp']\n",
    "            )\n",
    "\n",
    "        # ãƒ¦ãƒªã‚·ãƒ¼ã‚ºã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ã‚’åˆæœŸåŒ–\n",
    "        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®åˆ†å‰²ãƒ»é›†ç´„ãªã©ã®é€šä¿¡ã‚„ç®¡ç†ã‚’è¡Œã†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "        self.ulysses_sharding_manager = FSDPUlyssesShardingManager(\n",
    "            self.ulysses_device_mesh\n",
    "        )\n",
    "\n",
    "        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ä½¿ç”¨ãƒ•ãƒ©ã‚°ã‚’å–å¾—ã—ã€å±æ€§ã«ä¿å­˜\n",
    "        self.use_remove_padding = self.config.model.get(\n",
    "            'use_remove_padding', False\n",
    "        )\n",
    "\n",
    "        # ãƒ—ãƒ­ã‚»ã‚¹ã”ã¨ã®ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨ˆç®—\n",
    "        self.config.micro_batch_size //= torch.distributed.get_world_size()\n",
    "\n",
    "    def _build_model(self, config):\n",
    "        \"\"\"\n",
    "        å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            config (DictConfig): ãƒ¯ãƒ¼ã‚«ãƒ¼ã®è¨­å®š\n",
    "        Returns:\n",
    "            reward_module (FSDP): FSDPã§ãƒ©ãƒƒãƒ—ã•ã‚ŒãŸå ±é…¬ãƒ¢ãƒ‡ãƒ«\n",
    "        \"\"\"\n",
    "\n",
    "        # HDFSãƒ‘ã‚¹ã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ”ãƒ¼\n",
    "        local_path = copy_local_path_from_hdfs(config.model.path)\n",
    "\n",
    "        # å…¥åŠ›ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©å¿œç”¨ï¼‰ã®è¨­å®šãŒãªã„å ´åˆ\n",
    "        if self.config.model.input_tokenizer is None:\n",
    "\n",
    "            # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’åˆ‡ã‚Šæ›¿ãˆãªã„ãƒ•ãƒ©ã‚°ã‚’ç„¡åŠ¹åŒ–\n",
    "            self._do_switch_chat_template = False\n",
    "\n",
    "        # å…¥åŠ›ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©å¿œç”¨ï¼‰ã®è¨­å®šãŒã‚ã‚‹å ´åˆ\n",
    "        else:\n",
    "\n",
    "            # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ãƒ•ãƒ©ã‚°ã‚’æœ‰åŠ¹åŒ–\n",
    "            self._do_switch_chat_template = True\n",
    "\n",
    "            # HDFSãƒ‘ã‚¹ã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ã‚³ãƒ”ãƒ¼\n",
    "            input_tokenizer_local_path = copy_local_path_from_hdfs(\n",
    "                config.model.input_tokenizer\n",
    "            )\n",
    "\n",
    "            # Hugging Faceã®å…¥åŠ›ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’åˆæœŸåŒ–\n",
    "            self.input_tokenizer = hf_tokenizer(\n",
    "                input_tokenizer_local_path,\n",
    "                trust_remote_code=config.model.get('trust_remote_code', False)\n",
    "            )\n",
    "\n",
    "            # Hugging Faceã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’åˆæœŸåŒ– \n",
    "            self.tokenizer = hf_tokenizer(\n",
    "                local_path,\n",
    "                trust_remote_code=config.model.get('trust_remote_code', False)\n",
    "            )\n",
    "\n",
    "\n",
    "        # trust_remote_codeè¨­å®šã‚’å–å¾—\n",
    "        trust_remote_code = config.model.get('trust_remote_code', False)\n",
    "\n",
    "        # å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’åˆæœŸåŒ–\n",
    "        model_config = AutoConfig.from_pretrained(\n",
    "            local_path, trust_remote_code=trust_remote_code\n",
    "        )\n",
    "\n",
    "        # å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ãƒ™ãƒ«æ•°ã‚’1ã«è¨­å®šï¼ˆå›å¸°ã‚¿ã‚¹ã‚¯ç”¨ï¼‰\n",
    "        model_config.num_labels = 1\n",
    "\n",
    "        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã‚’å–å¾—\n",
    "        use_remove_padding = config.model.get('use_remove_padding', False)\n",
    "\n",
    "        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ãƒ¢ãƒ‡ãƒ«ãŒå¯¾å¿œã—ã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "        if use_remove_padding:\n",
    "            check_model_support_rmpad(model_config.model_type)\n",
    "\n",
    "        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã‚’ä½¿ç”¨ã—ã€ã‹ã¤ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—ã‚µã‚¤ã‚ºãŒ1ã‚ˆã‚Šå¤§ãã„å ´åˆ\n",
    "        if use_remove_padding and self.ulysses_sequence_parallel_size > 1:\n",
    "            # ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ‘ãƒƒãƒã‚’é©ç”¨ã—ã¦ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã‚’æœ‰åŠ¹åŒ–\n",
    "            apply_monkey_patch(model_config, verbose=True)\n",
    "\n",
    "        # note that we have to create model in fp32. Otherwise, the optimizer is in bf16, which is incorrect\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ã‚’å–å¾—\n",
    "        init_context = get_init_weight_context_manager(\n",
    "            use_meta_tensor=not model_config.tie_word_embeddings\n",
    "        )\n",
    "\n",
    "        # åˆæœŸåŒ–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’é–‹å§‹\n",
    "        with init_context(), warnings.catch_warnings():\n",
    "\n",
    "            # è­¦å‘Šã‚’ç„¡è¦–\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            # åˆ†é¡å™¨ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’ç„¡åŠ¹åŒ–\n",
    "            setattr(model_config, 'classifier_dropout', 0.)\n",
    "\n",
    "            # å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–\n",
    "            reward_module = \\\n",
    "                AutoModelForTokenClassification.from_pretrained(\n",
    "                    pretrained_model_name_or_path=local_path,\n",
    "                    config=model_config,\n",
    "                    torch_dtype=torch.bfloat16,\n",
    "                    attn_implementation='flash_attention_2',\n",
    "                    trust_remote_code=trust_remote_code\n",
    "                )\n",
    "\n",
    "            # å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’bfloat16ã«å¤‰æ›\n",
    "            reward_module.to(torch.bfloat16)\n",
    "\n",
    "        # FSDPã®è‡ªå‹•ãƒ©ãƒƒãƒ—ãƒãƒªã‚·ãƒ¼ã‚’å–å¾—\n",
    "        auto_wrap_policy = get_fsdp_wrap_policy(\n",
    "            module=reward_module,\n",
    "            config=self.config.model.fsdp_config\n",
    "        )\n",
    "\n",
    "        # å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’FSDPã§ãƒ©ãƒƒãƒ—\n",
    "        # ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¤‡æ•°ã®GPUã«åˆ†æ•£ã•ã‚Œã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå‰Šæ¸›ã•ã‚Œã‚‹\n",
    "        reward_module = FSDP(\n",
    "            reward_module,\n",
    "            param_init_fn=init_fn,\n",
    "            use_orig_params=False,\n",
    "            auto_wrap_policy=auto_wrap_policy,\n",
    "            device_id=torch.cuda.current_device(),\n",
    "            sharding_strategy=ShardingStrategy.FULL_SHARD, # zero3\n",
    "            sync_module_states=True,\n",
    "            cpu_offload=CPUOffload(\n",
    "                offload_params=self.config.model.fsdp_config.param_offload\n",
    "            ),\n",
    "            forward_prefetch=False)\n",
    "\n",
    "        return reward_module\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.ONE_TO_ALL)\n",
    "    def init_model(self):\n",
    "        \"\"\"\n",
    "        å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "        \"\"\"\n",
    "        # å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "        import_external_libs(self.config.model.get('external_lib', None))\n",
    "\n",
    "        # å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰\n",
    "        self.reward_module = self._build_model(config=self.config)\n",
    "\n",
    "        # ä¸è¦ãªGPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def _forward_micro_batch(self, micro_batch):\n",
    "        \"\"\"\n",
    "        ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã«å¯¾ã—ã¦é †ä¼æ¬ã‚’è¡Œã†\n",
    "\n",
    "        Args:\n",
    "            micro_batch (Dict): ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã®ãƒ‡ãƒ¼ã‚¿\n",
    "        Returns:\n",
    "            rm_score (torch.Tensor): å ±é…¬ã‚¹ã‚³ã‚¢ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "\n",
    "        # å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–ã—ã€bfloat16ã§è‡ªå‹•ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "        with torch.no_grad(), torch.autocast(\n",
    "            device_type='cuda', dtype=torch.bfloat16\n",
    "        ):\n",
    "\n",
    "            # ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã®å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹IDã‚’å–å¾—\n",
    "            input_ids = micro_batch['input_ids']\n",
    "\n",
    "            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’å–å¾—\n",
    "            batch_size, seqlen = input_ids.shape\n",
    "\n",
    "            # ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "            attention_mask = micro_batch['attention_mask']\n",
    "\n",
    "            # ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã®ä½ç½®IDã‚’å–å¾—\n",
    "            position_ids = micro_batch['position_ids']\n",
    "\n",
    "            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "            if self.use_remove_padding:\n",
    "\n",
    "                # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’é™¤å»ã—ã¦å…¥åŠ›IDã‚’æ•´å½¢\n",
    "                input_ids_rmpad, indices, *_ = unpad_input(\n",
    "                    input_ids.unsqueeze(-1),\n",
    "                    attention_mask\n",
    "                )\n",
    "\n",
    "                # è»¢ç½®\n",
    "                # (seq_len, batch_size) -> (batch_size, seq_len) \n",
    "                # (1, total_nnz)\n",
    "                input_ids_rmpad = input_ids_rmpad.transpose(0, 1)\n",
    "\n",
    "                # å›è»¢ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ãŸã‚ã«ä½ç½®IDã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»\n",
    "                position_ids_rmpad = index_first_axis(\n",
    "                    rearrange(position_ids.unsqueeze(-1), \"b s ... -> (b s) ...\"),\n",
    "                    indices\n",
    "                ).transpose(0, 1)\n",
    "\n",
    "                # ãƒ¦ãƒªã‚·ãƒ¼ã‚ºã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—ã‚µã‚¤ã‚ºãŒ1ã‚ˆã‚Šå¤§ãã„å ´åˆ\n",
    "                if self.ulysses_sequence_parallel_size > 1:\n",
    "\n",
    "                    # å…¥åŠ›IDã¨ä½ç½®IDã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦ã‚¹ãƒ©ã‚¤ã‚¹\n",
    "                    input_ids_rmpad, position_ids_rmpad, pad_size = \\\n",
    "                        ulysses_pad_and_slice_inputs(\n",
    "                            input_ids_rmpad, \n",
    "                            sp_size=self.ulysses_sequence_parallel_size\n",
    "                        )\n",
    "\n",
    "                # Flash Attention Varlenã‚’ä½¿ç”¨ã—ã¦å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’é †ä¼æ¬\n",
    "                output = self.reward_module(\n",
    "                    input_ids=input_ids_rmpad,\n",
    "                    attention_mask=None,\n",
    "                    position_ids=position_ids_rmpad,\n",
    "                    use_cache=False\n",
    "                )\n",
    "\n",
    "                # å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ãƒ­ã‚¸ãƒƒãƒˆã‚’å–å¾—\n",
    "                reward_rmpad = output.logits\n",
    "\n",
    "                # ä½™åˆ†ãªæ¬¡å…ƒã‚’å‰Šé™¤\n",
    "                # (total_nnz)\n",
    "                reward_rmpad = reward_rmpad.squeeze(0)\n",
    "\n",
    "                # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—ã‚µã‚¤ã‚ºãŒ1ã‚ˆã‚Šå¤§ãã„å ´åˆ\n",
    "                if self.ulysses_sequence_parallel_size > 1:\n",
    "\n",
    "                    # å‡ºåŠ›ã‚’é›†ç´„\n",
    "                    reward_rmpad = gather_outpus_and_unpad(\n",
    "                        reward_rmpad,\n",
    "                        gather_dim=0,\n",
    "                        unpad_dim=0,\n",
    "                        padding_size=pad_size\n",
    "                    )\n",
    "\n",
    "                # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’å…ƒã«æˆ»ã—ã¦å ±é…¬ã‚¹ã‚³ã‚¢ã‚’å–å¾—\n",
    "                rm_score = pad_input(\n",
    "                    reward_rmpad,\n",
    "                    indices=indices,\n",
    "                    batch=batch_size,\n",
    "                    seqlen=seqlen\n",
    "                ).squeeze(-1)\n",
    "\n",
    "            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã‚’ä½¿ç”¨ã—ãªã„å ´åˆ\n",
    "            else:\n",
    "\n",
    "                # Flash Attentionã‚’ä½¿ç”¨ã—ã¦å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’é †ä¼æ¬\n",
    "                output = self.reward_module(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids\n",
    "                )\n",
    "\n",
    "                # å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ãƒ­ã‚¸ãƒƒãƒˆã‚’å–å¾—\n",
    "                # (batch_size, seq_len, 1)\n",
    "                rm_score = output.logits\n",
    "\n",
    "                # ä½™åˆ†ãªæ¬¡å…ƒã‚’å‰Šé™¤\n",
    "                rm_score = rm_score.squeeze(-1)\n",
    "\n",
    "            # æœ€å¾Œã®æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã‚’å–å¾—\n",
    "            # (bsz,)\n",
    "            eos_mask_idx = torch.argmax(position_ids * attention_mask, dim=-1)\n",
    "\n",
    "            # ãƒãƒƒãƒã”ã¨ã«æœ€å¾Œã®æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³ã®å ±é…¬ã‚¹ã‚³ã‚¢ã‚’å–å¾—\n",
    "            rm_score = rm_score[torch.arange(batch_size), eos_mask_idx]\n",
    "\n",
    "            return rm_score\n",
    "\n",
    "    def _expand_to_token_level(self, data: DataProto, scores: torch.Tensor):\n",
    "        \"\"\"\n",
    "        ã‚¹ã‚³ã‚¢ã‚’ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã«æ‹¡å¼µã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            data (DataProto): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿\n",
    "            scores (torch.Tensor): å ±é…¬ã‚¹ã‚³ã‚¢ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        Returns:\n",
    "            token_level_scores (torch.Tensor): ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®å ±é…¬ã‚¹ã‚³ã‚¢ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "        batch_size = data.batch.batch_size[0]\n",
    "\n",
    "        # expand as token_level_reward\n",
    "\n",
    "        # ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "        attention_mask = data.batch['attention_mask']\n",
    "\n",
    "        # ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã®ä½ç½®IDã‚’å–å¾—\n",
    "        position_ids = data.batch['position_ids']\n",
    "\n",
    "        # å¿œç­”ã®é•·ã•ã‚’å–å¾—\n",
    "        response_length = data.batch['responses'].shape[-1]\n",
    "\n",
    "        # æœ€å¾Œã®æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã‚’å–å¾—\n",
    "        # (bsz,)\n",
    "        eos_mask_idx = torch.argmax(position_ids * attention_mask, dim=-1)\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®ã‚¹ã‚³ã‚¢ã‚’åˆæœŸåŒ–\n",
    "        # (bsz, seqlen)\n",
    "        token_level_scores = torch.zeros_like(attention_mask, dtype=scores.dtype)\n",
    "\n",
    "        # ãƒãƒƒãƒã”ã¨ã«æœ€å¾Œã®æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¹ã‚³ã‚¢ã‚’è¨­å®š\n",
    "        token_level_scores[torch.arange(batch_size), eos_mask_idx] = scores\n",
    "\n",
    "        # å¿œç­”éƒ¨åˆ†ã«ã‚¹ã‚³ã‚¢ã‚’æ‹¡å¼µ \n",
    "        token_level_scores = token_level_scores[:, -response_length:]\n",
    "\n",
    "        return token_level_scores\n",
    "\n",
    "    def _switch_chat_template(self, data: DataProto):\n",
    "        \"\"\"\n",
    "        ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹\n",
    "\n",
    "        Args:\n",
    "            data (DataProto): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿\n",
    "        Returns:\n",
    "            rm_data (DataProto): ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒåˆ‡ã‚Šæ›¿ãˆã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿\n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã®æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’å–å¾—\n",
    "        src_max_length = data.batch['attention_mask'].shape[-1]\n",
    "\n",
    "        src_tokenizer = self.input_tokenizer\n",
    "        target_tokenizer = self.tokenizer\n",
    "\n",
    "        rm_input_ids = []\n",
    "        rm_attention_mask = []\n",
    "\n",
    "        for i in range(data.batch.batch_size[0]):\n",
    "            # extract raw prompt\n",
    "            chat: list = data.non_tensor_batch['raw_prompt'][i].tolist()\n",
    "\n",
    "            # extract response\n",
    "            response_ids = data.batch['responses'][i]\n",
    "            response_length = response_ids.shape[-1]\n",
    "            valid_response_length = data.batch['attention_mask'][i][-response_length:].sum()\n",
    "            valid_response_ids = response_ids[:valid_response_length]\n",
    "\n",
    "            # decode\n",
    "            response = src_tokenizer.decode(valid_response_ids)\n",
    "            # remove bos and eos\n",
    "            response = response.replace(src_tokenizer.eos_token, '')\n",
    "\n",
    "            chat.append({'role': 'assistant', 'content': response})\n",
    "\n",
    "            prompt_with_chat_template = target_tokenizer.apply_chat_template(\n",
    "                chat,\n",
    "                add_generation_prompt=False,\n",
    "                tokenize=False\n",
    "            )\n",
    "\n",
    "            if self.rank == 0 and i == 0:\n",
    "                # for debugging purpose\n",
    "                print(f'Switch template. chat: {prompt_with_chat_template}')\n",
    "\n",
    "            # the maximum length is actually determined by the reward model itself\n",
    "            max_length = self.config.get('max_length', src_max_length)\n",
    "            if max_length is None:\n",
    "                max_length = src_max_length\n",
    "            input_ids, attention_mask = verl_F.tokenize_and_postprocess_data(\n",
    "                prompt=prompt_with_chat_template,\n",
    "                tokenizer=target_tokenizer,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=target_tokenizer.pad_token_id,\n",
    "                left_pad=False,  # right padding\n",
    "                truncation=self.config.get('truncation', 'right'))  # truncate from the right\n",
    "\n",
    "            rm_input_ids.append(input_ids)\n",
    "            rm_attention_mask.append(attention_mask)\n",
    "\n",
    "        rm_input_ids = torch.cat(rm_input_ids, dim=0)\n",
    "        rm_attention_mask = torch.cat(rm_attention_mask, dim=0)\n",
    "\n",
    "        rm_position_ids = compute_position_id_with_mask(rm_attention_mask)\n",
    "\n",
    "        rm_inputs = {'input_ids': rm_input_ids, 'attention_mask': rm_attention_mask, 'position_ids': rm_position_ids}\n",
    "\n",
    "        return DataProto.from_dict(rm_inputs)\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\n",
    "    def compute_rm_score(self, data: DataProto):\n",
    "        data = data.to('cuda')\n",
    "        if self._do_switch_chat_template:\n",
    "            rm_data = self._switch_chat_template(data)\n",
    "\n",
    "        rm_data.batch = rm_data.batch.cuda()\n",
    "\n",
    "        # perform forward computation\n",
    "        with self.ulysses_sharding_manager:\n",
    "            rm_data = self.ulysses_sharding_manager.preprocess_data(data=rm_data)\n",
    "            data = self.ulysses_sharding_manager.preprocess_data(data=data)\n",
    "\n",
    "            use_dynamic_bsz = self.config.use_dynamic_bsz\n",
    "            if use_dynamic_bsz:\n",
    "                max_token_len = self.config.forward_max_token_len_per_gpu * self.ulysses_sequence_parallel_size\n",
    "                micro_batches, indices = rearrange_micro_batches(batch=rm_data.batch, max_token_len=max_token_len)\n",
    "            else:\n",
    "                micro_batches = rm_data.batch.split(self.config.micro_batch_size)\n",
    "            output = []\n",
    "            for micro_batch in micro_batches:\n",
    "                rm_score = self._forward_micro_batch(micro_batch)\n",
    "                output.append(rm_score)\n",
    "            scores = torch.cat(output, dim=0)  # (batch_size)\n",
    "\n",
    "            if use_dynamic_bsz:\n",
    "                indices = list(itertools.chain.from_iterable(indices))\n",
    "                assert len(indices) == scores.size(0), f\"{len(indices)} vs. {scores.size()}\"\n",
    "                revert_indices = torch.tensor(get_reverse_idx(indices), dtype=torch.long)\n",
    "                scores = scores[revert_indices]\n",
    "\n",
    "            token_level_scores = self._expand_to_token_level(data, scores)\n",
    "            # Note that this is only the scores, may not be the final rewards used to train RL\n",
    "            output = DataProto.from_dict(tensors={'rm_scores': token_level_scores})\n",
    "            output = self.ulysses_sharding_manager.postprocess_data(data=output)\n",
    "\n",
    "        output = output.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5aac21",
   "metadata": {},
   "source": [
    "### ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca972a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_string(length: int) -> str:\n",
    "    \"\"\"\n",
    "    æŒ‡å®šã•ã‚ŒãŸé•·ã•ã®ãƒ©ãƒ³ãƒ€ãƒ ãªè‹±æ•°å­—ã®æ–‡å­—åˆ—ã‚’ç”Ÿæˆã™ã‚‹\n",
    "    RayWokerGroupã§ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªIDã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨\n",
    "\n",
    "    Args:\n",
    "        length (int): ç”Ÿæˆã™ã‚‹æ–‡å­—åˆ—ã®é•·ã•\n",
    "    Returns:\n",
    "        str: ç”Ÿæˆã•ã‚ŒãŸãƒ©ãƒ³ãƒ€ãƒ ãªæ–‡å­—åˆ—\n",
    "    \"\"\"\n",
    "    letters_digits = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choice(letters_digits) for _ in range(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f52c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_generator(self, method_name, dispatch_fn, collect_fn, execute_fn, blocking):\n",
    "    \"\"\"\n",
    "    Rayã®ãƒªãƒ¢ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°\n",
    "    ãƒªãƒ¢ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€å¼•æ•°ã‚’åˆ†æ•£å‡¦ç†ç”¨ã«å¤‰æ›ã—ã€æŒ‡å®šã•ã‚ŒãŸãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œã—ã€çµæœã‚’é›†ç´„ã™ã‚‹\n",
    "    è¤‡é›‘ãªåˆ†æ•£å‡¦ç†ã‚’æŠ½è±¡åŒ–\n",
    "\n",
    "    Args:\n",
    "        self: ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n",
    "        method_name (str): å‘¼ã³å‡ºã™ãƒ¡ã‚½ãƒƒãƒ‰ã®åå‰\n",
    "        dispatch_fn (callable): å¼•æ•°ã‚’åˆ†æ•£å‡¦ç†ç”¨ã«å¤‰æ›ã™ã‚‹é–¢æ•°\n",
    "        collect_fn (callable): çµæœã‚’é›†ç´„ã™ã‚‹é–¢æ•°\n",
    "        execute_fn (callable): ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°\n",
    "        blocking (bool): ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°\n",
    "    Returns:\n",
    "        func (callable): ç”Ÿæˆã•ã‚ŒãŸãƒªãƒ¢ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰\n",
    "    \"\"\"\n",
    "\n",
    "    def func(*args, **kwargs):\n",
    "        # å¼•æ•°ã‚’åˆ†æ•£å‡¦ç†ç”¨ã«å¤‰æ›\n",
    "        args, kwargs = dispatch_fn(self, *args, **kwargs)\n",
    "\n",
    "        # æŒ‡å®šã•ã‚ŒãŸãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œ\n",
    "        output = execute_fn(method_name, *args, **kwargs)\n",
    "\n",
    "        # åŒæœŸå®Ÿè¡Œã®å ´åˆ\n",
    "        if blocking:\n",
    "            # çµæœã‚’å–å¾—\n",
    "            output = ray.get(output)\n",
    "\n",
    "        # çµæœã‚’é›†ç´„\n",
    "        output = collect_fn(self, output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b64280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayClassWithInitArgs(ClassWithInitArgs):\n",
    "    \"\"\"\n",
    "    Rayã®ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼\n",
    "    RayWorkerGroupã§ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cls, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        RayClassWithInitArgsã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "        å®Ÿéš›ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ç”Ÿæˆã¯__call__ãƒ¡ã‚½ãƒƒãƒ‰ã§è¡Œã†\n",
    "\n",
    "        Args:\n",
    "            cls: ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ã‚¹\n",
    "            *args: ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–å¼•æ•°\n",
    "            **kwargs: ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"RayClassWithInitArgsã‚’åˆæœŸåŒ– {cls=} {args=} {kwargs=}\")\n",
    "\n",
    "        super().__init__(cls, *args, **kwargs)\n",
    "\n",
    "        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–\n",
    "        self._options = {}\n",
    "\n",
    "        # è¿½åŠ ãƒªã‚½ãƒ¼ã‚¹ã®åˆæœŸåŒ–\n",
    "        self._additional_resource = {}\n",
    "\n",
    "    def set_additional_resource(self, additional_resource):\n",
    "        self._additional_resource = additional_resource\n",
    "\n",
    "    def update_options(self, options: Dict):\n",
    "        self._options.update(options)\n",
    "\n",
    "    def __call__(self, placement_group, placement_group_bundle_idx, use_gpu: bool = True, num_gpus=1, sharing_with=None) -> Any:\n",
    "        \"\"\"\n",
    "        Rayã®ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆã‚¢ã‚¯ã‚¿ãƒ¼ï¼‰ã‚’ç”Ÿæˆã™ã‚‹\n",
    "        sharing_withãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€å…±æœ‰å…ˆã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã¨åŒã˜ãƒãƒ¼ãƒ‰ã«ã‚¢ã‚¯ã‚¿ãƒ¼ã‚’é…ç½®ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            placement_group: Rayã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—\n",
    "            placement_group_bundle_idx: ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒãƒ³ãƒ‰ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "            use_gpu (bool): GPUã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°\n",
    "            num_gpus (int): ä½¿ç”¨ã™ã‚‹GPUã®æ•°\n",
    "            sharing_with: å…±æœ‰å…ˆã®ãƒ¯ãƒ¼ã‚«ãƒ¼\n",
    "        Returns:\n",
    "            Any: ç”Ÿæˆã•ã‚ŒãŸRayã®ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚«ãƒ¼\n",
    "        \"\"\"\n",
    "        logger.info(f\"RayClassWithInitArgsã‚’å‘¼ã³å‡ºã— {placement_group=} {placement_group_bundle_idx=} {use_gpu=} {num_gpus=} {sharing_with=}\")\n",
    "\n",
    "        # å…±æœ‰å…ˆã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "        if sharing_with is not None:\n",
    "\n",
    "            # å…±æœ‰å…ˆã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ãƒãƒ¼ãƒ‰IDã‚’å–å¾—\n",
    "            target_node_id = ray.get(sharing_with.get_node_id.remote())\n",
    "\n",
    "            # å…±æœ‰å…ˆã®GPUè¨­å®šã‚’å–å¾—\n",
    "            cuda_visible_devices = ray.get(\n",
    "                sharing_with.get_cuda_visible_devices.remote()\n",
    "            )\n",
    "\n",
    "            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥ã‚’ä½œæˆ\n",
    "            # å¼·åˆ¶çš„ã«å…±æœ‰å…ˆã®ãƒãƒ¼ãƒ‰ã«ã‚¢ã‚¯ã‚¿ãƒ¼ã‚’é…ç½®\n",
    "            options = {\n",
    "                \"scheduling_strategy\": \\\n",
    "                    NodeAffinitySchedulingStrategy(\n",
    "                        node_id=target_node_id,\n",
    "                        soft=False\n",
    "                    )\n",
    "            }\n",
    "\n",
    "            # ã‚¢ã‚¯ã‚¿ãƒ¼ã‚’èµ·å‹•\n",
    "            return self.cls.options(**options).remote(\n",
    "                *self.args,\n",
    "                cuda_visible_devices=cuda_visible_devices,\n",
    "                **self.kwargs\n",
    "            )\n",
    "\n",
    "        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥ã‚’ä½œæˆ\n",
    "        options = {\n",
    "            \"scheduling_strategy\":\n",
    "                PlacementGroupSchedulingStrategy(\n",
    "                    placement_group=placement_group,\n",
    "                    placement_group_bundle_index=placement_group_bundle_idx\n",
    "                )\n",
    "        }\n",
    "\n",
    "        # è¿½åŠ ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æ›´æ–°\n",
    "        options.update(self._options)\n",
    "\n",
    "        # GPUã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€GPUæ•°ã‚’è¨­å®š\n",
    "        if use_gpu:\n",
    "            options[\"num_gpus\"] = num_gpus\n",
    "\n",
    "        # è¿½åŠ ãƒªã‚½ãƒ¼ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«è¿½åŠ \n",
    "        if len(self._additional_resource) > 1:\n",
    "            for k, v in self._additional_resource.items():\n",
    "                options[k] = v\n",
    "\n",
    "        # ã‚¢ã‚¯ã‚¿ãƒ¼ã‚’èµ·å‹•\n",
    "        return self.cls.options(**options).remote(*self.args, **self.kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c8cce",
   "metadata": {},
   "source": [
    "#### RayResourcePool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayResourcePool(ResourcePool):\n",
    "    \"\"\"\n",
    "    Rayã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç®¡ç†ã™ã‚‹ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã‚¯ãƒ©ã‚¹\n",
    "    åˆ†æ•£å­¦ç¿’ã§ã€å¿…è¦ãªè¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’äº‹å‰ã«ç¢ºä¿ã—ã€ç®¡ç†ã™ã‚‹ãŸã‚ã®ã‚¯ãƒ©ã‚¹\n",
    "    RayWorkerGroupã§ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, process_on_nodes: List[int] = None, use_gpu: bool = True, name_prefix: str = \"\", max_colocate_count: int = 5, detached=False) -> None:\n",
    "        \"\"\"\n",
    "        RayResourcePoolã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            process_on_nodes (List[int], optional): å„ãƒãƒ¼ãƒ‰ã§ã®ãƒ—ãƒ­ã‚»ã‚¹æ•°ã®ãƒªã‚¹ãƒˆ\n",
    "            use_gpu (bool, optional): GPUã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°\n",
    "            name_prefix (str, optional): ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—åã®æ¥é ­è¾\n",
    "            max_colocate_count (int, optional): å„ãƒ—ãƒ­ã‚»ã‚¹ã§ã‚³ãƒ­ã‚±ãƒ¼ãƒˆã§ãã‚‹æœ€å¤§æ•°\n",
    "            detached (bool, optional):\n",
    "                ãƒ‡ã‚¿ãƒƒãƒãƒ‰ãƒ¢ãƒ¼ãƒ‰ã§ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆã™ã‚‹ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"RayResourcePoolã‚’åˆæœŸåŒ– {process_on_nodes=} {name_prefix=} {max_colocate_count=}\")\n",
    "\n",
    "        super().__init__(process_on_nodes, max_colocate_count)\n",
    "\n",
    "        # GPUä½¿ç”¨ãƒ•ãƒ©ã‚°ã‚’å±æ€§ã«ä¿å­˜\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—åã®æ¥é ­è¾ã‚’å±æ€§ã«ä¿å­˜\n",
    "        self.name_prefix = name_prefix\n",
    "\n",
    "        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã®åˆæœŸåŒ–\n",
    "        self.pgs = None\n",
    "\n",
    "        # ãƒ‡ã‚¿ãƒƒãƒãƒ‰ãƒ¢ãƒ¼ãƒ‰ãƒ•ãƒ©ã‚°ã‚’å±æ€§ã«ä¿å­˜\n",
    "        self.detached = detached\n",
    "\n",
    "    def get_placement_groups(self, strategy=\"STRICT_PACK\", name=None):\n",
    "        \"\"\"\n",
    "        ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã‹ã‚‰Rayã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã‚’å–å¾—ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            strategy (str, optional): ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã®é…ç½®æˆ¦ç•¥\n",
    "            name (str, optional): ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—åã®æ¥é ­è¾\n",
    "        Returns:\n",
    "            List[placement_group]: Rayã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒªã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "\n",
    "        # æ—¢ã«ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ãŒä½œæˆã•ã‚Œã¦ã„ã‚‹å ´åˆã€ãã‚Œã‚’è¿”ã™\n",
    "        if self.pgs is not None:\n",
    "            return self.pgs\n",
    "\n",
    "        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—åã®æ¥é ­è¾ã‚’æ±ºå®š\n",
    "        pg_name_prefix = name if name else \\\n",
    "            f\"{self.name_prefix}verl_group_{'_'.join([\n",
    "                str(count) for count in self._store\n",
    "            ])}:\"\n",
    "\n",
    "        print(f\"ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—åã®æ¥é ­è¾ {pg_name_prefix=}\")\n",
    "\n",
    "        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒãƒ³ãƒ‰ãƒ«ï¼ˆãƒªã‚½ãƒ¼ã‚¹ã®æœ€å°å˜ä½ï¼‰ã‚’ä½œæˆ\n",
    "        pg_scheme = [[{\n",
    "            \"CPU\": self.max_collocate_count,\n",
    "            \"GPU\": 1, # ãƒ¯ãƒ¼ã‚«ãƒ¼ã«ä»˜ã1ã¤ã®GPU\n",
    "        } if self.use_gpu else {\n",
    "            \"CPU\": self.max_collocate_count, \n",
    "        } for _ in range(process_count)] for process_count in self._store\n",
    "        ]\n",
    "\n",
    "        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã®å¯¿å‘½ã‚’è¨­å®š\n",
    "        lifetime = 'detached' if self.detached else None\n",
    "\n",
    "        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ\n",
    "        pgs = [\n",
    "            placement_group(\n",
    "                bundles=bundles,\n",
    "                strategy=strategy,\n",
    "                name=pg_name_prefix + str(idx), lifetime=lifetime\n",
    "            ) for idx, bundles in enumerate(pg_scheme)\n",
    "        ]\n",
    "\n",
    "        # å…¨ã¦ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ãŒæº–å‚™å®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ\n",
    "        ray.get([pg.ready() for pg in pgs])\n",
    "\n",
    "        # ä½œæˆã—ãŸãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã‚’å±æ€§ã«ä¿å­˜\n",
    "        self.pgs = pgs\n",
    "\n",
    "        return pgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6999f6",
   "metadata": {},
   "source": [
    "#### RayWorkerGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a27ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayWorkerGroup(WorkerGroup):\n",
    "    \"\"\"\n",
    "    è¤‡æ•°ã®åˆ†æ•£ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä¸€æ‹¬ç®¡ç†ã™ã‚‹ãŸã‚ã®ä¸­å¿ƒçš„ãªã‚¯ãƒ©ã‚¹\n",
    "\n",
    "    - RayResourcePoolã§ç¢ºä¿ã—ãŸãƒªã‚½ãƒ¼ã‚¹ä¸Šã«ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆRayã‚¢ã‚¯ã‚¿ãƒ¼ï¼‰ã‚’èµ·å‹•ã—ç®¡ç†\n",
    "    - åˆ†æ•£å®Ÿè¡Œã‚’æŠ½è±¡åŒ–ã—ã€è¤‡æ•°ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã«ä¸€æ–‰ã«ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œã•ã›ã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        resource_pool: RayResourcePool = None,\n",
    "        ray_cls_with_init: RayClassWithInitArgs = None,\n",
    "        bin_pack: bool = True,\n",
    "        name_prefix: str = None,\n",
    "        detached=False,\n",
    "        worker_names=None,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            resource_pool (RayResourcePool, optional):\n",
    "                ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’èµ·å‹•ã™ã‚‹ãŸã‚ã®ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«\n",
    "            ray_cls_with_init (RayClassWithInitArgs, optional):\n",
    "                ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ã‚¯ãƒ©ã‚¹ã¨åˆæœŸåŒ–å¼•æ•°ã‚’ä¿æŒã™ã‚‹ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼\n",
    "            bin_pack (bool, optional):\n",
    "                ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒãƒ³ãƒ‰ãƒ«ã‚’è©°ã‚è¾¼ã‚€ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°\n",
    "            name_prefix (str, optional):\n",
    "                ãƒ¯ãƒ¼ã‚«ãƒ¼åã®æ¥é ­è¾\n",
    "            detached (bool, optional):\n",
    "                ãƒ‡ã‚¿ãƒƒãƒãƒ‰ãƒ¢ãƒ¼ãƒ‰ã§ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’èµ·å‹•ã™ã‚‹ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°\n",
    "            worker_names (List[str], optional):\n",
    "                ãƒ‡ã‚¿ãƒƒãƒãƒ‰ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•ã—ãŸãƒ¯ãƒ¼ã‚«ãƒ¼ã®åå‰ã®ãƒªã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"RayWorkerGroupã‚’åˆæœŸåŒ– {name_prefix=} {detached=} {worker_names=}\")\n",
    "\n",
    "        super().__init__(resource_pool=resource_pool, **kwargs)\n",
    "\n",
    "        # RayClassWithInitArgsã‚’å±æ€§ã«ä¿å­˜\n",
    "        self.ray_cls_with_init = ray_cls_with_init\n",
    "\n",
    "        # ãƒ¯ãƒ¼ã‚«ãƒ¼åã®ãƒªã‚¹ãƒˆã‚’åˆæœŸåŒ–\n",
    "        self.name_prefix = get_random_string(length=6) if name_prefix is None else name_prefix\n",
    "\n",
    "        # \n",
    "        if worker_names is not None:\n",
    "            assert self._is_init_with_detached_workers\n",
    "            self._worker_names = worker_names\n",
    "\n",
    "        # \n",
    "        if self._is_init_with_detached_workers:\n",
    "            self._init_with_detached_workers(worker_names=worker_names)\n",
    "\n",
    "        else:\n",
    "            # æ–°è¦ã«ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’èµ·å‹•\n",
    "            self._init_with_resource_pool(\n",
    "                resource_pool=resource_pool,\n",
    "                ray_cls_with_init=ray_cls_with_init,\n",
    "                bin_pack=bin_pack,\n",
    "                detached=detached\n",
    "            )\n",
    "\n",
    "        if ray_cls_with_init is not None:\n",
    "            # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ã‚¹ãŒæŒã£ã¦ã„ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã‚°ãƒ«ãƒ¼ãƒ—ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦å‹•çš„ã«ç™»éŒ²ã™ã‚‹\n",
    "            self._bind_worker_method(self.ray_cls_with_init.cls, func_generator)\n",
    "\n",
    "    def _is_worker_alive(self, worker: ray.actor.ActorHandle):\n",
    "        worker_state_dict = get_actor(worker._actor_id.hex())\n",
    "        return worker_state_dict.get(\"state\", \"undefined\") == \"ALIVE\" \\\n",
    "            if worker_state_dict is not None else False\n",
    "\n",
    "    def _init_with_detached_workers(self, worker_names):\n",
    "        workers = [ray.get_actor(name=name) for name in worker_names]\n",
    "        self._workers = workers\n",
    "        self._world_size = len(worker_names)\n",
    "\n",
    "    def _init_with_resource_pool(self, resource_pool, ray_cls_with_init, bin_pack, detached):\n",
    "        use_gpu = resource_pool.use_gpu\n",
    "\n",
    "        strategy = \"PACK\"\n",
    "        if bin_pack:\n",
    "            strategy = \"STRICT_PACK\"\n",
    "        pgs = resource_pool.get_placement_groups(strategy=strategy)\n",
    "        world_size = resource_pool.world_size\n",
    "        self._world_size = world_size\n",
    "        # cia.add_kwarg(\"_world_size\", world_size)\n",
    "        num_gpus = 1 / resource_pool.max_collocate_count\n",
    "\n",
    "        rank = -1\n",
    "        for pg_idx, local_world_size in enumerate(resource_pool.store):\n",
    "            pg = pgs[pg_idx]\n",
    "            assert local_world_size <= pg.bundle_count, \\\n",
    "                f\"when generating for {self.name_prefix}, for the \"\n",
    "            for local_rank in range(local_world_size):\n",
    "                rank += 1\n",
    "\n",
    "                # we pass in environment variable at option so that Worker can use environment variable to set\n",
    "                env_vars = {\n",
    "                    'WORLD_SIZE': str(world_size),\n",
    "                    'RANK': str(rank),\n",
    "                    'WG_PREFIX': self.name_prefix,\n",
    "                    'WG_BACKEND': 'ray',\n",
    "                    'RAY_LOCAL_WORLD_SIZE': str(local_world_size),\n",
    "                    'RAY_LOCAL_RANK': str(local_rank),\n",
    "                }\n",
    "                if rank != 0:\n",
    "                    env_vars['MASTER_ADDR'] = self._master_addr\n",
    "                    env_vars['MASTER_PORT'] = self._master_port\n",
    "\n",
    "                cia_name = type(ray_cls_with_init.cls).__name__\n",
    "                match = re.search(r\"ActorClass\\(([^)]+)\\)\", cia_name)  # ray.remote(Obj) -> \"ActorClass(Obj)\"\n",
    "                cia_name = match.group(1) if match else cia_name  # \"ActorClass(Obj)\" -> \"Obj\"\n",
    "                name = f\"{self.name_prefix}{cia_name}_{pg_idx}:{local_rank}\"  # e.g. Worker_2:5\n",
    "\n",
    "                ray_cls_with_init.update_options({'runtime_env': {'env_vars': env_vars}, 'name': name})\n",
    "\n",
    "                if detached:\n",
    "                    ray_cls_with_init.update_options({'lifetime': 'detached'})\n",
    "\n",
    "                # create a worker\n",
    "                worker = ray_cls_with_init(placement_group=pg,\n",
    "                                           placement_group_bundle_idx=local_rank,\n",
    "                                           use_gpu=use_gpu,\n",
    "                                           num_gpus=num_gpus)\n",
    "                self._workers.append(worker)\n",
    "                self._worker_names.append(name)\n",
    "\n",
    "                if rank == 0:\n",
    "                    register_center_actor = None\n",
    "                    for _ in range(120):\n",
    "                        if f\"{self.name_prefix}_register_center\" not in list_named_actors():\n",
    "                            time.sleep(1)\n",
    "                        else:\n",
    "                            register_center_actor = ray.get_actor(f\"{self.name_prefix}_register_center\")\n",
    "                            break\n",
    "                    assert register_center_actor is not None, f\"failed to get register_center_actor: {self.name_prefix}_register_center in {list_named_actors(all_namespaces=True)}\"\n",
    "                    rank_zero_info = ray.get(register_center_actor.get_rank_zero_info.remote())\n",
    "                    self._master_addr, self._master_port = rank_zero_info['MASTER_ADDR'], rank_zero_info['MASTER_PORT']\n",
    "                    # print(f\"rank_zero_info: {rank_zero_info}\")\n",
    "                    # print(f\"master_addr: {self._master_addr}, master_port: {self._master_port}\")\n",
    "\n",
    "    @property\n",
    "    def worker_names(self):\n",
    "        return self._worker_names\n",
    "\n",
    "    @classmethod\n",
    "    def from_detached(cls, worker_names=None, ray_cls_with_init=None):\n",
    "        worker_group = cls(resource_pool=None,\n",
    "                           ray_cls_with_init=ray_cls_with_init,\n",
    "                           name_prefix=None,\n",
    "                           worker_names=worker_names)\n",
    "        return worker_group\n",
    "\n",
    "    def spawn(self, prefix_set):\n",
    "        \"\"\"\n",
    "        ç‰©ç†çš„ã«ã¯åŒã˜ãƒ¯ãƒ¼ã‚«ãƒ¼ç¾¤ã‹ã‚‰ã€è«–ç†çš„ã«ç•°ãªã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼ç¾¤ã‚’ç”Ÿæˆã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            prefix_set (Set[str]): ãƒ¯ãƒ¼ã‚«ãƒ¼åã®æ¥é ­è¾ã®ã‚»ãƒƒãƒˆ\n",
    "        Returns:\n",
    "            Dict[str, RayWorkerGroup]: æ¥é ­è¾ã”ã¨ã«åˆ†ã‘ã‚‰ã‚ŒãŸRayWorkerGroupã®è¾æ›¸\n",
    "        \"\"\"\n",
    "\n",
    "        def _rebind_actor_methods(worker_group, actor_name):\n",
    "            \"\"\"\n",
    "            ãƒ¡ã‚½ãƒƒãƒ‰åã®æ¥é ­è¾ã‚’å–ã‚Šé™¤ãã€å…ƒã®åå‰ã«ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å†ãƒã‚¤ãƒ³ãƒ‰ã™ã‚‹\n",
    "            ãŸã¨ãˆã°ã€\"actor_update_policy\"ã‚’\"update_policy\"ã«å†ãƒã‚¤ãƒ³ãƒ‰ã™ã‚‹ãªã©\n",
    "\n",
    "            Args:\n",
    "                worker_group (RayWorkerGroup): ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—\n",
    "                actor_name (str): ãƒ¯ãƒ¼ã‚«ãƒ¼åã®æ¥é ­è¾\n",
    "            \"\"\"\n",
    "\n",
    "            # æ¥é ­è¾ã‚’å®šç¾©\n",
    "            prefix: str = actor_name + '_'\n",
    "\n",
    "            # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã®å…¨ã¦ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’èª¿ã¹ã‚‹\n",
    "            for method_name in dir(worker_group):\n",
    "\n",
    "                # ãƒ¡ã‚½ãƒƒãƒ‰åãŒæ¥é ­è¾ã§å§‹ã¾ã‚‹å ´åˆ\n",
    "                if method_name.startswith(prefix):\n",
    "\n",
    "                    # æ¥é ­è¾ã‚’å–ã‚Šé™¤ã„ãŸå…ƒã®ãƒ¡ã‚½ãƒƒãƒ‰åã‚’å–å¾—ï¼ˆPython 3.9ä»¥é™ã§æœ‰åŠ¹ï¼‰\n",
    "                    # ä¾‹: actor_update_policy -> update_policy\n",
    "                    original_method_name = method_name.removeprefix(prefix)\n",
    "\n",
    "                    # ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å–å¾— \n",
    "                    method = getattr(worker_group, method_name)\n",
    "\n",
    "                    # å…ƒã®ãƒ¡ã‚½ãƒƒãƒ‰åã§å†ãƒã‚¤ãƒ³ãƒ‰\n",
    "                    setattr(worker_group, original_method_name, method)\n",
    "\n",
    "        new_worker_group_dict = {}\n",
    "\n",
    "        # æŒ‡å®šã•ã‚ŒãŸå½¹å‰²ã”ã¨ã«ãƒ«ãƒ¼ãƒ—ï¼ˆactor, refãªã©ï¼‰\n",
    "        for prefix in prefix_set:\n",
    "\n",
    "            # æ—¢ã«å­˜åœ¨ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼ã«æ¥ç¶šã™ã‚‹ã ã‘ã®è»½é‡ãªãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ\n",
    "            new_worker_group = self.from_detached(\n",
    "                worker_names=self._worker_names,\n",
    "                ray_cls_with_init=self.ray_cls_with_init\n",
    "            )\n",
    "\n",
    "            # ãƒ¡ã‚½ãƒƒãƒ‰ã®å†ãƒã‚¤ãƒ³ãƒ‰\n",
    "            _rebind_actor_methods(new_worker_group, prefix)\n",
    "\n",
    "            # æ–°ã—ã„ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è¾æ›¸ã«è¿½åŠ \n",
    "            new_worker_group_dict[prefix] = new_worker_group\n",
    "\n",
    "        return new_worker_group_dict\n",
    "\n",
    "    def execute_rank_zero_sync(self, method_name: str, *args, **kwargs):\n",
    "        return ray.get(self.execute_all_async(method_name, **args, **kwargs))\n",
    "\n",
    "    def execute_rank_zero_async(self, method_name: str, *args, **kwargs):\n",
    "        remote_call = getattr(self._workers[0], method_name)\n",
    "        return remote_call.remote(*args, **kwargs)\n",
    "\n",
    "    def execute_rank_zero(self, method_name: str, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        ãƒ©ãƒ³ã‚¯0ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã«å¯¾ã—ã¦ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            method_name (str): å‘¼ã³å‡ºã™ãƒ¡ã‚½ãƒƒãƒ‰ã®åå‰\n",
    "            *args: ãƒ¡ã‚½ãƒƒãƒ‰ã®å¼•æ•°\n",
    "            **kwargs: ãƒ¡ã‚½ãƒƒãƒ‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°\n",
    "        Returns:\n",
    "            Any: ãƒ©ãƒ³ã‚¯0ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‹ã‚‰ã®ãƒªãƒ¢ãƒ¼ãƒˆå‘¼ã³å‡ºã—ã®çµæœ\n",
    "        \"\"\"\n",
    "        return self.execute_rank_zero_async(method_name, *args, **kwargs)\n",
    "\n",
    "    def execute_all(self, method_name: str, *args, **kwargs):\n",
    "        return self.execute_all_async(method_name, *args, **kwargs)\n",
    "\n",
    "    def execute_all_sync(self, method_name: str, *args, **kwargs):\n",
    "        return ray.get(self.execute_all_async(method_name, *args, **kwargs))\n",
    "\n",
    "    def execute_all_async(self, method_name: str, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        å…¨ã¦ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã«å¯¾ã—ã¦éåŒæœŸã«ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            method_name (str): å‘¼ã³å‡ºã™ãƒ¡ã‚½ãƒƒãƒ‰ã®åå‰\n",
    "            *args: ãƒ¡ã‚½ãƒƒãƒ‰ã®å¼•æ•°\n",
    "            **kwargs: ãƒ¡ã‚½ãƒƒãƒ‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°\n",
    "        Returns:\n",
    "            List: å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã‹ã‚‰ã®ãƒªãƒ¢ãƒ¼ãƒˆå‘¼ã³å‡ºã—ã®çµæœã®ãƒªã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "        # è¿™é‡Œæˆ‘ä»¬å‡è®¾ï¼Œå¦‚æœ args å’Œ kwargs é‡Œé¢æ‰€æœ‰çš„å‚æ•°éƒ½æ˜¯ listï¼Œä¸”æ‰€æœ‰çš„ list é•¿åº¦éƒ½ä¸ len(self._workers) ä¸€è‡´çš„è¯ï¼Œæˆ‘ä»¬ä¼šæŠŠ\n",
    "        # list ä¸­çš„æ¯ä¸€ä¸ªåˆ†åˆ«å‘åˆ°å¯¹åº”çš„ worker ä¸Šå»\n",
    "        print(f\"execute_all_async: method {method_name}({args}, {kwargs})\")\n",
    "        length = len(self._workers)\n",
    "\n",
    "        if all(isinstance(arg, list) for arg in args) and \\\n",
    "            all(isinstance(kwarg, list) for kwarg in kwargs.values()):\n",
    "\n",
    "            if all(len(arg) == length for arg in args) and \\\n",
    "                all(len(kwarg) == length for kwarg in kwargs.values()):\n",
    "                # print(f\"splitting args and kwargs into {length} shards\")\n",
    "                result = []\n",
    "\n",
    "                for i in range(length):\n",
    "                    sliced_args = tuple(arg[i] for arg in args)\n",
    "                    sliced_kwargs = {k: v[i] for k, v in kwargs.items()}\n",
    "                    remote_call = getattr(self._workers[i], method_name)\n",
    "                    result.append(remote_call.remote(*sliced_args, **sliced_kwargs))\n",
    "                return result\n",
    "\n",
    "        # å…¨ã¦ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã«å¯¾ã—ã¦ãƒ¡ã‚½ãƒƒãƒ‰ã‚’éåŒæœŸã«å®Ÿè¡Œ\n",
    "        return [getattr(worker, method_name).remote(*args, **kwargs) for worker in self._workers]\n",
    "\n",
    "    @property\n",
    "    def master_address(self):\n",
    "        return self._master_addr\n",
    "\n",
    "    @property\n",
    "    def master_port(self):\n",
    "        return self._master_port\n",
    "\n",
    "    @property\n",
    "    def workers(self):\n",
    "        return self._workers\n",
    "\n",
    "    @property\n",
    "    def world_size(self):\n",
    "        return self._world_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9257035",
   "metadata": {},
   "source": [
    "### ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371941cc",
   "metadata": {},
   "source": [
    "#### ã‚«ã‚¹ã‚¿ãƒ å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df006ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "WorkerType = Type[Worker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadfe4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Role(Enum):\n",
    "    \"\"\"\n",
    "    ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å½¹å‰²ã‚’å®šç¾©ã™ã‚‹åˆ—æŒ™å‹\n",
    "    \"\"\"\n",
    "    Actor = 0\n",
    "    Rollout = 1\n",
    "    ActorRollout = 2\n",
    "    Critic = 3\n",
    "    RefPolicy = 4\n",
    "    RewardModel = 5\n",
    "    ActorRolloutRef = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c26f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mean(values, mask, axis=None):\n",
    "    \"\"\"\n",
    "    ãƒã‚¹ã‚¯ä»˜ãã®ãƒ†ãƒ³ã‚½ãƒ«ã®å¹³å‡ã‚’è¨ˆç®—ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        values (torch.Tensor): å€¤ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        mask (torch.Tensor): ãƒã‚¹ã‚¯ã®ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆ0ã¾ãŸã¯1ã®å€¤ã‚’æŒã¤ï¼‰\n",
    "        axis (int or tuple of int, optional): å¹³å‡ã‚’è¨ˆç®—ã™ã‚‹è»¸\n",
    "    Returns:\n",
    "        torch.Tensor: ãƒã‚¹ã‚¯ä»˜ãã®å¹³å‡å€¤ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "    \"\"\"\n",
    "    return (values * mask).sum(axis=axis) / mask.sum(axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def _timer(name: str, timing_raw: Dict[str, float]):\n",
    "    \"\"\"\n",
    "    å‡¦ç†æ™‚é–“ã‚’è¨ˆæ¸¬ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£\n",
    "\n",
    "    Args:\n",
    "        name (str): è¨ˆæ¸¬ã™ã‚‹å‡¦ç†ã®åå‰\n",
    "        timing_raw (Dict[str, float]): è¨ˆæ¸¬çµæœã‚’ä¿å­˜ã™ã‚‹è¾æ›¸\n",
    "    \"\"\"\n",
    "    with Timer(name=name, logger=None) as timer:\n",
    "        yield # å‘¼ã³å‡ºã—å…ƒã®withãƒ–ãƒ­ãƒƒã‚¯å†…ã®å‡¦ç†ãŒå®Ÿè¡Œã•ã‚Œã‚‹\n",
    "\n",
    "    timing_raw[name] = timer.last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca8f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.AdaptiveKLController, kl_penalty='kl'):\n",
    "    \"\"\"\n",
    "    KLãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’é©ç”¨ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®å ±é…¬ã‚’è¨ˆç®—ã™ã‚‹\n",
    "    PPOã§å…ƒã®ãƒãƒªã‚·ãƒ¼ã‹ã‚‰ã®ä¹–é›¢ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "\n",
    "    Args:\n",
    "        data (DataProto): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿\n",
    "        kl_ctrl (core_algos.AdaptiveKLController): KLã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼\n",
    "        kl_penalty (str): KLãƒšãƒŠãƒ«ãƒ†ã‚£ã®ã‚¿ã‚¤ãƒ—\n",
    "    Returns:\n",
    "        data (DataProto): KLãƒšãƒŠãƒ«ãƒ†ã‚£ãŒé©ç”¨ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿\n",
    "        metrics (Dict[str, float]): KLé–¢é€£ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    # ç”Ÿæˆã•ã‚ŒãŸå¿œç­”ã‚’å–å¾—\n",
    "    responses = data.batch['responses']\n",
    "\n",
    "    # å¿œç­”ã®é•·ã•ã‚’å–å¾—\n",
    "    response_length = responses.size(1)\n",
    "\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®ã‚¹ã‚³ã‚¢ã‚’å–å¾—\n",
    "    token_level_scores = data.batch['token_level_scores']\n",
    "\n",
    "    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "    batch_size = data.batch.batch_size[0]\n",
    "\n",
    "    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "    attention_mask = data.batch['attention_mask']\n",
    "\n",
    "    # å¿œç­”éƒ¨åˆ†ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "    response_mask = attention_mask[:, -response_length:]\n",
    "\n",
    "    # compute kl between ref_policy and current policy\n",
    "\n",
    "    # å‚ç…§ãƒ¢ãƒ‡ãƒ«ã®å¯¾æ•°ç¢ºç‡ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "    if 'ref_log_prob' in data.batch.keys():\n",
    "\n",
    "        # KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’è¨ˆç®—\n",
    "        kld = core_algos.kl_penalty(\n",
    "            data.batch['old_log_probs'],\n",
    "            data.batch['ref_log_prob'],\n",
    "            kl_penalty=kl_penalty\n",
    "        )  # (batch_size, response_length)\n",
    "\n",
    "        # ãƒã‚¹ã‚¯ã‚’é©ç”¨ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¯0ã«ã™ã‚‹ï¼‰\n",
    "        kld = kld * response_mask\n",
    "\n",
    "        # KLä¿‚æ•°ã‚’å–å¾—\n",
    "        beta = kl_ctrl.value\n",
    "\n",
    "    # å‚ç…§ãƒ¢ãƒ‡ãƒ«ã®å¯¾æ•°ç¢ºç‡ãŒå­˜åœ¨ã—ãªã„å ´åˆ\n",
    "    else:\n",
    "        # KLãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’é©ç”¨ã—ãªã„\n",
    "        beta = 0\n",
    "\n",
    "        # KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’ã‚¼ãƒ­ã«è¨­å®š\n",
    "        kld = torch.zeros_like(response_mask, dtype=torch.float32)\n",
    "\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®å ±é…¬ã‚’è¨ˆç®—\n",
    "    # æœ€çµ‚å ±é…¬ = å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚³ã‚¢ - ä¿‚æ•° * KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹\n",
    "    token_level_rewards = token_level_scores - beta * kld\n",
    "\n",
    "    # ãƒãƒƒãƒå…¨ä½“ã®å¹³å‡KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’è¨ˆç®—\n",
    "    current_kl = masked_mean(kld, mask=response_mask, axis=-1)\n",
    "    current_kl = torch.mean(current_kl, dim=0).item()\n",
    "\n",
    "    # KLã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’æ›´æ–°\n",
    "    # KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã«åŸºã¥ã„ã¦KLä¿‚æ•°betaã‚’èª¿æ•´\n",
    "    kl_ctrl.update(current_kl=current_kl, n_steps=batch_size)\n",
    "\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®å ±é…¬ã‚’ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜\n",
    "    data.batch['token_level_rewards'] = token_level_rewards\n",
    "\n",
    "    # KLé–¢é€£ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½œæˆ\n",
    "    metrics = {'critic/kl': current_kl, 'critic/kl_coeff': beta}\n",
    "\n",
    "    return data, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b356aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantage(data: DataProto, adv_estimator, gamma=1.0, lam=1.0, num_repeat=1):\n",
    "    \"\"\"\n",
    "    å¼·åŒ–å­¦ç¿’ã«ãŠã‘ã‚‹ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã¨ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆå‰²å¼•ç´¯ç©å ±é…¬ï¼‰ã‚’è¨ˆç®—ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        data (DataProto): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿\n",
    "        adv_estimator (str): ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸æ¨å®šå™¨ã®ã‚¿ã‚¤ãƒ—ï¼ˆ'gae'ã¾ãŸã¯'grpo'ï¼‰\n",
    "        gamma (float): å‰²å¼•ç‡\n",
    "        lam (float): GAEã®Î»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        num_repeat (int): ãƒ‡ãƒ¼ã‚¿ã®ç¹°ã‚Šè¿”ã—æ•°ï¼ˆæœªä½¿ç”¨ï¼‰\n",
    "    Returns:\n",
    "        data (DataProto): ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã¨ãƒªã‚¿ãƒ¼ãƒ³ãŒè¨ˆç®—ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿\n",
    "    \"\"\"\n",
    "\n",
    "    # GAEï¼ˆGeneralized Advantage Estimationï¼‰ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "    if adv_estimator == 'gae':\n",
    "\n",
    "        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãŒäºˆæ¸¬ã—ãŸçŠ¶æ…‹ä¾¡å€¤\n",
    "        values = data.batch['values']\n",
    "\n",
    "        # ç”Ÿæˆã•ã‚ŒãŸå¿œç­”ã‚’å–å¾—\n",
    "        responses = data.batch['responses']\n",
    "\n",
    "        # å¿œç­”ã®é•·ã•ã‚’å–å¾—\n",
    "        response_length = responses.size(-1)\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "        attention_mask = data.batch['attention_mask']\n",
    "\n",
    "        # å¿œç­”éƒ¨åˆ†ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "        response_mask = attention_mask[:, -response_length:]\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®å ±é…¬ã‚’å–å¾—\n",
    "        token_level_rewards = data.batch['token_level_rewards']\n",
    "\n",
    "        # ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã¨ãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—\n",
    "        advantages, returns = core_algos.compute_gae_advantage_return(\n",
    "            token_level_rewards=token_level_rewards, # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®å ±é…¬\n",
    "            values=values, # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®çŠ¶æ…‹ä¾¡å€¤äºˆæ¸¬\n",
    "            eos_mask=response_mask, # å¿œç­”éƒ¨åˆ†ã®ãƒã‚¹ã‚¯\n",
    "            gamma=gamma, # å‰²å¼•ç‡\n",
    "            lam=lam # GAEã®Î»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæœªæ¥ã®å ±é…¬ã®å½±éŸ¿åº¦ã‚’èª¿æ•´ï¼‰\n",
    "        )\n",
    "\n",
    "        # ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã‚’ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜\n",
    "        data.batch['advantages'] = advantages\n",
    "\n",
    "        # ãƒªã‚¿ãƒ¼ãƒ³ã‚’ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜\n",
    "        data.batch['returns'] = returns\n",
    "\n",
    "    # GRPOï¼ˆGeneralized Reward Policy Optimizationï¼‰ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "    elif adv_estimator == 'grpo':\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®å ±é…¬ã‚’å–å¾—\n",
    "        token_level_rewards = data.batch['token_level_rewards']\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ãŒã©ã®ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾å¿œã™ã‚‹ã‹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
    "        index = data.non_tensor_batch['uid']\n",
    "\n",
    "        # ç”Ÿæˆã•ã‚ŒãŸå¿œç­”ã‚’å–å¾—\n",
    "        responses = data.batch['responses']\n",
    "\n",
    "        # å¿œç­”ã®é•·ã•ã‚’å–å¾—\n",
    "        response_length = responses.size(-1)\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "        attention_mask = data.batch['attention_mask']\n",
    "\n",
    "        # å¿œç­”éƒ¨åˆ†ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "        response_mask = attention_mask[:, -response_length:]\n",
    "\n",
    "        # ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã¨ãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—\n",
    "        advantages, returns = core_algos.compute_grpo_outcome_advantage(\n",
    "            token_level_rewards=token_level_rewards,\n",
    "            eos_mask=response_mask,\n",
    "            index=index\n",
    "        )\n",
    "\n",
    "        # ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã‚’ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜\n",
    "        data.batch['advantages'] = advantages\n",
    "\n",
    "        # ãƒªã‚¿ãƒ¼ãƒ³ã‚’ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜\n",
    "        data.batch['returns'] = returns\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2401a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_metrics(metrics: dict):\n",
    "    \"\"\"\n",
    "    è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å¹³å‡åŒ–ã™ã‚‹\n",
    "    åˆ†æ•£å­¦ç¿’ã§ã¯ã€è¤‡æ•°ã®GPUã‹ã‚‰å­¦ç¿’çµæœãŒãƒªã‚¹ãƒˆå½¢å¼ã§è¿”ã•ã‚Œã‚‹ãŸã‚\n",
    "\n",
    "    Args:\n",
    "        metrics (dict): ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¾æ›¸\n",
    "    Returns:\n",
    "        dict: å¹³å‡åŒ–ã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¾æ›¸\n",
    "    \"\"\"\n",
    "\n",
    "    # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¾æ›¸ã®ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã§ãƒ«ãƒ¼ãƒ—\n",
    "    for key, val in metrics.items():\n",
    "\n",
    "        # å¹³å‡ã‚’è¨ˆç®—ã—ã¦æ›´æ–°\n",
    "        metrics[key] = np.mean(val)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c5af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_response_info(batch):\n",
    "    \"\"\"\n",
    "    ãƒãƒƒãƒã‹ã‚‰ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å›ç­”ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨ã€å›ç­”éƒ¨åˆ†ã®ãƒã‚¹ã‚¯ã‚’è¨ˆç®—ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        batch (DataProto): å…¥åŠ›ãƒãƒƒãƒ\n",
    "    Returns:\n",
    "        dict: å›ç­”ã®ãƒã‚¹ã‚¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ã€å›ç­”ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å«ã‚€è¾æ›¸\n",
    "    \"\"\"\n",
    "\n",
    "    # ãƒãƒƒãƒã‹ã‚‰å›ç­”ã®é•·ã•ã‚’å–å¾—\n",
    "    response_length = batch.batch['responses'].shape[-1]\n",
    "\n",
    "    # ãƒãƒƒãƒã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã®ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "    prompt_mask = batch.batch['attention_mask'][:, :-response_length]\n",
    "\n",
    "    # ãƒãƒƒãƒã‹ã‚‰å›ç­”éƒ¨åˆ†ã®ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "    response_mask = batch.batch['attention_mask'][:, -response_length:]\n",
    "\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—\n",
    "    prompt_length = prompt_mask.sum(-1).float()\n",
    "\n",
    "    # å›ç­”ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—\n",
    "    response_length = response_mask.sum(-1).float()  # (batch_size,)\n",
    "\n",
    "    # çµæœã‚’è¾æ›¸ã§è¿”ã™\n",
    "    return dict(\n",
    "        response_mask=response_mask, # å›ç­”éƒ¨åˆ†ã®ãƒã‚¹ã‚¯\n",
    "        prompt_length=prompt_length, # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "        response_length=response_length, # å›ç­”ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_data_metrics(batch, use_critic=True):\n",
    "    \"\"\"\n",
    "    ãƒãƒƒãƒã‹ã‚‰æ§˜ã€…ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã™ã‚‹\n",
    "    å­¦ç¿’é€²æ—ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã«ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "\n",
    "    Args:\n",
    "        batch (DataProto): å…¥åŠ›ãƒãƒƒãƒ\n",
    "        use_critic (bool): ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°\n",
    "    Returns:\n",
    "        dict: è¨ˆç®—ã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¾æ›¸\n",
    "    \"\"\"\n",
    "\n",
    "    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆå ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚³ã‚¢ï¼‰\n",
    "    sequence_score = batch.batch['token_level_scores'].sum(-1)\n",
    "\n",
    "    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã®å ±é…¬ã‚’è¨ˆç®—ï¼ˆå ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚³ã‚¢ã‹ã‚‰KLãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’å¼•ã„ãŸã‚‚ã®ï¼‰\n",
    "    sequence_reward = batch.batch['token_level_rewards'].sum(-1)\n",
    "\n",
    "    # ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã‚’å–å¾—\n",
    "    advantages = batch.batch['advantages']\n",
    "\n",
    "    # ãƒªã‚¿ãƒ¼ãƒ³ã‚’å–å¾—\n",
    "    returns = batch.batch['returns']\n",
    "\n",
    "    # ãƒãƒƒãƒã‹ã‚‰å›ç­”ã®é•·ã•ã‚’å–å¾—\n",
    "    max_response_length = batch.batch['responses'].shape[-1]\n",
    "\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã®ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "    prompt_mask = batch.batch['attention_mask'][:, :-max_response_length].bool()\n",
    "\n",
    "    # å›ç­”éƒ¨åˆ†ã®ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "    response_mask = batch.batch['attention_mask'][:, -max_response_length:].bool()\n",
    "\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã®æœ€å¤§é•·ã‚’å–å¾—\n",
    "    max_prompt_length = prompt_mask.size(-1)\n",
    "\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å›ç­”ã®æƒ…å ±ã‚’è¨ˆç®—\n",
    "    response_info = _compute_response_info(batch)\n",
    "\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å–å¾— \n",
    "    prompt_length = response_info['prompt_length']\n",
    "\n",
    "    # å›ç­”ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å–å¾—\n",
    "    response_length = response_info['response_length']\n",
    "\n",
    "    # ãƒã‚¹ã‚¯ã‚’é©ç”¨ã—ã¦æœ‰åŠ¹ãªã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã‚’å–å¾—\n",
    "    valid_adv = torch.masked_select(advantages, response_mask)\n",
    "\n",
    "    # ãƒã‚¹ã‚¯ã‚’é©ç”¨ã—ã¦æœ‰åŠ¹ãªãƒªã‚¿ãƒ¼ãƒ³ã‚’å–å¾—\n",
    "    valid_returns = torch.masked_select(returns, response_mask)\n",
    "\n",
    "    # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ä¾¡å€¤é–¢æ•°ã®åˆ†æ•£ã‚’è¨ˆç®—\n",
    "    if use_critic:\n",
    "        values = batch.batch['values']\n",
    "        valid_values = torch.masked_select(values, response_mask)\n",
    "        return_diff_var = torch.var(valid_returns - valid_values)\n",
    "        return_var = torch.var(valid_returns)\n",
    "\n",
    "    # å„ç¨®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã—ã¦è¾æ›¸ã«ã¾ã¨ã‚ã‚‹\n",
    "    metrics = {\n",
    "        # score\n",
    "        'critic/score/mean':\n",
    "            torch.mean(sequence_score).detach().item(),\n",
    "        'critic/score/max':\n",
    "            torch.max(sequence_score).detach().item(),\n",
    "        'critic/score/min':\n",
    "            torch.min(sequence_score).detach().item(),\n",
    "        # reward\n",
    "        'critic/rewards/mean':\n",
    "            torch.mean(sequence_reward).detach().item(),\n",
    "        'critic/rewards/max':\n",
    "            torch.max(sequence_reward).detach().item(),\n",
    "        'critic/rewards/min':\n",
    "            torch.min(sequence_reward).detach().item(),\n",
    "        # adv\n",
    "        'critic/advantages/mean':\n",
    "            torch.mean(valid_adv).detach().item(),\n",
    "        'critic/advantages/max':\n",
    "            torch.max(valid_adv).detach().item(),\n",
    "        'critic/advantages/min':\n",
    "            torch.min(valid_adv).detach().item(),\n",
    "        # returns\n",
    "        'critic/returns/mean':\n",
    "            torch.mean(valid_returns).detach().item(),\n",
    "        'critic/returns/max':\n",
    "            torch.max(valid_returns).detach().item(),\n",
    "        'critic/returns/min':\n",
    "            torch.min(valid_returns).detach().item(),\n",
    "        **({\n",
    "            # values\n",
    "            'critic/values/mean': torch.mean(valid_values).detach().item(),\n",
    "            'critic/values/max': torch.max(valid_values).detach().item(),\n",
    "            'critic/values/min': torch.min(valid_values).detach().item(),\n",
    "            # vf explained var\n",
    "            'critic/vf_explained_var': (1.0 - return_diff_var / (return_var + 1e-5)).detach().item(),\n",
    "        } if use_critic else {}),\n",
    "\n",
    "        # response length\n",
    "        'response_length/mean':\n",
    "            torch.mean(response_length).detach().item(),\n",
    "        'response_length/max':\n",
    "            torch.max(response_length).detach().item(),\n",
    "        'response_length/min':\n",
    "            torch.min(response_length).detach().item(),\n",
    "        'response_length/clip_ratio':\n",
    "            torch.mean(torch.eq(response_length, max_response_length).float()).detach().item(),\n",
    "        # prompt length\n",
    "        'prompt_length/mean':\n",
    "            torch.mean(prompt_length).detach().item(),\n",
    "        'prompt_length/max':\n",
    "            torch.max(prompt_length).detach().item(),\n",
    "        'prompt_length/min':\n",
    "            torch.min(prompt_length).detach().item(),\n",
    "        'prompt_length/clip_ratio':\n",
    "            torch.mean(torch.eq(prompt_length, max_prompt_length).float()).detach().item(),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7bac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_timing_metrics(batch, timing_raw):\n",
    "    \"\"\"\n",
    "    ãƒãƒƒãƒã‹ã‚‰å‡¦ç†ã«ã‹ã‹ã£ãŸæ™‚é–“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        batch (DataProto): å…¥åŠ›ãƒãƒƒãƒ\n",
    "        timing_raw (Dict[str, float]): ç”Ÿã®å‡¦ç†æ™‚é–“ãƒ‡ãƒ¼ã‚¿\n",
    "    Returns:\n",
    "        dict: è¨ˆç®—ã•ã‚ŒãŸæ™‚é–“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¾æ›¸\n",
    "    \"\"\"\n",
    "\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å›ç­”ã®æƒ…å ±ã‚’è¨ˆç®—\n",
    "    response_info = _compute_response_info(batch)\n",
    "\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—\n",
    "    num_prompt_tokens = torch.sum(response_info['prompt_length']).item()\n",
    "\n",
    "    # å›ç­”ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—\n",
    "    num_response_tokens = torch.sum(response_info['response_length']).item()\n",
    "\n",
    "    # å…¨ä½“ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—\n",
    "    num_overall_tokens = num_prompt_tokens + num_response_tokens\n",
    "\n",
    "    # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¾æ›¸ã«ã¾ã¨ã‚ã‚‹\n",
    "    num_tokens_of_section = {\n",
    "        'gen': num_response_tokens, # ç”Ÿæˆæ™‚ã¯å›ç­”éƒ¨åˆ†ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "        **{\n",
    "            name: num_overall_tokens for name in ['ref', 'values', 'adv', 'update_critic', 'update_actor']\n",
    "        }, # ãã®ä»–ã¯å…¨ä½“ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        **{\n",
    "            f'timing_s/{name}': value for name, value in timing_raw.items()\n",
    "        }, # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨ã®ç”Ÿã®æ™‚é–“ï¼ˆç§’ï¼‰\n",
    "        **{\n",
    "            f'timing_per_token_ms/{name}': timing_raw[name] * 1000 / num_tokens_of_section[name] for name in set(num_tokens_of_section.keys(\n",
    "            )) & set(timing_raw.keys())\n",
    "        }, # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®æ™‚é–“ï¼ˆãƒŸãƒªç§’ï¼‰\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d769f55",
   "metadata": {},
   "source": [
    "#### ResourcePoolManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c9dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ResourcePoolManager:\n",
    "    \"\"\"\n",
    "    ç‰©ç†çš„ãªGPUãƒªã‚½ãƒ¼ã‚¹ã¨è«–ç†çš„ãªå½¹å‰²ã®å¯¾å¿œé–¢ä¿‚ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    # ç‰©ç†çš„ãªãƒªã‚½ãƒ¼ã‚¹ã®å®šç¾©\n",
    "    # ä¾‹: {'actor_pool': [4, 4], 'critic_pool': [2, 2]}\n",
    "    # ã“ã®å ´åˆã€actor_poolã¯2ãƒãƒ¼ãƒ‰ã§æ›¸ããƒãƒ¼ãƒ‰ã«4ã¤ã®GPUã‚’æŒã¤\n",
    "    resource_pool_spec: dict[str, list[int]]\n",
    "\n",
    "    # å½¹å‰²ã¨ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«åã®å¯¾å¿œé–¢ä¿‚\n",
    "    # ä¾‹: {Role.Actor: 'actor_pool', Role.Critic: 'critic_pool'}\n",
    "    mapping: dict[Role, str]\n",
    "\n",
    "    # å®Ÿéš›ã«ä½œæˆã•ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã®è¾æ›¸\n",
    "    resource_pool_dict: dict[str, RayResourcePool] = field(default_factory=dict)\n",
    "\n",
    "    def create_resource_pool(self):\n",
    "        \"\"\"\n",
    "        ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆã™ã‚‹\n",
    "        \"\"\"\n",
    "\n",
    "        # resource_pool_specã«åŸºã¥ã„ã¦ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–\n",
    "        for resource_pool_name, process_on_nodes in self.resource_pool_spec.items():\n",
    "\n",
    "            # RayResourcePoolã‚’ä½œæˆ\n",
    "            resource_pool = RayResourcePool(\n",
    "                process_on_nodes=process_on_nodes, # ãƒãƒ¼ãƒ‰ã”ã¨ã®ãƒ—ãƒ­ã‚»ã‚¹æ•°\n",
    "                use_gpu=True, # GPUã‚’ä½¿ç”¨\n",
    "                max_colocate_count=1, # 1ã¤ã®ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã«ä»˜ã1ã¤ã®å·¨å¤§ãªåˆ†æ•£ãƒ¢ãƒ‡ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã‚’é…ç½®\n",
    "                name_prefix=resource_pool_name # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—åã®æ¥é ­è¾\n",
    "            )\n",
    "\n",
    "            # ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã‚’è¾æ›¸ã«ä¿å­˜\n",
    "            self.resource_pool_dict[resource_pool_name] = resource_pool\n",
    "\n",
    "    def get_resource_pool(self, role: Role) -> RayResourcePool:\n",
    "        \"\"\"\n",
    "        æŒ‡å®šã•ã‚ŒãŸå½¹å‰²ã«å¯¾å¿œã™ã‚‹ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã‚’å–å¾—ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            role (Role): ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å½¹å‰²\n",
    "        Returns:\n",
    "            RayResourcePool: æŒ‡å®šã•ã‚ŒãŸå½¹å‰²ã«å¯¾å¿œã™ã‚‹ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«\n",
    "        \"\"\"\n",
    "        return self.resource_pool_dict[self.mapping[role]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ec699",
   "metadata": {},
   "source": [
    "#### RayPPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8187a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayPPOTrainer(object):\n",
    "    \"\"\"\n",
    "    PPOãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, tokenizer, role_worker_mapping: dict[Role, WorkerType], resource_pool_manager: ResourcePoolManager, ray_worker_group_cls: RayWorkerGroup = RayWorkerGroup, reward_fn=None, val_reward_fn=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config: PPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®è¨­å®š\n",
    "            tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
    "            role_worker_mapping (dict[Role, WorkerType]): å½¹å‰²ã¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—\n",
    "            resource_pool_manager (ResourcePoolManager): ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼\n",
    "            ray_worker_group_cls (RayWorkerGroup): Rayãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚¯ãƒ©ã‚¹\n",
    "            reward_fn: å ±é…¬é–¢æ•°\n",
    "            val_reward_fn: æ¤œè¨¼ç”¨å ±é…¬é–¢æ•°\n",
    "        \"\"\"\n",
    "        print(f\"PPOãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’åˆæœŸåŒ–\")\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å±æ€§ã«ä¿å­˜\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # è¨­å®šã‚’å±æ€§ã«ä¿å­˜\n",
    "        self.config = config\n",
    "\n",
    "        # å ±é…¬é–¢æ•°ã‚’å±æ€§ã«ä¿å­˜\n",
    "        self.reward_fn = reward_fn\n",
    "\n",
    "        # æ¤œè¨¼ç”¨å ±é…¬é–¢æ•°ã‚’å±æ€§ã«ä¿å­˜\n",
    "        self.val_reward_fn = val_reward_fn\n",
    "\n",
    "        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¨ãƒ³ã‚¸ãƒ³ã®ä½¿ç”¨æœ‰ç„¡ã‚’è¨­å®šã‹ã‚‰å–å¾—\n",
    "        # Trueã®å ´åˆã€ActorRolloutãƒ¯ãƒ¼ã‚«ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹\n",
    "        self.hybrid_engine = config.actor_rollout_ref.hybrid_engine\n",
    "\n",
    "        assert self.hybrid_engine, 'Currently, only support hybrid engine'\n",
    "\n",
    "        if self.hybrid_engine:\n",
    "            assert Role.ActorRollout in role_worker_mapping, f'{role_worker_mapping.keys()=}'\n",
    "\n",
    "        # å½¹å‰²ã¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å±æ€§ã«ä¿å­˜\n",
    "        self.role_worker_mapping = role_worker_mapping\n",
    "\n",
    "        # ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’å±æ€§ã«ä¿å­˜\n",
    "        self.resource_pool_manager = resource_pool_manager\n",
    "\n",
    "        # å‚ç…§ãƒãƒªã‚·ãƒ¼ã®ä½¿ç”¨æœ‰ç„¡ã‚’è¨­å®š\n",
    "        self.use_reference_policy = Role.RefPolicy in role_worker_mapping\n",
    "\n",
    "        # å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨æœ‰ç„¡ã‚’è¨­å®š\n",
    "        self.use_rm = Role.RewardModel in role_worker_mapping\n",
    "\n",
    "        # Rayãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚¯ãƒ©ã‚¹ã‚’å±æ€§ã«ä¿å­˜\n",
    "        self.ray_worker_group_cls = ray_worker_group_cls\n",
    "\n",
    "        # å‚ç…§ãƒãƒªã‚·ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "        if self.use_reference_policy:\n",
    "\n",
    "            # å›ºå®šã®KLã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "            if config.algorithm.kl_ctrl.type == 'fixed':\n",
    "\n",
    "                # å›ºå®šã®KLã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’åˆæœŸåŒ–\n",
    "                self.kl_ctrl = core_algos.FixedKLController(\n",
    "                    kl_coef=config.algorithm.kl_ctrl.kl_coef\n",
    "                )\n",
    "\n",
    "            # é©å¿œçš„ãªKLã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "            elif config.algorithm.kl_ctrl.type == 'adaptive':\n",
    "\n",
    "                assert config.algorithm.kl_ctrl.horizon > 0, \\\n",
    "                    f'horizon must be larger than 0. Got {config.critic.kl_ctrl.horizon}'\n",
    "\n",
    "                # é©å¿œçš„ãªKLã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’åˆæœŸåŒ–\n",
    "                self.kl_ctrl = core_algos.AdaptiveKLController(\n",
    "                    init_kl_coef=config.algorithm.kl_ctrl.kl_coef,\n",
    "                    target_kl=config.algorithm.kl_ctrl.target_kl,\n",
    "                    horizon=config.algorithm.kl_ctrl.horizon\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        # å‚ç…§ãƒãƒªã‚·ãƒ¼ã‚’ä½¿ç”¨ã—ãªã„å ´åˆã€KLã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã®KLä¿‚æ•°ã‚’0ã«è¨­å®š\n",
    "        else:\n",
    "            self.kl_ctrl = core_algos.FixedKLController(kl_coef=0.)\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ\n",
    "        self._create_dataloader()\n",
    "\n",
    "    def _create_dataloader(self):\n",
    "        \"\"\"\n",
    "        è¨“ç·´ã¨æ¤œè¨¼ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆã™ã‚‹\n",
    "        \"\"\"\n",
    "\n",
    "        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆæœŸåŒ–\n",
    "        self.train_dataset = RLHFDataset(\n",
    "            parquet_files=self.config.data.train_files,\n",
    "            tokenizer=self.tokenizer,\n",
    "            prompt_key=self.config.data.prompt_key,\n",
    "            max_prompt_length=self.config.data.max_prompt_length,\n",
    "            filter_prompts=True,\n",
    "            return_raw_chat=self.config.data.get('return_raw_chat', False),\n",
    "            truncation='error'\n",
    "        )\n",
    "\n",
    "        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–\n",
    "        self.train_dataloader = DataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            batch_size=self.config.data.train_batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆæœŸåŒ– \n",
    "        self.val_dataset = RLHFDataset(\n",
    "            parquet_files=self.config.data.val_files,\n",
    "            tokenizer=self.tokenizer,\n",
    "            prompt_key=self.config.data.prompt_key,\n",
    "            max_prompt_length=self.config.data.max_prompt_length,\n",
    "            filter_prompts=True,\n",
    "            return_raw_chat=self.config.data.get('return_raw_chat', False),\n",
    "            truncation='error'\n",
    "        )\n",
    "\n",
    "        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–\n",
    "        self.val_dataloader = DataLoader(\n",
    "            dataset=self.val_dataset,\n",
    "            batch_size=len(self.val_dataset),\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "        assert len(self.train_dataloader) >= 1\n",
    "        assert len(self.val_dataloader) >= 1\n",
    "\n",
    "        print(f'è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒãƒƒãƒæ•°: {len(self.train_dataloader)}')\n",
    "        print(f'æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒãƒƒãƒæ•°: {len(self.val_dataloader)}')\n",
    "\n",
    "        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ç·è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è¨ˆç®—\n",
    "        # ã‚¨ãƒãƒƒã‚¯æ•° * ãƒãƒƒãƒæ•°\n",
    "        total_training_steps = len(self.train_dataloader) * self.config.trainer.total_epochs\n",
    "\n",
    "        # è¨­å®šã§ç·è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "        if self.config.trainer.total_training_steps is not None:\n",
    "            # è¨­å®šã®å€¤ã§ä¸Šæ›¸ã\n",
    "            total_training_steps = self.config.trainer.total_training_steps\n",
    "\n",
    "        # ç·è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’å±æ€§ã«ä¿å­˜\n",
    "        self.total_training_steps = total_training_steps\n",
    "\n",
    "        print(f'ç·è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°: {self.total_training_steps}')\n",
    "\n",
    "        # è¨­å®šã‚’æ§‹é€ åŒ–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "        OmegaConf.set_struct(self.config, True)\n",
    "\n",
    "        # è¨­å®šã‚’æ›´æ–°\n",
    "        with open_dict(self.config):\n",
    "\n",
    "            # ã‚¢ã‚¯ã‚¿ãƒ¼ã€ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã€å‚ç…§ãƒãƒªã‚·ãƒ¼ã®æœ€é©åŒ–é–¢æ•°ã®ç·è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è¨­å®š\n",
    "            self.config.actor_rollout_ref.actor.optim.total_training_steps = total_training_steps\n",
    "\n",
    "            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®æœ€é©åŒ–é–¢æ•°ã®ç·è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è¨­å®š\n",
    "            self.config.critic.optim.total_training_steps = total_training_steps\n",
    "\n",
    "    def _validate(self):\n",
    "        \"\"\"\n",
    "        æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹\n",
    "        ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®å ±é…¬ãƒ¢ãƒ‡ãƒ«ã§ã®ã¿è©•ä¾¡ã‚’è¡Œã†\n",
    "        \"\"\"\n",
    "\n",
    "        reward_tensor_lst = []\n",
    "\n",
    "        data_source_lst = []\n",
    "\n",
    "        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã§ãƒ«ãƒ¼ãƒ—\n",
    "        for test_data in self.val_dataloader:\n",
    "\n",
    "            # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’DataProtoã«å¤‰æ›\n",
    "            test_batch = DataProto.from_single_dict(test_data)\n",
    "\n",
    "            # å ±é…¬ãƒ¢ãƒ‡ãƒ«ãŒæœ‰åŠ¹ãªå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "            if self.config.reward_model.enable and \\\n",
    "                test_batch[0].non_tensor_batch['reward_model']['style'] == 'model':\n",
    "                return {}\n",
    "\n",
    "            # ç”Ÿæˆã®ãŸã‚ã«å¿…è¦ãªæƒ…å ±ã‚’åˆ†é›¢\n",
    "            test_gen_batch = test_batch.pop(['input_ids', 'attention_mask', 'position_ids'])\n",
    "\n",
    "            # ç”Ÿæˆè¨­å®šã‚’ãƒ¡ã‚¿æƒ…å ±ã«è¿½åŠ \n",
    "            test_gen_batch.meta_info = {\n",
    "                'eos_token_id': self.tokenizer.eos_token_id,\n",
    "                'pad_token_id': self.tokenizer.pad_token_id,\n",
    "                'recompute_log_prob': False,\n",
    "                'do_sample': False,\n",
    "                'validate': True,\n",
    "            }\n",
    "\n",
    "            # pad to be divisible by dp_size\n",
    "\n",
    "            # \n",
    "            test_gen_batch_padded, pad_size = pad_dataproto_to_divisor(\n",
    "                test_gen_batch,\n",
    "                self.actor_rollout_wg.world_size\n",
    "            )\n",
    "\n",
    "            test_output_gen_batch_padded = self.actor_rollout_wg.generate_sequences(\n",
    "                test_gen_batch_padded\n",
    "            )\n",
    "\n",
    "            # unpad\n",
    "            test_output_gen_batch = unpad_dataproto(\n",
    "                test_output_gen_batch_padded, pad_size=pad_size\n",
    "            )\n",
    "\n",
    "            print('validation generation end')\n",
    "\n",
    "            test_batch = test_batch.union(test_output_gen_batch)\n",
    "\n",
    "            # evaluate using reward_function\n",
    "            # for certain reward function (e.g. sandbox), the generation can overlap with reward\n",
    "            reward_tensor = self.val_reward_fn(test_batch)\n",
    "\n",
    "            reward_tensor_lst.append(reward_tensor)\n",
    "            data_source_lst.append(test_batch.non_tensor_batch.get('data_source', ['unknown'] * reward_tensor.shape[0]))\n",
    "\n",
    "        reward_tensor = torch.cat(reward_tensor_lst, dim=0).sum(-1).cpu()  # (batch_size,)\n",
    "        data_sources = np.concatenate(data_source_lst, axis=0)\n",
    "        # evaluate test_score based on data source\n",
    "        data_source_reward = {}\n",
    "        for i in range(reward_tensor.shape[0]):\n",
    "            data_source = data_sources[i]\n",
    "            if data_source not in data_source_reward:\n",
    "                data_source_reward[data_source] = []\n",
    "            data_source_reward[data_source].append(reward_tensor[i].item())\n",
    "\n",
    "        metric_dict = {}\n",
    "        for data_source, rewards in data_source_reward.items():\n",
    "            metric_dict[f'val/test_score/{data_source}'] = np.mean(rewards)\n",
    "\n",
    "        return metric_dict\n",
    "\n",
    "    def init_workers(self):\n",
    "        \"\"\"\n",
    "        Rayã‚’ä½¿ã£ã¦ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆã—ã€å„å½¹å‰²ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ï¼ˆGPUã®ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰ã®ä½œæˆ\n",
    "        self.resource_pool_manager.create_resource_pool()\n",
    "\n",
    "        # ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã”ã¨ã«ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ã‚¹ã‚’æ ¼ç´ã™ã‚‹è¾æ›¸ã‚’åˆæœŸåŒ–\n",
    "        self.resource_pool_to_cls = {\n",
    "            pool: {} for pool in self.resource_pool_manager.resource_pool_dict.values()\n",
    "        }\n",
    "\n",
    "        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "        # True\n",
    "        if self.hybrid_engine:\n",
    "\n",
    "            # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã®ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã‚’å–å¾—\n",
    "            resource_pool = self.resource_pool_manager.get_resource_pool(Role.ActorRollout)\n",
    "\n",
    "            # \n",
    "            actor_rollout_cls = RayClassWithInitArgs(\n",
    "                cls=self.role_worker_mapping[Role.ActorRollout],\n",
    "                config=self.config.actor_rollout_ref,\n",
    "                role='actor_rollout'\n",
    "            )\n",
    "\n",
    "            self.resource_pool_to_cls[resource_pool]['actor_rollout'] = actor_rollout_cls\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # GAEï¼ˆGeneralized Advantage Estimationï¼‰ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "        if self.config.algorithm.adv_estimator == 'gae':\n",
    "\n",
    "            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã‚’å–å¾—\n",
    "            resource_pool = self.resource_pool_manager.get_resource_pool(Role.Critic)\n",
    "\n",
    "            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ\n",
    "            critic_cls = RayClassWithInitArgs(\n",
    "                cls=self.role_worker_mapping[Role.Critic],\n",
    "                config=self.config.critic # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®è¨­å®š\n",
    "            )\n",
    "\n",
    "            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ã‚¹ã‚’ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã«ç™»éŒ²\n",
    "            self.resource_pool_to_cls[resource_pool]['critic'] = critic_cls\n",
    "\n",
    "            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ä½¿ç”¨ãƒ•ãƒ©ã‚°ã‚’æœ‰åŠ¹åŒ–\n",
    "            self.use_critic = True\n",
    "\n",
    "        # GRPOï¼ˆGeneralized Reward Policy Optimizationï¼‰ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "        elif self.config.algorithm.adv_estimator == 'grpo':\n",
    "\n",
    "            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ä½¿ç”¨ãƒ•ãƒ©ã‚°ã‚’ç„¡åŠ¹åŒ–\n",
    "            self.use_critic = False\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # å‚ç…§ãƒãƒªã‚·ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "        if self.use_reference_policy:\n",
    "\n",
    "            # å‚ç…§ãƒãƒªã‚·ãƒ¼ã®ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã‚’å–å¾—\n",
    "            resource_pool = self.resource_pool_manager.get_resource_pool(Role.RefPolicy)\n",
    "\n",
    "            # å‚ç…§ãƒãƒªã‚·ãƒ¼ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ\n",
    "            ref_policy_cls = RayClassWithInitArgs(\n",
    "                self.role_worker_mapping[Role.RefPolicy],\n",
    "                config=self.config.actor_rollout_ref, # å‚ç…§ãƒãƒªã‚·ãƒ¼ã¯ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã¨åŒã˜è¨­å®šã‚’ä½¿ç”¨\n",
    "                role='ref'\n",
    "            )\n",
    "\n",
    "            # å‚ç…§ãƒãƒªã‚·ãƒ¼ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ã‚¹ã‚’ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã«ç™»éŒ²\n",
    "            self.resource_pool_to_cls[resource_pool]['ref'] = ref_policy_cls\n",
    "\n",
    "        # å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "        if self.use_rm:\n",
    "\n",
    "            # å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã‚’å–å¾—\n",
    "            resource_pool = self.resource_pool_manager.get_resource_pool(Role.RewardModel)\n",
    "\n",
    "            # å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ\n",
    "            rm_cls = RayClassWithInitArgs(\n",
    "                self.role_worker_mapping[Role.RewardModel],\n",
    "                config=self.config.reward_model # å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š\n",
    "            )\n",
    "\n",
    "            # å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ã‚¹ã‚’ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã«ç™»éŒ²\n",
    "            self.resource_pool_to_cls[resource_pool]['rm'] = rm_cls\n",
    "\n",
    "\n",
    "        # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’åˆæœŸåŒ–\n",
    "        all_wg = {}\n",
    "\n",
    "        self.wg_dicts = []\n",
    "\n",
    "        # ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ã‚¹ã®è¾æ›¸ã§ãƒ«ãƒ¼ãƒ—\n",
    "        for resource_pool, class_dict in self.resource_pool_to_cls.items():\n",
    "\n",
    "            # è¤‡æ•°ã®å½¹å‰²ã‚’ã¾ã¨ã‚ãŸè¤‡åˆã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ\n",
    "            worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)\n",
    "\n",
    "            # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ\n",
    "            wg_dict = self.ray_worker_group_cls(\n",
    "                resource_pool=resource_pool,\n",
    "                ray_cls_with_init=worker_dict_cls\n",
    "            )\n",
    "\n",
    "            # Rayã‚’ä½¿ã£ã¦ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’èµ·å‹•\n",
    "            spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())\n",
    "\n",
    "            # ã™ã¹ã¦ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’1ã¤ã®è¾æ›¸ã«ã¾ã¨ã‚ã‚‹\n",
    "            all_wg.update(spawn_wg)\n",
    "\n",
    "            self.wg_dicts.append(wg_dict)\n",
    "\n",
    "        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "        if self.use_critic:\n",
    "\n",
    "            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å–å¾—\n",
    "            self.critic_wg = all_wg['critic']\n",
    "\n",
    "            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã‚’åˆæœŸåŒ–\n",
    "            self.critic_wg.init_model()\n",
    "\n",
    "        # å‚ç…§ãƒãƒªã‚·ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "        if self.use_reference_policy:\n",
    "\n",
    "            # å‚ç…§ãƒãƒªã‚·ãƒ¼ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å–å¾—\n",
    "            self.ref_policy_wg = all_wg['ref']\n",
    "\n",
    "            # å‚ç…§ãƒãƒªã‚·ãƒ¼ã‚’åˆæœŸåŒ–\n",
    "            self.ref_policy_wg.init_model()\n",
    "\n",
    "\n",
    "        # ãƒªãƒ¯ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "        if self.use_rm:\n",
    "\n",
    "            # ãƒªãƒ¯ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å–å¾—\n",
    "            self.rm_wg = all_wg['rm']\n",
    "\n",
    "            # ãƒªãƒ¯ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–\n",
    "            self.rm_wg.init_model()\n",
    "\n",
    "        # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å–å¾—\n",
    "        self.actor_rollout_wg = all_wg['actor_rollout']\n",
    "\n",
    "        # ã‚¢ã‚¯ã‚¿ãƒ¼ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’åˆæœŸåŒ–ï¼ˆvLLMã¯å¤§é‡ã«ãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚æœ€å¾Œã«åˆæœŸåŒ–ã™ã‚‹ï¼‰\n",
    "        self.actor_rollout_wg.init_model()\n",
    "\n",
    "    def _save_checkpoint(self):\n",
    "        \"\"\"\n",
    "        ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹\n",
    "        \"\"\"\n",
    "\n",
    "        # ã‚¢ã‚¯ã‚¿ãƒ¼ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "        actor_local_path = os.path.join(\n",
    "            self.config.trainer.default_local_dir,\n",
    "            'actor',\n",
    "            f'global_step_{self.global_steps}'\n",
    "        )\n",
    "\n",
    "        # HDFSã®è¨­å®šãŒã‚ã‚‹å ´åˆã¯ã€ãƒªãƒ¢ãƒ¼ãƒˆãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "        actor_remote_path = None if self.config.trainer.default_hdfs_dir is None \\\n",
    "            else os.path.join(self.config.trainer.default_hdfs_dir, 'actor')\n",
    "\n",
    "        # ã‚¢ã‚¯ã‚¿ãƒ¼ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜\n",
    "        self.actor_rollout_wg.save_checkpoint(actor_local_path, actor_remote_path)\n",
    "\n",
    "        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "        if self.use_critic:\n",
    "\n",
    "            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "            critic_local_path = os.path.join(\n",
    "                self.config.trainer.default_local_dir,\n",
    "                'critic',\n",
    "                f'global_step_{self.global_steps}'\n",
    "            )\n",
    "\n",
    "            # HDFSã®è¨­å®šãŒã‚ã‚‹å ´åˆã¯ã€ãƒªãƒ¢ãƒ¼ãƒˆãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "            critic_remote_path = None if self.config.trainer.default_hdfs_dir is None \\\n",
    "                else os.path.join(self.config.trainer.default_hdfs_dir, 'critic')\n",
    "\n",
    "            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜\n",
    "            self.critic_wg.save_checkpoint(critic_local_path, critic_remote_path)\n",
    "\n",
    "    def _balance_batch(self, batch: DataProto, metrics, logging_prefix='global_seqlen'):\n",
    "        \"\"\"\n",
    "        GPUã”ã¨ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå‡ç­‰ã«ãªã‚‹ã‚ˆã†ã«ã€ãƒ‡ãƒ¼ã‚¿ã®ä¸¦ã³é †ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦è² è·åˆ†æ•£ã‚’è¡Œã†\n",
    "\n",
    "        Args:\n",
    "            batch (DataProto): å…¥åŠ›ãƒãƒƒãƒ\n",
    "            metrics (dict): ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ ¼ç´ã™ã‚‹è¾æ›¸\n",
    "            logging_prefix (str): ãƒ­ã‚°ã«ä½¿ç”¨ã™ã‚‹ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹\n",
    "        Returns:\n",
    "            None: ãƒãƒƒãƒã¯ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹ã§ä¸¦ã³æ›¿ãˆã‚‰ã‚Œã‚‹\n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒãƒƒãƒã‹ã‚‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "        attention_mask = batch.batch['attention_mask']\n",
    "\n",
    "        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "        batch_size = attention_mask.shape[0]\n",
    "\n",
    "        # å„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—\n",
    "        # (train_batch_size,)\n",
    "        global_seqlen_lst = batch.batch['attention_mask'].view(batch_size, -1).sum(-1).tolist()\n",
    "\n",
    "        # ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºï¼ˆä½¿ç”¨ã—ã¦ã„ã‚‹GPUã®æ•°ï¼‰ã‚’å–å¾—\n",
    "        world_size = self.actor_rollout_wg.world_size\n",
    "\n",
    "        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«åŸºã¥ã„ã¦ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—\n",
    "        global_partition_lst = get_seqlen_balanced_partitions(\n",
    "            global_seqlen_lst,\n",
    "            k_partitions=world_size, # GPUã®æ•°\n",
    "            equal_size=True # å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®ã‚µã‚¤ã‚ºã‚’å‡ç­‰ã«ã™ã‚‹\n",
    "        )\n",
    "\n",
    "        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "        global_idx = torch.tensor([j for partition in global_partition_lst for j in partition])\n",
    "\n",
    "        # ãƒãƒƒãƒã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åŸºã¥ã„ã¦ä¸¦ã³æ›¿ãˆ\n",
    "        batch.reorder(global_idx)\n",
    "\n",
    "        # ä¸¦ã³æ›¿ãˆå¾Œã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ãƒªã‚¹ãƒˆã‚’å†è¨ˆç®—\n",
    "        global_balance_stats = log_seqlen_unbalance(\n",
    "            seqlen_list=global_seqlen_lst,\n",
    "            partitions=global_partition_lst,\n",
    "            prefix=logging_prefix\n",
    "        )\n",
    "\n",
    "        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è² è·åˆ†æ•£ã®çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ \n",
    "        metrics.update(global_balance_stats)\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        å­¦ç¿’ã®ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—\n",
    "        \"\"\"\n",
    "\n",
    "        logger = Tracking(\n",
    "            project_name=self.config.trainer.project_name,\n",
    "            experiment_name=self.config.trainer.experiment_name,\n",
    "            default_backend=self.config.trainer.logger,\n",
    "            config=OmegaConf.to_container(self.config, resolve=True)\n",
    "        )\n",
    "\n",
    "        self.global_steps = 0\n",
    "\n",
    "        # perform validation before training\n",
    "        # currently, we only support validation using the reward_function.\n",
    "        if self.val_reward_fn is not None and self.config.trainer.get('val_before_train', True):\n",
    "            val_metrics = self._validate()\n",
    "            pprint(f'Initial validation metrics: {val_metrics}')\n",
    "            logger.log(data=val_metrics, step=self.global_steps)\n",
    "            if self.config.trainer.get('val_only', False):\n",
    "                return\n",
    "\n",
    "        # we start from step 1\n",
    "        self.global_steps += 1\n",
    "\n",
    "        # ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ãƒ«ãƒ¼ãƒ—\n",
    "        for epoch in range(self.config.trainer.total_epochs):\n",
    "\n",
    "            # ãƒãƒƒãƒã”ã¨ã«ãƒ«ãƒ¼ãƒ—\n",
    "            for batch_dict in self.train_dataloader:\n",
    "\n",
    "                print(f'ã‚¨ãƒãƒƒã‚¯ {epoch}, ã‚¹ãƒ†ãƒƒãƒ— {self.global_steps}')\n",
    "\n",
    "                metrics = {}\n",
    "\n",
    "                timing_raw = {}\n",
    "\n",
    "                # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’DataProtoã«å¤‰æ›\n",
    "                batch: DataProto = DataProto.from_single_dict(batch_dict)\n",
    "\n",
    "                # ç”Ÿæˆã«å¿…è¦ãªæƒ…å ±ã‚’åˆ†é›¢\n",
    "                gen_batch = batch.pop(batch_keys=['input_ids', 'attention_mask', 'position_ids'])\n",
    "\n",
    "                # ã‚¹ãƒ†ãƒƒãƒ—ã®å‡¦ç†æ™‚é–“ã‚’è¨˜éŒ²\n",
    "                with _timer('step', timing_raw):\n",
    "\n",
    "                    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã®å‡¦ç†æ™‚é–“ã‚’è¨ˆæ¸¬\n",
    "                    with _timer('gen', timing_raw):\n",
    "\n",
    "                        # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆï¼‰\n",
    "                        gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)\n",
    "\n",
    "                    # å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1ã¤1ã¤ã«å¯¾ã—ã¦ä¸€æ„ãªUIDã‚’ç”Ÿæˆ\n",
    "                    batch.non_tensor_batch['uid'] = np.array(\n",
    "                        [str(uuid.uuid4()) for _ in range(len(batch.batch))],\n",
    "                        dtype=object\n",
    "                    )\n",
    "\n",
    "                    # ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã®æ•°ã ã‘ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¤‡è£½\n",
    "                    batch = batch.repeat(\n",
    "                        repeat_times=self.config.actor_rollout_ref.rollout.n,\n",
    "                        interleave=True\n",
    "                    )\n",
    "\n",
    "                    # è¤‡è£½ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ç”Ÿæˆçµæœã‚’çµåˆ\n",
    "                    batch = batch.union(gen_batch_output)\n",
    "\n",
    "                    # å„GPUãŒå‡¦ç†ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå‡ç­‰ã«ãªã‚‹ã‚ˆã†ã«ãƒãƒƒãƒã‚’ä¸¦ã³æ›¿ãˆã‚‹ï¼ˆè² è·åˆ†æ•£ï¼‰\n",
    "                    self._balance_batch(batch, metrics=metrics)\n",
    "\n",
    "                    # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—ã—ã¦ãƒ¡ã‚¿æƒ…å ±ã«ä¿å­˜\n",
    "                    batch.meta_info['global_token_num'] = torch.sum(\n",
    "                        batch.batch['attention_mask'], dim=-1\n",
    "                    ).tolist()\n",
    "\n",
    "                    # å‚ç…§ãƒãƒªã‚·ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "                    if self.use_reference_policy:\n",
    "\n",
    "                        # å‚ç…§ãƒãƒªã‚·ãƒ¼ã®å‡¦ç†æ™‚é–“ã‚’è¨ˆæ¸¬\n",
    "                        with _timer('ref', timing_raw):\n",
    "\n",
    "                            # å‚ç…§ãƒãƒªã‚·ãƒ¼ã®å¯¾æ•°ç¢ºç‡ã‚’è¨ˆç®—\n",
    "                            ref_log_prob = self.ref_policy_wg.compute_ref_log_prob(batch)\n",
    "\n",
    "                            batch = batch.union(ref_log_prob)\n",
    "\n",
    "                    # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "                    if self.use_critic:\n",
    "\n",
    "                        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®å‡¦ç†æ™‚é–“ã‚’è¨ˆæ¸¬\n",
    "                        with _timer('values', timing_raw):\n",
    "\n",
    "                            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã«ç¾åœ¨ã®çŠ¶æ…‹ã®ä¾¡å€¤ã‚’äºˆæ¸¬ã•ã›ã‚‹ï¼ˆGAEç”¨ï¼‰\n",
    "                            values = self.critic_wg.compute_values(batch)\n",
    "\n",
    "                            # \n",
    "                            batch = batch.union(values)\n",
    "\n",
    "                    with _timer('adv', timing_raw):\n",
    "\n",
    "                        # å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "                        if self.use_rm:\n",
    "                            # å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—\n",
    "                            reward_tensor = self.rm_wg.compute_rm_score(batch)\n",
    "                            batch = batch.union(reward_tensor)\n",
    "\n",
    "                        # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®å ±é…¬é–¢æ•°ã¨çµ„ã¿åˆã‚ã›ã‚‹\n",
    "                        reward_tensor = self.reward_fn(batch)\n",
    "\n",
    "                        # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®ã‚¹ã‚³ã‚¢ã‚’ãƒãƒƒãƒã«ä¿å­˜\n",
    "                        batch.batch['token_level_scores'] = reward_tensor\n",
    "\n",
    "                        # KLãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’é©ç”¨ã™ã‚‹å ´åˆ\n",
    "                        if not self.config.actor_rollout_ref.actor.use_kl_loss:\n",
    "\n",
    "                            # KLãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’è¨ˆç®—ã—ã¦é©ç”¨\n",
    "                            batch, kl_metrics = apply_kl_penalty(\n",
    "                                batch,\n",
    "                                kl_ctrl=self.kl_ctrl,\n",
    "                                kl_penalty=self.config.algorithm.kl_penalty\n",
    "                            )\n",
    "\n",
    "                            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«KLé–¢é€£ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¿½åŠ \n",
    "                            metrics.update(kl_metrics)\n",
    "                        else:\n",
    "                            # KLãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’é©ç”¨ã—ãªã„å ´åˆã€KLä¿‚æ•°ã‚’0ã«è¨­å®š\n",
    "                            batch.batch['token_level_rewards'] = batch.batch['token_level_scores']\n",
    "\n",
    "                        # ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã‚’è¨ˆç®—\n",
    "                        batch = compute_advantage(\n",
    "                            batch,\n",
    "                            adv_estimator=self.config.algorithm.adv_estimator,\n",
    "                            gamma=self.config.algorithm.gamma,\n",
    "                            lam=self.config.algorithm.lam,\n",
    "                            num_repeat=self.config.actor_rollout_ref.rollout.n\n",
    "                        )\n",
    "\n",
    "                    # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "                    if self.use_critic:\n",
    "\n",
    "                        with _timer('update_critic', timing_raw):\n",
    "                            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã‚’æ›´æ–°\n",
    "                            critic_output = self.critic_wg.update_critic(batch)\n",
    "\n",
    "                        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã®å‡ºåŠ›ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é›†ç´„\n",
    "                        critic_output_metrics = reduce_metrics(critic_output.meta_info['metrics'])\n",
    "\n",
    "                        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ \n",
    "                        metrics.update(critic_output_metrics)\n",
    "\n",
    "                    # ã‚¢ã‚¯ã‚¿ãƒ¼ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æœŸé–“ãŒçµ‚äº†ã—ãŸå ´åˆ\n",
    "                    if self.config.trainer.critic_warmup <= self.global_steps:\n",
    "\n",
    "                        with _timer('update_actor', timing_raw):\n",
    "                            # ã‚¢ã‚¯ã‚¿ãƒ¼ã‚’æ›´æ–°\n",
    "                            actor_output = self.actor_rollout_wg.update_actor(batch)\n",
    "\n",
    "                        # ã‚¢ã‚¯ã‚¿ãƒ¼ã®å‡ºåŠ›ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é›†ç´„\n",
    "                        actor_output_metrics = reduce_metrics(actor_output.meta_info['metrics'])\n",
    "\n",
    "                        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ \n",
    "                        metrics.update(actor_output_metrics)\n",
    "\n",
    "                    # æ¤œè¨¼\n",
    "                    if self.val_reward_fn is not None and self.config.trainer.test_freq > 0 and \\\n",
    "                        self.global_steps % self.config.trainer.test_freq == 0:\n",
    "\n",
    "                        with _timer('testing', timing_raw):\n",
    "                            # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è©•ä¾¡\n",
    "                            val_metrics: dict = self._validate()\n",
    "\n",
    "                        # æ¤œè¨¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¿½åŠ \n",
    "                        metrics.update(val_metrics)\n",
    "\n",
    "                    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜\n",
    "                    if self.config.trainer.save_freq > 0 and \\\n",
    "                            self.global_steps % self.config.trainer.save_freq == 0:\n",
    "\n",
    "                        with _timer('save_checkpoint', timing_raw):\n",
    "                            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜\n",
    "                            self._save_checkpoint()\n",
    "\n",
    "                # ãƒ‡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "                metrics.update(compute_data_metrics(batch=batch, use_critic=self.use_critic))\n",
    "\n",
    "                # å‡¦ç†æ™‚é–“ã«é–¢ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "                metrics.update(compute_timing_metrics(batch=batch, timing_raw=timing_raw))\n",
    "\n",
    "                # ãƒ­ã‚°ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²\n",
    "                logger.log(data=metrics, step=self.global_steps)\n",
    "\n",
    "                # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ\n",
    "                self.global_steps += 1\n",
    "\n",
    "                # ç·è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°ã«é”ã—ãŸå ´åˆ\n",
    "                if self.global_steps >= self.total_training_steps:\n",
    "\n",
    "                    # æ¤œè¨¼ç”¨ã®å ±é…¬é–¢æ•°ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "                    if self.val_reward_fn is not None:\n",
    "\n",
    "                        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æœ€çµ‚è©•ä¾¡\n",
    "                        val_metrics = self._validate()\n",
    "                        \n",
    "                        pprint(f'æœ€çµ‚çš„ãªæ¤œè¨¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {val_metrics}')\n",
    "\n",
    "                        logger.log(data=val_metrics, step=self.global_steps)\n",
    "\n",
    "                    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†\n",
    "                    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed599d",
   "metadata": {},
   "source": [
    "### å ±é…¬é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_solution(solution_str):\n",
    "    \"\"\"Extract the equation from the solution string.\"\"\"\n",
    "    # Remove everything before the first \"Assistant:\"\n",
    "    if \"Assistant:\" in solution_str:\n",
    "        solution_str = solution_str.split(\"Assistant:\", 1)[1]\n",
    "    elif \"<|im_start|>assistant\" in solution_str:\n",
    "        solution_str = solution_str.split(\"<|im_start|>assistant\", 1)[1]\n",
    "    else:\n",
    "        return None\n",
    "    solution_str = solution_str.split('\\n')[-1]\n",
    "\n",
    "    answer_pattern = r'<answer>(.*?)</answer>'\n",
    "    match = re.finditer(answer_pattern, solution_str)\n",
    "    matches = list(match)\n",
    "    if matches:\n",
    "        final_answer = matches[-1].group(1).strip()\n",
    "    else:\n",
    "        final_answer = None\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc301caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_equation(equation_str, available_numbers):\n",
    "    \"\"\"Validate that equation only uses available numbers and each number once.\"\"\"\n",
    "    try:\n",
    "        # Extract all numbers from the equation\n",
    "        numbers_in_eq = [int(n) for n in re.findall(r'\\d+', equation_str)]\n",
    "        \n",
    "        # Check if all numbers in equation are available\n",
    "        available_numbers = sorted(available_numbers)\n",
    "        numbers_in_eq = sorted(numbers_in_eq)\n",
    "        \n",
    "        # Each number should be used exactly once\n",
    "        return numbers_in_eq == available_numbers\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_equation(equation_str):\n",
    "    \"\"\"Safely evaluate the arithmetic equation using eval() with precautions.\"\"\"\n",
    "    try:\n",
    "        # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace\n",
    "        allowed_pattern = r'^[\\d+\\-*/().\\s]+$'\n",
    "        if not re.match(allowed_pattern, equation_str):\n",
    "            raise ValueError(\"Invalid characters in equation.\")\n",
    "\n",
    "        # Evaluate the equation with restricted globals and locals\n",
    "        result = eval(equation_str, {\"__builtins__\": None}, {})\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b5ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(solution_str, ground_truth, method='strict', format_score=0.1, score=1.):\n",
    "    \"\"\"The scoring function for countdown task.\n",
    "    \n",
    "    Args:\n",
    "        solution_str: the solution text\n",
    "        ground_truth: dictionary containing target number and available numbers\n",
    "        method: the method to extract the solution\n",
    "        format_score: the score for correct format but wrong answer\n",
    "        score: the score for the correct answer\n",
    "    \"\"\"\n",
    "    target = ground_truth['target']\n",
    "    numbers = ground_truth['numbers']\n",
    "    \n",
    "    equation = extract_solution(solution_str=solution_str)\n",
    "    do_print = random.randint(1, 64) == 1\n",
    "    \n",
    "    if do_print:\n",
    "        print(f\"--------------------------------\")\n",
    "        print(f\"Target: {target} | Numbers: {numbers}\")\n",
    "        print(f\"Extracted equation: {equation}\")\n",
    "        print(f\"Solution string: {solution_str}\")\n",
    "\n",
    "    if equation is None:\n",
    "        if do_print:\n",
    "            print(f\"No equation found\")\n",
    "        return 0\n",
    "    \n",
    "    # Validate equation uses correct numbers\n",
    "    if not validate_equation(equation, numbers):\n",
    "        if do_print:\n",
    "            print(f\"Invalid equation\")\n",
    "        return format_score\n",
    "        \n",
    "    # Evaluate equation\n",
    "    try:\n",
    "        result = evaluate_equation(equation)\n",
    "        if result is None:\n",
    "            if do_print:\n",
    "                print(f\"Could not evaluate equation\")\n",
    "            return format_score\n",
    "            \n",
    "        if abs(result - target) < 1e-5:  # Account for floating point precision\n",
    "            if do_print:\n",
    "                print(f\"Correct equation: {equation} = {result}\")\n",
    "            return score\n",
    "        else:\n",
    "            if do_print:\n",
    "                print(f\"Wrong result: equation = {result}, target = {target}\")\n",
    "            return format_score\n",
    "    except:\n",
    "        if do_print:\n",
    "            print(f\"Error evaluating equation\")\n",
    "        return format_score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cfd477",
   "metadata": {},
   "source": [
    "## è¨“ç·´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe9a36e",
   "metadata": {},
   "source": [
    "```sh\n",
    "N_GPUS=1 BASE_MODEL=\"qwen/qwen2.5-0.5b\" DATA_DIR=\"countdown\" ROLLOUT_TP_SIZE=1 VLLM_ATTENTION_BACKEND=\"XFORMERS\" bash ./scripts/train_tiny_zero.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã®ç’°å¢ƒå¤‰æ•°ã‚’å¤‰æ•°åŒ–\n",
    "\n",
    "BASE_MODEL=\"qwen/qwen2.5-0.5b\"\n",
    "DATA_DIR = \"countdown\"\n",
    "EXPERIMENT_NAME = \"countdown_ppo_demo\"\n",
    "N_GPUS = 1\n",
    "ROLLOUT_TP_SIZE = 1\n",
    "VLLM_ATTENTION_BACKEND = \"XFORMERS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPOãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’èª­ã¿è¾¼ã¿\n",
    "\n",
    "# TinyZero/verl/trainer/config/ppo_trainer.yaml\n",
    "ppo_trainer_yaml = \"\"\"\n",
    "data:\n",
    "  tokenizer: null\n",
    "  train_files: ~/data/rlhf/gsm8k/train.parquet\n",
    "  val_files: ~/data/rlhf/gsm8k/test.parquet\n",
    "  prompt_key: prompt\n",
    "  max_prompt_length: 512\n",
    "  max_response_length: 512\n",
    "  train_batch_size: 1024\n",
    "  val_batch_size: 1312\n",
    "  return_raw_input_ids: False  # This should be set to true when the tokenizer between policy and rm differs\n",
    "  return_raw_chat: False\n",
    "\n",
    "actor_rollout_ref:\n",
    "  hybrid_engine: True\n",
    "  model:\n",
    "    path: ~/models/deepseek-llm-7b-chat\n",
    "    external_lib: null\n",
    "    override_config: { }\n",
    "    enable_gradient_checkpointing: False\n",
    "    use_remove_padding: False\n",
    "  actor:\n",
    "    strategy: fsdp  # This is for backward-compatibility\n",
    "    ppo_mini_batch_size: 256\n",
    "    ppo_micro_batch_size: 64\n",
    "    use_dynamic_bsz: False\n",
    "    ppo_max_token_len_per_gpu: 16384 # n * ${data.max_prompt_length} + ${data.max_response_length}\n",
    "    grad_clip: 1.0\n",
    "    clip_ratio: 0.2\n",
    "    entropy_coeff: 0.001\n",
    "    use_kl_loss: False # True for GRPO\n",
    "    kl_loss_coef: 0.001 # for grpo\n",
    "    kl_loss_type: low_var_kl # for grpo\n",
    "    ppo_epochs: 1\n",
    "    shuffle: False\n",
    "    ulysses_sequence_parallel_size: 1 # sp size\n",
    "    optim:\n",
    "      lr: 1e-6\n",
    "      lr_warmup_steps_ratio: 0.  # the total steps will be injected during runtime\n",
    "      min_lr_ratio: null   # only useful for warmup with cosine\n",
    "      warmup_style: constant  # select from constant/cosine\n",
    "      total_training_steps: -1  # must be override by program\n",
    "    fsdp_config:\n",
    "      wrap_policy:\n",
    "        # transformer_layer_cls_to_wrap: None\n",
    "        min_num_params: 0\n",
    "      param_offload: False\n",
    "      grad_offload: False\n",
    "      optimizer_offload: False\n",
    "      fsdp_size: -1\n",
    "  ref:\n",
    "    fsdp_config:\n",
    "      param_offload: False\n",
    "      wrap_policy:\n",
    "        # transformer_layer_cls_to_wrap: None\n",
    "        min_num_params: 0\n",
    "      fsdp_size: -1\n",
    "    log_prob_micro_batch_size: 128\n",
    "    log_prob_use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}\n",
    "    log_prob_max_token_len_per_gpu: ${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}\n",
    "    ulysses_sequence_parallel_size: ${actor_rollout_ref.actor.ulysses_sequence_parallel_size} # sp size\n",
    "  rollout:\n",
    "    name: vllm\n",
    "    temperature: 1.0\n",
    "    top_k: -1 # 0 for hf rollout, -1 for vllm rollout\n",
    "    top_p: 1\n",
    "    prompt_length: ${data.max_prompt_length}  # not use for opensource\n",
    "    response_length: ${data.max_response_length}\n",
    "    # for vllm rollout\n",
    "    dtype: bfloat16 # should align with FSDP\n",
    "    gpu_memory_utilization: 0.5\n",
    "    ignore_eos: False\n",
    "    enforce_eager: True\n",
    "    free_cache_engine: True\n",
    "    load_format: dummy_dtensor\n",
    "    tensor_model_parallel_size: 2\n",
    "    max_num_batched_tokens: 8192\n",
    "    max_num_seqs: 1024\n",
    "    log_prob_micro_batch_size: 128\n",
    "    log_prob_use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}\n",
    "    log_prob_max_token_len_per_gpu: ${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}\n",
    "    # for hf rollout\n",
    "    do_sample: True\n",
    "    # number of responses (i.e. num sample times)\n",
    "    n: 1 # > 1 for grpo\n",
    "\n",
    "critic:\n",
    "  strategy: fsdp\n",
    "  optim:\n",
    "    lr: 1e-5\n",
    "    lr_warmup_steps_ratio: 0.  # the total steps will be injected during runtime\n",
    "    min_lr_ratio: null   # only useful for warmup with cosine\n",
    "    warmup_style: constant  # select from constant/cosine\n",
    "    total_training_steps: -1  # must be override by program\n",
    "  model:\n",
    "    path: ~/models/deepseek-llm-7b-chat\n",
    "    tokenizer_path: ${actor_rollout_ref.model.path}\n",
    "    override_config: { }\n",
    "    external_lib: ${actor_rollout_ref.model.external_lib}\n",
    "    enable_gradient_checkpointing: False\n",
    "    use_remove_padding: False\n",
    "    fsdp_config:\n",
    "      param_offload: False\n",
    "      grad_offload: False\n",
    "      optimizer_offload: False\n",
    "      wrap_policy:\n",
    "        # transformer_layer_cls_to_wrap: None\n",
    "        min_num_params: 0\n",
    "      fsdp_size: -1\n",
    "  ppo_mini_batch_size: ${actor_rollout_ref.actor.ppo_mini_batch_size}\n",
    "  ppo_micro_batch_size: 64\n",
    "  forward_micro_batch_size: ${critic.ppo_micro_batch_size}\n",
    "  use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}\n",
    "  ppo_max_token_len_per_gpu: 32768 # (${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}) * 2\n",
    "  forward_max_token_len_per_gpu: ${critic.ppo_max_token_len_per_gpu}\n",
    "  ulysses_sequence_parallel_size: 1 # sp size\n",
    "  ppo_epochs: ${actor_rollout_ref.actor.ppo_epochs}\n",
    "  shuffle: ${actor_rollout_ref.actor.shuffle}\n",
    "  grad_clip: 1.0\n",
    "  cliprange_value: 0.5\n",
    "\n",
    "reward_model:\n",
    "  enable: False\n",
    "  strategy: fsdp\n",
    "  model:\n",
    "    input_tokenizer: ${actor_rollout_ref.model.path}  # set this to null if the chat template is identical\n",
    "    path: ~/models/FsfairX-LLaMA3-RM-v0.1\n",
    "    external_lib: ${actor_rollout_ref.model.external_lib}\n",
    "    use_remove_padding: False\n",
    "    fsdp_config:\n",
    "      min_num_params: 0\n",
    "      param_offload: False\n",
    "  micro_batch_size: 64\n",
    "  max_length: null\n",
    "  ulysses_sequence_parallel_size: 1 # sp size\n",
    "  use_dynamic_bsz: ${critic.use_dynamic_bsz}\n",
    "  forward_max_token_len_per_gpu: ${critic.forward_max_token_len_per_gpu}\n",
    "\n",
    "algorithm:\n",
    "  gamma: 1.0\n",
    "  lam: 1.0\n",
    "  adv_estimator: gae\n",
    "  kl_penalty: kl  # how to estimate kl divergence\n",
    "  kl_ctrl:\n",
    "    type: fixed\n",
    "    kl_coef: 0.001\n",
    "\n",
    "trainer:\n",
    "  total_epochs: 30\n",
    "  total_training_steps: null\n",
    "  project_name: verl_examples\n",
    "  experiment_name: gsm8k\n",
    "  logger: [ 'console', 'wandb' ]\n",
    "  nnodes: 1\n",
    "  n_gpus_per_node: 8\n",
    "  save_freq: -1\n",
    "  test_freq: -1\n",
    "  critic_warmup: 0\n",
    "  default_hdfs_dir: ~/experiments/gsm8k/ppo/${trainer.experiment_name}\n",
    "  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}\n",
    "\"\"\"\n",
    "\n",
    "ppo_trainer_config = OmegaConf.create(ppo_trainer_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684dfd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä¸Šæ›¸ã\n",
    "\n",
    "# TinyZero/scripts/train_tiny_zero.shã‚ˆã‚ŠæŠœç²‹\n",
    "# python3 -m verl.trainer.main_ppo \\\n",
    "# data.train_files=$DATA_DIR/train.parquet \\\n",
    "# data.val_files=$DATA_DIR/test.parquet \\\n",
    "# data.train_batch_size=32 \\\n",
    "# data.val_batch_size=32 \\\n",
    "# data.max_prompt_length=256 \\\n",
    "# data.max_response_length=1024 \\\n",
    "# actor_rollout_ref.model.path=$BASE_MODEL \\\n",
    "# actor_rollout_ref.model.use_remove_padding=True \\\n",
    "# actor_rollout_ref.model.enable_gradient_checkpointing=True \\\n",
    "# actor_rollout_ref.actor.use_dynamic_bsz=True \\\n",
    "# actor_rollout_ref.actor.optim.lr=1e-6 \\\n",
    "# actor_rollout_ref.actor.ppo_mini_batch_size=4 \\\n",
    "# actor_rollout_ref.actor.ppo_micro_batch_size=4 \\\n",
    "# actor_rollout_ref.rollout.log_prob_micro_batch_size=1 \\\n",
    "# actor_rollout_ref.rollout.tensor_model_parallel_size=$ROLLOUT_TP_SIZE \\\n",
    "# actor_rollout_ref.rollout.gpu_memory_utilization=0.1 \\\n",
    "# actor_rollout_ref.ref.log_prob_micro_batch_size=1 \\\n",
    "# critic.optim.lr=1e-5 \\\n",
    "# critic.model.path=$BASE_MODEL \\\n",
    "# critic.ppo_micro_batch_size=1 \\\n",
    "# algorithm.kl_ctrl.kl_coef=0.001 \\\n",
    "# trainer.logger=[] \\\n",
    "# +trainer.val_before_train=False \\\n",
    "# trainer.default_hdfs_dir=null \\\n",
    "# trainer.n_gpus_per_node=$N_GPUS \\\n",
    "# trainer.nnodes=1 \\\n",
    "# trainer.save_freq=100 \\\n",
    "# trainer.test_freq=100 \\\n",
    "# trainer.project_name=TinyZero \\\n",
    "# trainer.experiment_name=$EXPERIMENT_NAME \\\n",
    "# trainer.total_epochs=15 2>&1 | tee verl_demo.log\n",
    "\n",
    "ppo_trainer_config.data.train_files = os.path.join(DATA_DIR, \"train.parquet\")\n",
    "ppo_trainer_config.data.val_files = os.path.join(DATA_DIR, \"test.parquet\")\n",
    "ppo_trainer_config.data.train_batch_size = 32\n",
    "ppo_trainer_config.data.val_batch_size = 32\n",
    "ppo_trainer_config.data.max_prompt_length = 256\n",
    "ppo_trainer_config.data.max_response_length = 1024\n",
    "ppo_trainer_config.actor_rollout_ref.model.path = BASE_MODEL\n",
    "ppo_trainer_config.actor_rollout_ref.model.use_remove_padding = True\n",
    "ppo_trainer_config.actor_rollout_ref.model.enable_gradient_checkpointing = True\n",
    "ppo_trainer_config.actor_rollout_ref.actor.use_dynamic_bsz = True\n",
    "ppo_trainer_config.actor_rollout_ref.actor.optim.lr = 1e-6\n",
    "ppo_trainer_config.actor_rollout_ref.actor.ppo_mini_batch_size = 4\n",
    "ppo_trainer_config.actor_rollout_ref.actor.ppo_micro_batch_size = 4\n",
    "ppo_trainer_config.actor_rollout_ref.rollout.log_prob_micro_batch_size = 1\n",
    "ppo_trainer_config.actor_rollout_ref.rollout.tensor_model_parallel_size = ROLLOUT_TP_SIZE\n",
    "ppo_trainer_config.actor_rollout_ref.rollout.gpu_memory_utilization = 0.1\n",
    "ppo_trainer_config.actor_rollout_ref.ref.log_prob_micro_batch_size = 1\n",
    "ppo_trainer_config.critic.optim.lr = 1e-5\n",
    "ppo_trainer_config.critic.model.path = BASE_MODEL\n",
    "ppo_trainer_config.critic.ppo_micro_batch_size = 1\n",
    "ppo_trainer_config.algorithm.kl_ctrl.kl_coef = 0.001\n",
    "ppo_trainer_config.trainer.logger = []\n",
    "ppo_trainer_config.trainer.val_before_train = False\n",
    "ppo_trainer_config.trainer.default_hdfs_dir = None\n",
    "ppo_trainer_config.trainer.n_gpus_per_node = N_GPUS\n",
    "ppo_trainer_config.trainer.nnodes = 1\n",
    "ppo_trainer_config.trainer.save_freq = 100\n",
    "ppo_trainer_config.trainer.test_freq = 100\n",
    "ppo_trainer_config.trainer.project_name = \"TinyZero\"\n",
    "ppo_trainer_config.trainer.experiment_name = EXPERIMENT_NAME\n",
    "\n",
    "# ãƒ‡ãƒãƒƒã‚°ç›®çš„ã§ã‚¨ãƒãƒƒã‚¯æ•°ã‚’1ã«è¨­å®š\n",
    "ppo_trainer_config.trainer.total_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1c62c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_rm_score_fn(data_source):\n",
    "    # if data_source == 'openai/gsm8k':\n",
    "    #     return gsm8k.compute_score\n",
    "    # elif data_source == 'lighteval/MATH':\n",
    "    #     return math.compute_score\n",
    "    # elif \"multiply\" in data_source or \"arithmetic\" in data_source:\n",
    "    #     return multiply.compute_score\n",
    "    # elif \"countdown\" in data_source:\n",
    "    #     return countdown.compute_score\n",
    "    # else:\n",
    "    #     raise NotImplementedError\n",
    "    return compute_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5be65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardManager():\n",
    "    \"\"\"The reward manager.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, num_examine) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_examine = num_examine  # the number of batches of decoded responses to print to the console\n",
    "\n",
    "    def __call__(self, data: DataProto):\n",
    "        \"\"\"We will expand this function gradually based on the available datasets\"\"\"\n",
    "\n",
    "        # If there is rm score, we directly return rm score. Otherwise, we compute via rm_score_fn\n",
    "        if 'rm_scores' in data.batch.keys():\n",
    "            return data.batch['rm_scores']\n",
    "\n",
    "        reward_tensor = torch.zeros_like(data.batch['responses'], dtype=torch.float32)\n",
    "\n",
    "        already_print_data_sources = {}\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            data_item = data[i]  # DataProtoItem\n",
    "\n",
    "            prompt_ids = data_item.batch['prompts']\n",
    "\n",
    "            prompt_length = prompt_ids.shape[-1]\n",
    "\n",
    "            valid_prompt_length = data_item.batch['attention_mask'][:prompt_length].sum()\n",
    "            valid_prompt_ids = prompt_ids[-valid_prompt_length:]\n",
    "\n",
    "            response_ids = data_item.batch['responses']\n",
    "            valid_response_length = data_item.batch['attention_mask'][prompt_length:].sum()\n",
    "            valid_response_ids = response_ids[:valid_response_length]\n",
    "\n",
    "            # decode\n",
    "            sequences = torch.cat((valid_prompt_ids, valid_response_ids))\n",
    "            sequences_str = self.tokenizer.decode(sequences)\n",
    "\n",
    "            ground_truth = data_item.non_tensor_batch['reward_model']['ground_truth']\n",
    "\n",
    "            # select rm_score\n",
    "            data_source = data_item.non_tensor_batch['data_source']\n",
    "            compute_score_fn = _select_rm_score_fn(data_source)\n",
    "\n",
    "            score = compute_score_fn(solution_str=sequences_str, ground_truth=ground_truth)\n",
    "            reward_tensor[i, valid_response_length - 1] = score\n",
    "\n",
    "            if data_source not in already_print_data_sources:\n",
    "                already_print_data_sources[data_source] = 0\n",
    "\n",
    "            if already_print_data_sources[data_source] < self.num_examine:\n",
    "                already_print_data_sources[data_source] += 1\n",
    "                print(sequences_str)\n",
    "\n",
    "        return reward_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff4926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ray.remote\n",
    "def main_task(config):\n",
    "    logger.info(f\"ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹\")\n",
    "\n",
    "    # from verl.utils.fs import copy_local_path_from_hdfs\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    # 1) åˆæœŸåŒ–\n",
    "\n",
    "    # 1-1) è¨­å®šã‚’è¡¨ç¤º\n",
    "    # pprint(OmegaConf.to_container(config, resolve=True))\n",
    "    OmegaConf.resolve(config)\n",
    "    logger.debug(f\"è¨­å®š: {config}\")\n",
    "\n",
    "    # 1-2) Hadoop File System (HDFS) ã‹ã‚‰ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "    local_path = copy_local_path_from_hdfs(config.actor_rollout_ref.model.path)\n",
    "    logger.debug(f\"ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹: {local_path}\")\n",
    "\n",
    "    # 1-3) ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–\n",
    "    tokenizer = hf_tokenizer(local_path)\n",
    "    logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼: {tokenizer}\")\n",
    "\n",
    "    # 1-4) Rayã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã®åˆæœŸåŒ–\n",
    "    if config.actor_rollout_ref.actor.strategy == 'fsdp':\n",
    "        logger.debug(\"FSDPãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨\")\n",
    "        assert config.actor_rollout_ref.actor.strategy == config.critic.strategy\n",
    "        # from verl.workers.fsdp_workers import ActorRolloutRefWorker, CriticWorker\n",
    "        # from verl.single_controller.ray import RayWorkerGroup\n",
    "        ray_worker_group_cls = RayWorkerGroup\n",
    "\n",
    "    # elif config.actor_rollout_ref.actor.strategy == 'megatron':\n",
    "    #     logger.debug(\"Megatronãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨\")\n",
    "    #     assert config.actor_rollout_ref.actor.strategy == config.critic.strategy\n",
    "    #     from verl.workers.megatron_workers import ActorRolloutRefWorker, CriticWorker\n",
    "    #     from verl.single_controller.ray.megatron import NVMegatronRayWorkerGroup\n",
    "    #     ray_worker_group_cls = NVMegatronRayWorkerGroup\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # from verl.trainer.ppo.ray_trainer import ResourcePoolManager, Role\n",
    "\n",
    "    role_worker_mapping = {\n",
    "        Role.ActorRollout: ray.remote(ActorRolloutRefWorker),\n",
    "        Role.Critic: ray.remote(CriticWorker),\n",
    "        Role.RefPolicy: ray.remote(ActorRolloutRefWorker)\n",
    "    }\n",
    "\n",
    "    global_pool_id = 'global_pool'\n",
    "\n",
    "    resource_pool_spec = {\n",
    "        global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes,\n",
    "    }\n",
    "\n",
    "    mapping = {\n",
    "        Role.ActorRollout: global_pool_id,\n",
    "        Role.Critic: global_pool_id,\n",
    "        Role.RefPolicy: global_pool_id,\n",
    "    }\n",
    "\n",
    "    # we should adopt a multi-source reward function here\n",
    "    # - for rule-based rm, we directly call a reward score\n",
    "    # - for model-based rm, we call a model\n",
    "    # - for code related prompt, we send to a sandbox if there are test cases\n",
    "    # - finally, we combine all the rewards together\n",
    "    # - The reward type depends on the tag of the data\n",
    "    if config.reward_model.enable:\n",
    "        if config.reward_model.strategy == 'fsdp':\n",
    "            logger.debug(\"FSDPå ±é…¬ãƒ¢ãƒ‡ãƒ«ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½¿ç”¨\")\n",
    "            # from verl.workers.fsdp_workers import RewardModelWorker\n",
    "        # elif config.reward_model.strategy == 'megatron':\n",
    "        #     logger.debug(\"Megatronå ±é…¬ãƒ¢ãƒ‡ãƒ«ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½¿ç”¨\")\n",
    "        #     from verl.workers.megatron_workers import RewardModelWorker\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        role_worker_mapping[Role.RewardModel] = ray.remote(RewardModelWorker)\n",
    "\n",
    "        mapping[Role.RewardModel] = global_pool_id\n",
    "    else:\n",
    "        logger.debug(\"é–¢æ•°ãƒ™ãƒ¼ã‚¹ã®å ±é…¬ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½¿ç”¨\")\n",
    "\n",
    "    reward_fn = RewardManager(tokenizer=tokenizer, num_examine=0)\n",
    "\n",
    "    # Note that we always use function-based RM for validation\n",
    "    val_reward_fn = RewardManager(tokenizer=tokenizer, num_examine=1)\n",
    "\n",
    "    resource_pool_manager = ResourcePoolManager(\n",
    "        resource_pool_spec=resource_pool_spec,\n",
    "        mapping=mapping\n",
    "    )\n",
    "\n",
    "    trainer = RayPPOTrainer(\n",
    "        config=config,\n",
    "        tokenizer=tokenizer,\n",
    "        role_worker_mapping=role_worker_mapping,\n",
    "        resource_pool_manager=resource_pool_manager,\n",
    "        ray_worker_group_cls=ray_worker_group_cls,\n",
    "        reward_fn=reward_fn,\n",
    "        val_reward_fn=val_reward_fn\n",
    "    )\n",
    "\n",
    "    trainer.init_workers()\n",
    "\n",
    "    trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7faacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ã‚’é–‹å§‹\n",
    "\n",
    "# @hydra.main(config_path='config', config_name='ppo_trainer', version_base=None)\n",
    "def main(config):\n",
    "\n",
    "    # RayãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆ\n",
    "    if not ray.is_initialized():\n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«ã®Rayã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–\n",
    "        logger.debug(f\"Rayã‚’åˆæœŸåŒ–\")\n",
    "        ray.init(runtime_env={\n",
    "            'env_vars': {'TOKENIZERS_PARALLELISM': 'true', 'NCCL_DEBUG': 'WARN'}\n",
    "        })\n",
    "\n",
    "    # ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ\n",
    "    # ray.get(main_task.remote(config))\n",
    "    main_task(config)\n",
    "\n",
    "main(ppo_trainer_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
